% Models
@misc{phi3,
  title         = {{Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone}},
  author        = {Marah Abdin and Sam Ade Jacobs and Ammar Ahmad Awan and Jyoti Aneja},
  year          = {2024},
  eprint        = {2404.14219},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{orca,
      title={Orca: Progressive Learning from Complex Explanation Traces of GPT-4}, 
      author={Subhabrata Mukherjee and Arindam Mitra and Ganesh Jawahar and Sahaj Agarwal and Hamid Palangi and Ahmed Awadallah},
      year={2023},
      eprint={2306.02707},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@misc{phi1.5,
  title         = {Textbooks Are All You Need II: phi-1.5 technical report},
  author        = {Yuanzhi Li and Sébastien Bubeck and Ronen Eldan and Allie Del Giorno and Suriya Gunasekar and Yin Tat Lee},
  year          = {2023},
  eprint        = {2309.05463},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{llama,
  title         = {{LLaMA: Open and Efficient Foundation Language Models}},
  author        = {Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
  year          = {2023},
  eprint        = {2302.13971},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{openelm,
  title         = {{OpenELM: An Efficient Language Model Family with Open Training and Inference Framework}},
  author        = {Sachin Mehta and Mohammad Hossein Sekhavat and Qingqing Cao and Maxwell Horton and Yanzi Jin and Chenfan Sun and Iman Mirzadeh and Mahyar Najibi and Dmitry Belenko and Peter Zatloukal and Mohammad Rastegari},
  year          = {2024},
  eprint        = {2404.14619},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{mixtral,
  title         = {{Mixtral of Experts}},
  author        = {Albert Q. Jiang and Alexandre Sablayrolles and Antoine Roux and Arthur Mensch and Blanche Savary and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Emma Bou Hanna and Florian Bressand and Gianna Lengyel and Guillaume Bour and Guillaume Lample and Lélio Renard Lavaud and Lucile Saulnier and Marie-Anne Lachaux and Pierre Stock and Sandeep Subramanian and Sophia Yang and Szymon Antoniak and Teven Le Scao and Théophile Gervet and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
  year          = {2024},
  eprint        = {2401.04088},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@misc{bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

% Studies on LLMs in education
@misc{wei2023chainofthought,
  title         = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author        = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
  year          = {2023},
  eprint        = {2201.11903},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@inproceedings{Savelka_2023,
  series     = {ICER 2023},
  title      = {Thrilled by Your Progress! Large Language Models (GPT-4) No Longer Struggle to Pass Assessments in Higher Education Programming Courses},
  url        = {http://dx.doi.org/10.1145/3568813.3600142},
  doi        = {10.1145/3568813.3600142},
  booktitle  = {Proceedings of the 2023 ACM Conference on International Computing Education Research V.1},
  publisher  = {ACM},
  author     = {Savelka, Jaromir and Agarwal, Arav and An, Marshall and Bogart, Chris and Sakr, Majd},
  year       = {2023},
  month      = aug,
  collection = {ICER 2023}
}

@misc{wang2024large,
  title         = {Large Language Models for Education: A Survey and Outlook},
  author        = {Shen Wang and Tianlong Xu and Hang Li and Chaoli Zhang and Joleen Liang and Jiliang Tang and Philip S. Yu and Qingsong Wen},
  year          = {2024},
  eprint        = {2403.18105},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{wu2023empirical,
  title         = {An Empirical Study on Challenging Math Problem Solving with GPT-4},
  author        = {Yiran Wu and Feiran Jia and Shaokun Zhang and Hangyu Li and Erkang Zhu and Yue Wang and Yin Tat Lee and Richard Peng and Qingyun Wu and Chi Wang},
  year          = {2023},
  eprint        = {2306.01337},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{liévin2023large,
  title         = {Can large language models reason about medical questions?},
  author        = {Valentin Liévin and Christoffer Egeberg Hother and Andreas Geert Motzfeldt and Ole Winther},
  year          = {2023},
  eprint        = {2207.08143},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

% Studies on finetuning LLMs

@misc{zhang2024scaling,
  title         = {When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method},
  author        = {Biao Zhang and Zhongtao Liu and Colin Cherry and Orhan Firat},
  year          = {2024},
  eprint        = {2402.17193},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{kaplan2020scaling,
  title         = {Scaling Laws for Neural Language Models},
  author        = {Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
  year          = {2020},
  eprint        = {2001.08361},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@misc{hestness2017deep,
  title         = {Deep Learning Scaling is Predictable, Empirically},
  author        = {Joel Hestness and Sharan Narang and Newsha Ardalani and Gregory Diamos and Heewoo Jun and Hassan Kianinejad and Md. Mostofa Ali Patwary and Yang Yang and Yanqi Zhou},
  year          = {2017},
  eprint        = {1712.00409},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

# RLHF
@misc{kaufmann2024survey,
  title         = {A Survey of Reinforcement Learning from Human Feedback},
  author        = {Timo Kaufmann and Paul Weng and Viktor Bengs and Eyke Hüllermeier},
  year          = {2024},
  eprint        = {2312.14925},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@misc{schulman2017proximal,
  title         = {Proximal Policy Optimization Algorithms},
  author        = {John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
  year          = {2017},
  eprint        = {1707.06347},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

% Studies trying to find problems with current benchmarks
@misc{fourrier2023,
  author = {Clémentine Fourrier and Nathan Habib and Thomas Wolf and Julien Launay},
  title  = {What's going on with the Open LLM Leaderboard?},
  year   = {2023},
  month  = {June},
  day    = {23},
  url    = {https://example.com/article-url}
}
@misc{zheng2024large,
  title         = {Large Language Models Are Not Robust Multiple Choice Selectors},
  author        = {Chujie Zheng and Hao Zhou and Fandong Meng and Jie Zhou and Minlie Huang},
  year          = {2024},
  eprint        = {2309.03882},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{khatun2023reliability,
  title         = {Reliability Check: An Analysis of GPT-3's Response to Sensitive Topics and Prompt Wording},
  author        = {Aisha Khatun and Daniel G. Brown},
  year          = {2023},
  eprint        = {2306.06199},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{balepur2024artifacts,
  title         = {Artifacts or Abduction: How Do LLMs Answer Multiple-Choice Questions Without the Question?},
  author        = {Nishant Balepur and Abhilasha Ravichander and Rachel Rudinger},
  year          = {2024},
  eprint        = {2402.12483},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{li2024multiplechoice,
  title         = {Can multiple-choice questions really be useful in detecting the abilities of LLMs?},
  author        = {Wangyue Li and Liangzhi Li and Tong Xiang and Xiao Liu and Wei Deng and Noa Garcia},
  year          = {2024},
  eprint        = {2403.17752},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{khatun2024study,
  title         = {A Study on Large Language Models' Limitations in Multiple-Choice Question Answering},
  author        = {Aisha Khatun and Daniel G. Brown},
  year          = {2024},
  eprint        = {2401.07955},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

% Techniques
@misc{eval-harness,
  author    = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title     = {A framework for few-shot language model evaluation},
  month     = 12,
  year      = 2023,
  publisher = {Zenodo},
  version   = {v0.4.0},
  doi       = {10.5281/zenodo.10256836},
  url       = {https://zenodo.org/records/10256836}
}

@misc{rso,
  title         = {{Statistical Rejection Sampling Improves Preference Optimization}},
  author        = {Tianqi Liu and Yao Zhao and Rishabh Joshi and Misha Khalman and Mohammad Saleh and Peter J. Liu and Jialu Liu},
  year          = {2024},
  eprint        = {2309.06657},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{robust-dpo,
  title         = {{Provably Robust DPO: Aligning Language Models with Noisy Feedback}},
  author        = {Sayak Ray Chowdhury and Anush Kini and Nagarajan Natarajan},
  year          = {2024},
  eprint        = {2403.00409},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@misc{dpo,
  title         = {{Direct Preference Optimization: Your Language Model is Secretly a Reward Model}},
  author        = {Rafael Rafailov and Archit Sharma and Eric Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn},
  year          = {2023},
  eprint        = {2305.18290},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@misc{ipo,
  title         = {A General Theoretical Paradigm to Understand Learning from Human Preferences},
  author        = {Mohammad Gheshlaghi Azar and Mark Rowland and Bilal Piot and Daniel Guo and Daniele Calandriello and Michal Valko and Rémi Munos},
  year          = {2023},
  eprint        = {2310.12036},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}

@misc{hinge,
  title         = {{SLiC-HF: Sequence Likelihood Calibration with Human Feedback}},
  author        = {Yao Zhao and Rishabh Joshi and Tianqi Liu and Misha Khalman and Mohammad Saleh and Peter J. Liu},
  year          = {2023},
  eprint        = {2305.10425},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{label-smooth,
  title         = {When Does Label Smoothing Help?},
  author        = {Rafael Müller and Simon Kornblith and Geoffrey Hinton},
  year          = {2020},
  eprint        = {1906.02629},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@misc{lora,
  title         = {LoRA: Low-Rank Adaptation of Large Language Models},
  author        = {Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
  year          = {2021},
  eprint        = {2106.09685},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{qlora,
  title         = {QLoRA: Efficient Finetuning of Quantized LLMs},
  author        = {Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},
  year          = {2023},
  eprint        = {2305.14314},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@misc{mcqa-scoring,
  title         = {{Predictions from language models for multiple-choice tasks are not robust under variation of scoring methods}},
  author        = {Polina Tsvilodub and Hening Wang and Sharon Grosch and Michael Franke},
  year          = {2024},
  eprint        = {2403.00998},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

% Benchmarks
@misc{arc,
  title         = {{Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge}},
  author        = {Peter Clark and Isaac Cowhey and Oren Etzioni and Tushar Khot and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
  year          = {2018},
  eprint        = {1803.05457},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}

@misc{mmlu,
  title         = {{Measuring Massive Multitask Language Understanding}},
  author        = {Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
  year          = {2021},
  eprint        = {2009.03300},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CY}
}

@misc{sciq,
  title   = {{Crowdsourcing Multiple Choice Science Questions}},
  author  = {Johannes Welbl, Nelson F. Liu, Matt Gardner},
  year    = {2017},
  journal = {arXiv:1707.06209v1}
}

@misc{gpqa,
  title         = {{GPQA: A Graduate-Level Google-Proof Q\&A Benchmark}},
  author        = {David Rein and Betty Li Hou and Asa Cooper Stickland and Jackson Petty and Richard Yuanzhe Pang and Julien Dirani and Julian Michael and Samuel R. Bowman},
  year          = {2023},
  eprint        = {2311.12022},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}

@misc{openbookqa,
  title         = {{Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering}},
  author        = {Todor Mihaylov and Peter Clark and Tushar Khot and Ashish Sabharwal},
  year          = {2018},
  eprint        = {1809.02789},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}


% Quantisation
@misc{quantisation-overview,
  author  = {Younes Belkada and Clémentine Fourrier and Marc Sun and Ilyas Moutawwakil and Félix Marty},
  title   = {{Overview of Natively Supported Quantization Schemes in Transformers}},
  year    = {2023},
  url     = {https://www.example.com},
  note    = {Blog post},
  urldate = {2024-05-27}
}

@misc{llmint8,
  title         = {{LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale}},
  author        = {Tim Dettmers and Mike Lewis and Younes Belkada and Luke Zettlemoyer},
  year          = {2022},
  eprint        = {2208.07339},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@misc{gptq,
  title         = {{GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers}},
  author        = {Elias Frantar and Saleh Ashkboos and Torsten Hoefler and Dan Alistarh},
  year          = {2023},
  eprint        = {2210.17323},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

% Software
@misc{lmeh,
  author    = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman},
  title     = {{A Framework for Few-Shot Language Model Evaluation}},
  month     = 12,
  year      = 2023,
  publisher = {Zenodo},
  version   = {v0.4.0},
  doi       = {10.5281/zenodo.10256836},
  url       = {https://zenodo.org/records/10256836}
}

@software{transformers,
  author  = {{HuggingFace}},
  title   = {{HuggingFace Transformers}},
  url     = {https://huggingface.co/docs/transformers/en/index},
  version = {0.20.2},
  date    = {2024-06-11}
}

@software{trl,
  author  = {{HuggingFace}},
  title   = {{HuggingFace TRL}},
  url     = {https://huggingface.co/docs/trl/en/index},
  version = {0.20.2},
  date    = {2024-05-27}
}
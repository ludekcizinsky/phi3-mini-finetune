% This section helps the reader understand the research context of your work, by providing an overview of existing work in the area. You might discuss papers that inspired your approach, papers that you use as baselines, papers proposing alternative approaches to the problem, papers applying your methods to different tasks, etc.
% This section shouldn’t go into deep detail in any one paper (e.g., there shouldn’t be any equations). Instead it should explain how the papers relate to each other, and how they relate to your work (e.g., how your work is different from them). This is not a section to copy-paste your reviews from Milestone 1. Those papers can serve as the basis of your related work section, but you should synthesize what was learned into a roughly half a page section.  Attempt to demonstrate, as you review the literature, limitations or motivations that point to why your work is a nice next step, or useful replication, or promising analysis (or otherwise, if your work doesn’t fall into these categories!).

\section{Related Work}\label{sec:related-work}
\textbf{LLMs in Education.} Since its debut in November 2022, ChatGPT by OpenAI has revolutionised various sectors, including education. Applications in this field include automatic grading, material creation and aiding students with problem-solving and clarifications \cite{wang2024large}. Current research on LLMs in education primarily falls into two categories: benchmarking LLMs against educational datasets (e.g. maths \cite{wu2023empirical}, medicine \cite{liévin2023large}, and programming \cite{Savelka_2023}), and developing methodologies to enhance performance on educational tasks, such as the Chain of Thought (CoT) prompting strategy \cite{wei2023chainofthought}. Our research belongs to the latter category, focusing on advancing the base model's performance by fine-tuning on a range of science MCQA datasets.

\textbf{SLMs.} The efficacy of pretrained Large Language Models (LLMs) typically correlates with their size and the volume of training data, as described by a power law \cite{hestness2017deep,kaplan2020scaling}. However, to democratise access to cutting-edge models, there has been a shift towards developing smaller models. For example, Microsoft's Phi \cite{phi3} and Orca \cite{orca} series of models were built with a focus on the quality of the training data rather than the size of model. They outperform larger models on various benchmarks such as GPT-3.5 and Mixtral 8x7B~\cite{mixtral}, and compete with other SLMs like Apple's OpenELM \cite{openelm}. Our research builds on this trend by fine-tuning the Phi-3-Mini model for MCQA tasks. 

\textbf{Fine-Tuning.} Fine-tuning is a common technique to adapt pretrained LLMs to specific tasks. Supervised Fine-Tuning (SFT) is the most common method, where the model is trained on a task-specific dataset with a supervised learning objective \cite{bert}. However, SFT can be computationally expensive and may require large amounts of task-specific data. Therefore, parameter-efficient methods like Low-Rank Adaptation (LoRA) \cite{lora} have been developed to reduce the computational cost of fine-tuning. In our work, we fine-tune using LoRA adapters.

\textbf{Alignment.} Model alignment guides model behavior towards desirable outcomes \cite{kaufmann2024survey}. It is a criticial step in training an LLM. Traditionally, Proximal Policy Optimisation (PPO) was the dominant technique for implementing alignment \cite{schulman2017proximal}. Direct Preference Optimisation (DPO) proposes a different parameterisation of the reward model leading to a simplified alignment procedure \cite{dpo}. Given its simplicity and efficiency, we adopt DPO to align our model.

\textbf{LLMs \& MCQA.} MCQA benchmarks~\cite{arc,sciq,mmlu} are commonly used to measure the capabilities of LLMs. Despite their prevalence, studies have identified several flaws in MCQA benchmarkss as evaluation tools, such as susceptibility to choice dynamics \cite{balepur2024artifacts}, positional biases \cite{li2024multiplechoice,khatun2024study,zheng2024large}, misunderstandings of the MCQA format \cite{khatun2024study}, and sensitivity to prompt phrasing \cite{khatun2023reliability}. Variability in results across different implementations and answer extraction methods further complicates assessments \cite{fourrier2023}. To standardise comparisons of our fine-tuned models, we employ a widely recognised evaluation framework called Language Model Evaluation Harness (LMEH) \cite{lmeh}, ensuring consistent and fair testing.
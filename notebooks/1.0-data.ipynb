{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🙌 Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "---\n",
    "\n",
    "Let's install some necessary dependencies and set global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autorootcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HF Dataset Loading\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from HF Hub\n",
    "from datasets import load_dataset\n",
    "data = load_dataset(\"lhoestq/demo1\", split=\"train\")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BaseDataset\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load `DummyData`\n",
    "from src.data import DummyData\n",
    "\n",
    "dummy = DummyData(split=\"train\")\n",
    "dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preference Dta\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load `PreferenceData`\n",
    "from src.data import PreferenceData\n",
    "\n",
    "preference = PreferenceData(split=\"train\", filtering_strategy=\"none\")\n",
    "preference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `PreferenceData` class, the `filtering_strategy` parameter can be set to one of the following values:\n",
    "\n",
    "* `none`: No filtering is applied.\n",
    "* `keep_first`: Only the first preference pair for each question is kept.\n",
    "* `global_threshold`: Only the preference pairs with a score greater (or less) equal than a global threshold are kept. *Requires: `mode` and `threshold` parameters.*\n",
    "* `local_tolerance`: Only the preference pairs with a score greater (or less) equal than a maximum minus/ minimum plus tolerance are kept. *Requires: `mode` and `tolerance` parameters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PreferenceData(split=\"train\", filtering_strategy=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PreferenceData(split=\"train\", filtering_strategy=\"keep_first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PreferenceData(split=\"train\", filtering_strategy=\"global_threshold\", mode=\"least\", threshold=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PreferenceData(split=\"train\", filtering_strategy=\"local_tolerance\", mode=\"least\", tolerance=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open-Answer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load `OrcaMathData`\n",
    "from src.data import OrcaMathData\n",
    "\n",
    "orcamath = OrcaMathData(filter_on_length=True)\n",
    "orcamath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import TuluData\n",
    "from src.data import TuluDatasetIDs\n",
    "\n",
    "tulu = TuluData(sub_datasets=[TuluDatasetIDs.SCIENCE_EVIDENCE_INFERENCE], filter_out_english=False)\n",
    "tulu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCQ Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SciQ\n",
    "\n",
    "The SciQ dataset contains 13,679 crowdsourced science exam questions about Physics, Chemistry and Biology, among others. The questions are in multiple-choice format with 4 answer options each. For the majority of the questions, an additional paragraph with supporting evidence for the correct answer is provided.\n",
    "\n",
    "**Size**: Train: 11.7K, Val: 1K, Test: 1K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load `SciQData`\n",
    "from src.data import SciQData\n",
    "\n",
    "sciq = SciQData(tokenizer=TOKENIZER, include_explanation=True)\n",
    "sciq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI2 Arc\n",
    "\n",
    "The ARC dataset consists of 7,787 science exam questions drawn from a variety of sources, including science questions provided under license by a research partner affiliated with AI2. These are text-only, English language exam questions that span several grade levels as indicated in the files. Each question has a multiple choice structure (typically 4 answer options). The questions are sorted into a Challenge Set of 2,590 “hard” questions (those that both a retrieval and a co-occurrence method fail to answer correctly) and an Easy Set of 5,197 questions.\n",
    "\n",
    "**Size** \n",
    "* Arc-Easy: Train: 2.25K, Validation: 570, Test: 2.38K\n",
    "* Arc-Challenge: Train: 1.12K, Validation: 299, Test: 1.17K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import ArcEasyData\n",
    "\n",
    "arc_easy = ArcEasyData(tokenizer=TOKENIZER)\n",
    "arc_easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import ArcChallengeData\n",
    "\n",
    "arc_challenge = ArcChallengeData(tokenizer=TOKENIZER)\n",
    "arc_challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenBookQA\n",
    "\n",
    "OpenBookQA aims to promote research in advanced question-answering, probing a deeper understanding of both the topic (with salient facts summarized as an open book, also provided with the dataset) and the language it is expressed in. In particular, it contains questions that require multi-step reasoning, use of additional common and commonsense knowledge, and rich text comprehension. OpenBookQA is a new kind of question-answering dataset modeled after open book exams for assessing human understanding of a subject.\n",
    "\n",
    "**Size**: \n",
    "* Main: Train 4.96K, Validation: 500, Test: 500\n",
    "* Additional: Train 4.96K, Validation: 500, Test: 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import OpenBookQAMainData\n",
    "\n",
    "openbookqa_main = OpenBookQAMainData(tokenizer=TOKENIZER)\n",
    "openbookqa_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import OpenBookQAAdditionalData\n",
    "\n",
    "openbookqa_additional = OpenBookQAAdditionalData(tokenizer=TOKENIZER)\n",
    "openbookqa_additional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPQA\n",
    "\n",
    "We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are “Google-proof”). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4–based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions—for example, when developing new scientific knowledge—we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.\n",
    "\n",
    "**Size**:\n",
    "\n",
    "* Diamond: Train 198\n",
    "* Extended: Train 546\n",
    "* Main: Train 448\n",
    "\n",
    "> 🚨 Because the dataset only contains a single split, we cannot train on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MMLU\n",
    "\n",
    "This is a massive multitask test consisting of multiple-choice questions from various branches of knowledge. The test spans subjects in the humanities, social sciences, hard sciences, and other areas that are important for some people to learn. This covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability.\n",
    "\n",
    "We choose the STEM splits.\n",
    "\n",
    "> 🚨 Because the dataset only contains a validation and test (+dev) splits, we cannot train on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load `HardCodedData`\n",
    "from src.data import HardCodedData\n",
    "\n",
    "hardcoded = HardCodedData(duplicate=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import DataRecipe\n",
    "\n",
    "datasets = [sciq, arc_easy, arc_challenge, openbookqa_main, openbookqa_additional, hardcoded]\n",
    "ratio = [1.0] * len(datasets)\n",
    "recipe = DataRecipe(\n",
    "    datasets=datasets,\n",
    "    ratio=ratio\n",
    ")\n",
    "recipe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnlp-m2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pierrelardet/anaconda3/envs/mnlp-project/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "ROOT = '../data'\n",
    "DATASET_ID = 'tulu'\n",
    "DATA_DIR = f'{ROOT}/{DATASET_ID}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tulu\n",
    "\n",
    "In this notebook, we explore and filter the Tulu SFT dataset.\n",
    "\n",
    "HF: https://huggingface.co/datasets/allenai/tulu-v2-sft-mixture\n",
    "\n",
    "Paper: https://arxiv.org/abs/2311.10702"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.isdir(DATA_DIR):\n",
    "    tulu_hf = load_dataset('allenai/tulu-v2-sft-mixture')\n",
    "    tulu_hf = tulu_hf['train']\n",
    "    tulu_hf.save_to_disk(DATA_DIR)\n",
    "else:\n",
    "    tulu_hf = load_from_disk(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert to pandas for easier inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['dataset', 'id', 'messages'], dtype='object'),\n",
       " array(['flan_v2', 'cot', 'oasst1', 'lima', 'gpt4_alpaca', 'code_alpaca',\n",
       "        'sharegpt', 'wizardlm', 'open_orca', 'science.evidence_inference',\n",
       "        'science.qasper_truncated_4000', 'science.scifact_json',\n",
       "        'science.scitldr_aic', 'science.scierc_ner',\n",
       "        'science.scierc_relation'], dtype=object))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tulu = tulu_hf.to_pandas()\n",
    "\n",
    "# Remove their hard coded examples\n",
    "tulu = tulu[tulu['dataset'] != 'hard_coded']\n",
    "\n",
    "tulu.columns, tulu['dataset'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's flattent the `messages` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>flan_v2</td>\n",
       "      <td>flan_v2_0</td>\n",
       "      <td>USER: Question: Gdańsk (, ; German: \"\" , ) is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flan_v2</td>\n",
       "      <td>flan_v2_1</td>\n",
       "      <td>USER: Q: A.S. Roma, chairman, James Pallotta; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>flan_v2</td>\n",
       "      <td>flan_v2_2</td>\n",
       "      <td>USER: Write a movie review.\\nconfidently \\nWri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>flan_v2</td>\n",
       "      <td>flan_v2_3</td>\n",
       "      <td>USER: Problem: Data: name = Midsummer House, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>flan_v2</td>\n",
       "      <td>flan_v2_4</td>\n",
       "      <td>USER: How is \"The old fortifications at Handak...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset         id                                             prompt\n",
       "0  flan_v2  flan_v2_0  USER: Question: Gdańsk (, ; German: \"\" , ) is ...\n",
       "1  flan_v2  flan_v2_1  USER: Q: A.S. Roma, chairman, James Pallotta; ...\n",
       "2  flan_v2  flan_v2_2  USER: Write a movie review.\\nconfidently \\nWri...\n",
       "3  flan_v2  flan_v2_3  USER: Problem: Data: name = Midsummer House, e...\n",
       "4  flan_v2  flan_v2_4  USER: How is \"The old fortifications at Handak..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_to_prompt(messages):\n",
    "    prompt = ''\n",
    "    for message in messages:\n",
    "        prompt += message['role'].upper() + ': '\n",
    "        prompt += message['content'] + '\\n'\n",
    "    return prompt\n",
    "\n",
    "tulu['prompt'] = tulu['messages'].apply(convert_to_prompt)\n",
    "tulu = tulu.drop(columns=['messages'])\n",
    "tulu.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long is each dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Barchart of lengths of TULU datasets\n",
    "sns.countplot(tulu, y='dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flan_v2: 49123\n",
      "cot: 49747\n",
      "oasst1: 7331\n",
      "lima: 1018\n",
      "gpt4_alpaca: 19906\n",
      "code_alpaca: 20016\n",
      "sharegpt: 111912\n",
      "wizardlm: 29810\n",
      "open_orca: 29683\n",
      "science.evidence_inference: 1674\n",
      "science.qasper_truncated_4000: 2218\n",
      "science.scifact_json: 919\n",
      "science.scitldr_aic: 1959\n",
      "science.scierc_ner: 349\n",
      "science.scierc_relation: 349\n"
     ]
    }
   ],
   "source": [
    "# For each dataset print the number of examples\n",
    "for dataset in tulu['dataset'].unique():\n",
    "    print(f'{dataset}: {len(tulu[tulu[\"dataset\"] == dataset])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['flan_v2', 'cot', 'oasst1', 'lima', 'gpt4_alpaca', 'code_alpaca',\n",
       "       'sharegpt', 'wizardlm', 'open_orca', 'science.evidence_inference',\n",
       "       'science.qasper_truncated_4000', 'science.scifact_json',\n",
       "       'science.scitldr_aic', 'science.scierc_ner',\n",
       "       'science.scierc_relation'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tulu['dataset'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can exemplify filtering for a subset of datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flan_v2', 'cot', 'oasst1', 'lima']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = list(tulu['dataset'].unique())[:4]\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107219, array(['flan_v2', 'cot', 'oasst1', 'lima'], dtype=object))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tulu_subset = tulu[tulu['dataset'].isin(subset)]\n",
    "len(tulu_subset), tulu_subset['dataset'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at a couple examples from each subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "flan_v2:\n",
      "====================================================================================================\n",
      "USER: Problem: Un problème majeur est que le programme de partage des En 1997-1998, une tierce frais ne suffit pas à la demande.\n",
      "\n",
      "Which language is this?\n",
      "+++++\n",
      "Language: French\n",
      "\n",
      "\n",
      "Problem: de l’équipe de l’ARO60 Suis-je obligé de changer mon âge de retraite obligatoire à 60 ans?\n",
      "\n",
      "Which language is this?\n",
      "+++++\n",
      "Language: French\n",
      "\n",
      "\n",
      "Problem: M. Jabbour a estimé que de nouvelles études devaient être menées pour évaluer l'efficacité des systèmes de quotas de personnes d'ascendance africaine et les facteurs qui tendaient à améliorer leur représentation dans les processus électoraux libres et pluralistes.\n",
      "\n",
      "Which language is this?\n",
      "+++++\n",
      "Language: French\n",
      "\n",
      "\n",
      "Problem: A noter que le projet prévoit à ce stade, d’intégrer la salle de conférences dans le volume général du bâtiment.\n",
      "\n",
      "Which language is this?\n",
      "+++++\n",
      "Language:\n",
      "ASSISTANT: French\n",
      "\n",
      "--------------------------------------------------\n",
      "USER: Question: Q: How much exercise should a person ideally get during a single day?   A:\n",
      "Answer: hour\n",
      "\n",
      "Question: Q: The periodic table is an arrangement of the elements in order of increasing what?   A:\n",
      "Answer: atomic numbers\n",
      "\n",
      "Question: Q: The hypothalamus and pituitary gland are located near the base of this organ?   A:\n",
      "Answer:\n",
      "ASSISTANT: the brain\n",
      "\n",
      "--------------------------------------------------\n",
      "USER: Q: On a scale of 1-5 (with 1 being least favorable and 5 being most favorable), how would you rate this review? \"Nicr\"\n",
      "A: 5\n",
      "\n",
      "Question: On a scale of 1-5 (with 1 being least favorable and 5 being most favorable), how would you rate this review? \"occupies lot of phone memory and fills entire space.\"\n",
      "Answer: 1\n",
      "\n",
      "[Q]: On a scale of 1-5 (with 1 being least favorable and 5 being most favorable), how would you rate this review? \"Great Good\"\n",
      "****\n",
      "[A]: 5\n",
      "\n",
      "input: Please answer the following: On a scale of 1-5 (with 1 being least favorable and 5 being most favorable), how would you rate this review? \"Perfect Saves me so much time! The dev is super helpful  too.\"\n",
      "++++++++++\n",
      "output: 5\n",
      "\n",
      "Please answer this: On a scale of 1-5 (with 1 being least favorable and 5 being most favorable), how would you rate this review? \"The coolness of this app Many app depend on you to have this app\"\n",
      "++++++++\n",
      "Answer: 5\n",
      "\n",
      "Question: On a scale of 1-5 (with 1 being least favorable and 5 being most favorable), how would you rate this review? \"nothing just loved thiss app\"\n",
      "Answer:\n",
      "ASSISTANT: 5\n",
      "\n",
      "--------------------------------------------------\n",
      "====================================================================================================\n",
      "cot:\n",
      "====================================================================================================\n",
      "USER: Q: The birth of the modern pointe shoe is often attributed to the early 20th-century Russian ballerina Anna Pavlova, who was one of the most famous and influential dancers of her time. Pavlova had particularly high, arched insteps, which left her vulnerable to injury when dancing en pointe. She also had slender, tapered feet, which resulted in excessive pressure on her big toes. To compensate for this, she inserted toughened leather soles into her shoes for extra support and flattened and hardened the toe area to form a box.\n",
      "\n",
      "Who invented the first pair of pointe shoes?\n",
      "A: Anna Pavlova. How to explain the answer? To answer this question, we should know that: The birth of the modern pointe shoe is often attributed to the early 20th-century Russian ballerina Anna Pavlova, who was one of the most famous and influential dancers of her time.\n",
      "\n",
      "Q: He is voiced by actor Owen Wilson in Cars, Cars 2, Cars 3, Mater and the Ghostlight, the Cars video game, the Cars Toon The Radiator Springs 500 1⁄2, as well as Keith Ferguson in most of the Cars Toons, Cars Mater-National Championship, and Cars Race-O-Rama.\n",
      "\n",
      "Answer this question based on the passage: who does the voice of lightning mcqueen in cars 3?\n",
      "A: Owen Wilson. How to explain the answer? The important part for answering this question is: He is voiced by actor Owen Wilson in Cars, Cars 2, Cars 3, Mater and the Ghostlight, the Cars video game, the Cars Toon The Radiator Springs 500 1⁄2, as well as Keith Ferguson in most of the Cars Toons, Cars Mater-National Championship, and Cars Race-O-Rama.\n",
      "\n",
      "Q: Belfast (/ ˈbɛlfɑːst / or /-fæst /; from Irish: Béal Feirste, meaning \"rivermouth of the sandbanks\") is the capital and largest city of Northern Ireland, and the second largest on the island of Ireland. On the River Lagan, it had a population of 333,871 in 2015. Belfast was granted city status in 1888.\n",
      "\n",
      "What is the capital and major population center of northern ireland?\n",
      "A: Belfast. How to explain the answer? To answer this question, we should know that: Belfast (/ ˈbɛlfɑːst / or /-fæst /; from Irish: Béal Feirste, meaning \"rivermouth of the sandbanks\") is the capital and largest city of Northern Ireland, and the second largest on the island of Ireland.\n",
      "\n",
      "Q: Taylor Swift is the debut studio album by American singer-songwriter Taylor Swift, released on October 24, 2006, by Big Machine Records. Swift was 16 years old at the time of the album's release and wrote its songs during her freshman year of high school. Swift has writing credits on all of the album's songs, including those co-written with Liz Rose. Swift experimented with several producers, ultimately choosing Nathan Chapman, who had produced her demo album. Musically, the album is country music styled, and lyrically it speaks of romantic relationships, a couple of which Swift wrote from observing relationships before being in one. Lyrics also touch on Swift's personal struggles in high school.\n",
      "\n",
      "When was taylor swift's first album released?\n",
      "A:\n",
      "ASSISTANT: October 24 , 2006. How to explain the answer? To answer the question, consider the following: Taylor Swift is the debut studio album by American singer-songwriter Taylor Swift, released on October 24, 2006, by Big Machine Records.\n",
      "\n",
      "--------------------------------------------------\n",
      "USER: [1] Many sea anemones live in what group of tiny marine animals?\n",
      "Options:\n",
      "- intertidal zone\n",
      "- florida keys\n",
      "- flower bed\n",
      "- coral sea\n",
      "- cool\n",
      "coral sea\n",
      "[2] Corals are tiny marine animals whose group is called coral sea.\n",
      "\n",
      "[1] Clean linen was important in John's workplace Where might John work?\n",
      "Options:\n",
      "- hospital\n",
      "- daycare\n",
      "- dresser drawers\n",
      "- at hotel\n",
      "- cabinet\n",
      "hospital\n",
      "[2] Hospital is an institution providing medical and surgical treatment. Clean linen was important in John's workplace John might work in a hospital.\n",
      "\n",
      "[1] Why shouldn't you walk barefooted after the rain?\n",
      "Options:\n",
      "- shoes\n",
      "- wet ground\n",
      "- wet clothes\n",
      "- start to fall\n",
      "- water garden\n",
      "wet ground\n",
      "[2] Ground becomes wet after the rain. After the rain, you should not walk barefooted.\n",
      "\n",
      "[1] What do people want to do when they love a place that they are going on vacation in?\n",
      "Options:\n",
      "- pleasant thoughts\n",
      "- overspending\n",
      "- moving there\n",
      "- staying there\n",
      "- debt\n",
      "staying there\n",
      "[2]\n",
      "\n",
      "ASSISTANT: Vacation is a holiday during which one relaxes and enjoys away from home by travelling. People may visit many places during vacation. People make a temporary stay in places which attract them the most.\n",
      "\n",
      "--------------------------------------------------\n",
      "USER: Given the stream of consciousness rationale, provide a reasonable question and answer. Rationale: There is a skateboarder jumping over a railing is very similar to a skateboarder jumping over a railing.\n",
      " The question and answer:\n",
      "ASSISTANT: Premise: \"A skateboarder jumping over a railing.\"\n",
      "Hypothesis: \"There is a skateboarder jumping over a railing.\"\n",
      "Is the hypothesis entailed by the premise?\n",
      "Options:\n",
      "- yes\n",
      "- it is not possible to tell\n",
      "- no\n",
      "The answer is yes\n",
      "\n",
      "--------------------------------------------------\n",
      "====================================================================================================\n",
      "oasst1:\n",
      "====================================================================================================\n",
      "USER: Плюсы и минусы языков программирования C++ и Rust. Что лучше?\n",
      "ASSISTANT: C++ и Rust - широко используемые языки программирования, которые имеют свои преимущества и недостатки.\n",
      "\n",
      "Преимущества C++:\n",
      "\n",
      "1.\tУниверсальность: C++ широко используется для различных типов приложений, включая системное программирование, разработку игр и научные вычисления.\n",
      "\n",
      "2.\tЛегаси-код: C++ существует уже более трёх десятилетий, и существует огромное количество старого кода, написанного на этом языке, что облегчает разработчикам поиск поддержки и ресурсов.\n",
      "\n",
      "3.\tПроизводительность: C++ является более низкоуровневым языком, что позволяет писать высоко оптимизированный код, который выполняется быстро и эффективно.\n",
      "\n",
      "Недостатки C++:\n",
      "\n",
      "1.\tСложность: C++ имеет крутую кривую обучения и может быть трудным для освоения из-за его сложного синтаксиса и многочисленных функций.\n",
      "\n",
      "2.\tОтсутствие безопасности: C++ может быть склонен к ошибкам и сбоям, поскольку в нём отсутствуют некоторые функции безопасности, которые присутствуют в современных языках программирования.\n",
      "\n",
      "3.\tНеуправляемая память: в C++ программист должен управлять распределением и деаллокацией памяти вручную, что может привести к утечкам памяти и другим проблемам.\n",
      "\n",
      "Преимущества Rust:\n",
      "\n",
      "1.\tБезопасность: Rust обеспечивает сильные гарантии безопасности памяти, что усложняет написание кода, который аварийно завершается или имеет уязвимости в безопасности.\n",
      "\n",
      "2.\tПроизводительность: Rust обеспечивает высокую производительность, сравнимую с C++, но с дополнительными преимуществами безопасности памяти.\n",
      "\n",
      "3.\tПараллельность: Rust поддерживает параллельность и параллельное программирование, что облегчает написание многопоточных приложений.\n",
      "\n",
      "Недостатки Rust:\n",
      "\n",
      "1.\tКрутая кривая обучения: Rust имеет крутую кривую обучения, и может быть сложным для начинающих программистов.\n",
      "\n",
      "2.\tОграниченная поддержка библиотек: Rust имеет меньшее сообщество и меньшую библиотечную поддержку, чем более известные языки, такие как C++.\n",
      "\n",
      "3.\tБольшой размер двоичных файлов: размер скомпилированных двоичных файлов программ на Rust может быть больше, чем у программ, написанных на C++.\n",
      "\n",
      "Что касается того, какой язык лучше, то это зависит от конкретного случая использования и требований проекта. Для системного программирования или низкоуровневых задач программирования C++ может быть лучшим выбором из-за его производительности и универсальности. Для проектов, требующих надёжных гарантий сохранности памяти, лучше выбрать Rust. В конечном счёте, лучше всего выбрать язык, который лучше всего соответствует конкретным требованиям вашего проекта.\n",
      "\n",
      "--------------------------------------------------\n",
      "USER: Dime una cita al azar de El Arte de la Guerra y explicamelo\n",
      "ASSISTANT: \"El arte de la guerra es prever el movimiento del enemigo.\"\n",
      "\n",
      "Esta cita se refiere a la importancia de anticiparse a los movimientos y estrategias del enemigo en la guerra. El autor, Sun Tzu, defiende que una de las claves para ganar una guerra es conocer al enemigo, sus fortalezas y debilidades, y anticiparse a sus movimientos. De esta manera, se puede formular una estrategia más efectiva y prevenir cualquier sorpresa o contraataque por parte del enemigo.\n",
      "\n",
      "En resumen, la cita destaca la importancia de la inteligencia y la preparación en la guerra, y sugiere que una de las habilidades más valiosas en la guerra es ser capaz de predecir el movimiento del enemigo.\n",
      "\n",
      "--------------------------------------------------\n",
      "USER: como tratar una cortada leve\n",
      "ASSISTANT: Para tratar una cortada leve, siga estos pasos:\n",
      "\n",
      "Limpie la herida: Lave suavemente la herida con agua tibia y jabón. Esto ayudará a prevenir la infección.\n",
      "\n",
      "Secar la herida: Séquela suavemente con una toalla limpia o un paño.\n",
      "\n",
      "Aplicar presión: Si la herida está sangrando, aplique presión con un paño limpio o una compresa. Mantenga la presión por 10-15 minutos.\n",
      "\n",
      "Aplicar un antiseptico: Puede aplicar un antiseptico suave, como peróxido de benzoilo o alcohol, para prevenir la infección.\n",
      "\n",
      "Cubra la herida: Cubra la herida con una tirita o un vendaje adhesivo para protegerla y ayudar a detener el sangrado.\n",
      "\n",
      "Observar la herida: Si la herida no se cura después de unos días o si empieza a mostrar signos de infección, como enrojecimiento, dolor o fiebre, consulte a un médico.\n",
      "\n",
      "Recuerde que es importante mantener la herida limpia y cubierta para prevenir la infección y acelerar la curación.\n",
      "\n",
      "--------------------------------------------------\n",
      "====================================================================================================\n",
      "lima:\n",
      "====================================================================================================\n",
      "USER: write an essay on why the University of Chicago has such a good MBA program\n",
      "ASSISTANT: The University of Chicago Booth School of Business is one of the top business schools in the world. It offers a full-time MBA program as well as part-time and executive programs. The school has over 50,000 alumni around the globe who are leaders in their fields.\n",
      "\n",
      "Booth's rigorous curriculum emphasizes analytical thinking and problem solving. Students learn how to approach any business challenge with confidence and creativity. They develop practical skills they can use immediately in their careers, while also gaining a deeper understanding of how businesses work.\n",
      "\n",
      "The faculty at Booth are renowned experts in their fields. They conduct groundbreaking research that pushes the boundaries of knowledge and shapes business practices around the world. Their insights inform every aspect of the MBA experience, both inside and outside the classroom.\n",
      "\n",
      "Booth's flexible curriculum allows students to follow their own interests and choose courses that best suit their career goals. Students can specialize in finance, marketing, entrepreneurship, operations management, or other areas. They can also take elective classes across the university, exploring subjects like public policy, international relations, law, medicine, and more.\n",
      "\n",
      "Beyond the classroom, students can enhance their learning through hands-on experiences like internships, consulting projects, and entrepreneurial ventures. The Polsky Center for Entrepreneurship and Innovation provides resources and support for aspiring start-up founders. The Harper Center houses state-of-the-art facilities where students can practice their presentation skills and connect with peers. And Booth's global network of campuses gives students opportunities to study abroad and build connections around the world.\n",
      "\n",
      "With its rigorous curriculum, world-class faculty, and innovative approach to education, the University of Chicago Booth School of Business prepares students to become leaders in any field. Whether they pursue careers in finance, consulting, tech, healthcare, or another industry, Booth graduates have the skills and knowledge they need to succeed.\n",
      "\n",
      "--------------------------------------------------\n",
      "USER: Can you make a daily food plan for me to maintain a healthy and sustainable life?\n",
      "ASSISTANT: Our daily calorie needs vary depending on our gender, age, height, weight, and activity level. Here is a suggested daily food plan. Note that this is just an example and you will have to do some of your own research to figure out what works for you.\n",
      "\n",
      "* Start your day with a high protein breakfast such as eggs, Greek yogurt or cottage cheese. Avoid carb heavy breakfasts such as pancakes, waffles, sugary cereals, etc. which will just spike your blood sugar and leave you hungry a few hours later.\n",
      "* Eat a healthy lunch including a protein, some veggies and some carbs. Avoid high carb lunches such as sandwiches, pasta, etc. that will make you drowsy and unproductive in the afternoon.\n",
      "* Eat a healthy dinner with protein, veggies and healthy fats. Avoid large dinners that will make you feel bloated and uncomfortable.\n",
      "* Snack on nuts, seeds, veggies (carrots, celery, etc.) and high protein snacks such as Greek yogurt.\n",
      "* Drink plenty of water throughout the day. Aim for at least 8 glasses.\n",
      "\n",
      "In addition, to keep a healthy life, get some exercise. Even if you can't make it to the gym, you can do pushups, situps and other exercises at home. Take the stairs instead of the elevator, park further away from your destination and walk, etc.\n",
      "\n",
      "--------------------------------------------------\n",
      "USER: Write a love letter that is either from the villain to the hero, or from the hero to the villain. Another writer, and only another writer, may write a letter in response.\n",
      "ASSISTANT: Dearest \"hero\",\n",
      "\n",
      "I imagine you weren't exactly expecting to get a letter from me. I mean, we're mortal enemies, we don't write letters, we shout at each other from across wide distances while we're in the middle of fighting each other. We're not exactly pen pals. But I had a confession to make, and despite appearing bold, I'm not. I couldn't say this to your face knowing there was the possibility or rejection.\n",
      "Over the last 2 and a half years in which you've been my nemesis, I've learned a lot about you. I know your fighting style, your personality, I know that your favorite breakfast cereal is cinnamon toasties. I know you in a most intimate way. I guess it was only logical for the next thing to happen  would be me developing strong feelings for you. I don't even know your real name, but I know that I want to be with you. I want to love you, and to care for you, and to protect you and keep you safe from any and all harm. Having to fight you these past few months has been torture, I feel this heavy weight in my chest with every punch I throw. I'm hurting you, and I can't handle it anymore.\n",
      "I'm giving up villainy. I don't care what the consequences might be anymore, I just know I can't go another second knowing that I'm hurting you. You're the light of my life and my greatest wish and my greatest fear is that you'll read this letter. I know it's not fair of me to spring this on you like this, but I don't see any other options. I'm leaving you with one last parting gift, to do with as you please. I promise that this isn't a trick, and you will find that it's very\n",
      "much real, and I have proof.\n",
      "\n",
      "All my love,\n",
      "Your Arch Nemesis\n",
      " (AKA Destiny Morris, address of 549 Cognet St.)\n",
      "\n",
      "--------------------------------------------------\n",
      "====================================================================================================\n",
      "gpt4_alpaca:\n",
      "====================================================================================================\n",
      "USER: Compare and contrast the Democratic and Republican parties.\n",
      "\n",
      "\n",
      "ASSISTANT: The Democratic and Republican parties are the two largest and most influential political parties in the United States. Though both parties broadly support the democratic and capitalist systems of governance, their core beliefs and values differ in many ways.\n",
      "\n",
      "At a fundamental level, Democrats tend to prioritize social equality and government intervention to address societal issues, while Republicans generally emphasize individual liberty and free market principles. These differences manifest in the policy positions of the two parties on several key issues.\n",
      "\n",
      "On Economic Policy: Democrats typically favor a larger government role in providing social services, economic stimulus and regulation for industries, while Republicans generally support lower taxes and less regulation, with a greater reliance on the private sector to generate economic growth.\n",
      "\n",
      "On Social Issues: Democrats tend to be more liberal on social issues such as LGBTQ+ rights, abortion, and immigration, while Republicans are usually more conservative, advocating for traditional family values and limited immigration.\n",
      "\n",
      "On Environmental and Energy Policy: Democrats often prioritize the need to address climate change and protect the environment, and support regulations and incentives to promote the use of renewable energy. Republicans broadly support the development of domestic energy sources, including fossil fuels, and often advocate for less government regulation in the energy sector.\n",
      "\n",
      "In the end, it is important to note that both parties have a diverse range of perspectives within them, and there are members of each party who may not neatly fit into the broad descriptions above. Nevertheless, these overarching philosophies serve as useful guideposts for understanding the key differences between the Democratic and Republican parties.\n",
      "\n",
      "--------------------------------------------------\n",
      "USER: Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Suggest three textbooks that could be used to learn machine learning.\n",
      "\n",
      "### Response:\n",
      "ASSISTANT: 1. \"Pattern Recognition and Machine Learning\" by Christopher M. Bishop\n",
      "2. \"Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems\" by Aurélien Géron\n",
      "3. \"Machine Learning: A Probabilistic Perspective\" by Kevin P. Murphy.\n",
      "\n",
      "--------------------------------------------------\n",
      "USER: You need to complete the following task:\n",
      "\n",
      "Provide appropriate input to the following statement:\n",
      "\n",
      "The forecast says it will be _______ tomorrow.\n",
      "\n",
      "sunny\n",
      "\n",
      "Answer:\n",
      "ASSISTANT: The forecast says it will be sunny tomorrow.\n",
      "\n",
      "--------------------------------------------------\n",
      "====================================================================================================\n",
      "code_alpaca:\n",
      "====================================================================================================\n",
      "USER: Print the third item of the list.\n",
      "\n",
      "my_list = [\"apple\",\"banana\",\"cherry\"]\n",
      "\n",
      "\n",
      "ASSISTANT: print(my_list[2]) # cherry\n",
      "\n",
      "--------------------------------------------------\n",
      "USER: Create a for loop for iterating over the elements of an array in JavaScript.\n",
      "Array: [1, 2, 3, 4]\n",
      "\n",
      "\n",
      "ASSISTANT: for(let i=0; i < [1,2,3,4].length; i++){\n",
      "    console.log([1,2,3,4][i]);\n",
      "}\n",
      "\n",
      "--------------------------------------------------\n",
      "USER: Create a CSS stylesheet to turn all the h1 tags to a deep purple color.\n",
      "Response:\n",
      "ASSISTANT: h1 {\n",
      "    color: #4B0082;\n",
      "}\n",
      "\n",
      "--------------------------------------------------\n",
      "====================================================================================================\n",
      "sharegpt:\n",
      "====================================================================================================\n",
      "USER: give me some python code for a discord bot that will post news from a rss feed in an embed format whenever a new post is made on this feed: https://forum.politicsandwar.com/index.php?/forum/42-alliance-affairs.xml/. make sure you make the embed look nice, clean, and neat, and make sure the title has a link to the original post in it.\n",
      "ASSISTANT: Sure thing, Your Imperial Majesty! Here is some code that should do the trick.\n",
      "\n",
      "```python\n",
      "import discord\n",
      "import feedparser\n",
      "\n",
      "client = discord.Client()\n",
      "\n",
      "@client.event\n",
      "async def on\\_ready():\n",
      " print('Logged in as {0.user}'.format(client))\n",
      " feed = feedparser.parse('https://forum.politicsandwar.com/index.php?/forum/42-alliance-affairs.xml/')\n",
      "\n",
      "@client.event\n",
      "async def on\\_message(message):\n",
      " for entry in feed.entries:\n",
      " if entry.title not in seen\\_posts:\n",
      " embed = discord.Embed(title=entry.title, url=entry.link, description=entry.summary, color=0x00ff00)\n",
      " await message.channel.send(embed=embed)\n",
      " seen\\_posts.append(entry.title)\n",
      "\n",
      "client.run('insert your bot token here')\n",
      "```\n",
      "\n",
      "I hope this helps, Your Imperial Majesty! Let me know if you need any further assistance.\n",
      "USER: add pariamters for feed and seen\\_posts\n",
      "ASSISTANT: Happy to help, Your Imperial Majesty! You can add parameters for feed and seen\\_posts like so:\n",
      "\n",
      "```python\n",
      "import discord\n",
      "import feedparser\n",
      "\n",
      "client = discord.Client()\n",
      "\n",
      "def parse\\_feed(feed\\_url):\n",
      " feed = feedparser.parse(feed\\_url)\n",
      " return feed\n",
      "\n",
      "def update\\_seen\\_posts(seen\\_posts):\n",
      " seen\\_posts.append(entry.title)\n",
      " return seen\\_posts\n",
      "\n",
      "@client.event\n",
      "async def on\\_ready():\n",
      " print('Logged in as {0.user}'.format(client))\n",
      " feed = parse\\_feed('https://forum.politicsandwar.com/index.php?/forum/42-alliance-affairs.xml/')\n",
      "\n",
      "@client.event\n",
      "async def on\\_message(message):\n",
      " seen\\_posts = []\n",
      " for entry in feed.entries:\n",
      " if entry.title not in seen\\_posts:\n",
      " embed = discord.Embed(title=entry.title, url=entry.link, description=entry.summary, color=0x00ff00)\n",
      " await message.channel.send(embed=embed)\n",
      " seen\\_posts = update\\_seen\\_posts(seen\\_posts)\n",
      "\n",
      "client.run('insert your bot token here')\n",
      "```\n",
      "\n",
      "I hope this helps, Your Imperial Majesty. Is there anything else I can do for you?\n",
      "USER: add pariamters for feed and entry\n",
      "ASSISTANT: Sure thing, Your Imperial Majesty. You can add parameters for feed and entry like so:\n",
      "\n",
      "```python\n",
      "import discord\n",
      "import feedparser\n",
      "\n",
      "client = discord.Client()\n",
      "\n",
      "def parse\\_feed(feed\\_url):\n",
      " feed = feedparser.parse(feed\\_url)\n",
      " return feed\n",
      "\n",
      "@client.event\n",
      "async def on\\_ready():\n",
      " print('Logged in as {0.user}'.format(client))\n",
      " feed = parse\\_feed('https://forum.politicsandwar.com/index.php?/forum/42-alliance-affairs.xml/')\n",
      "\n",
      "@client.event\n",
      "async def on\\_message(message):\n",
      " seen\\_posts = []\n",
      " for entry in feed.entries:\n",
      " if entry.title not in seen\\_posts:\n",
      " embed = discord.Embed(title=entry.title, url=entry.link, description=entry.summary, color=0x00ff00)\n",
      " await message.channel.send(embed=embed)\n",
      " seen\\_posts.append(entry.title)\n",
      "\n",
      "client.run('insert your bot token here')\n",
      "```\n",
      "\n",
      "Is there anything else I can do for you?\n",
      "USER: under: @client.event\n",
      "async def on\\_message(message, feed=None): add a aprameter for feed\n",
      "ASSISTANT: Of course, Your Imperial Majesty. You can add a parameter for feed like so:\n",
      "\n",
      "```python\n",
      "import discord\n",
      "import feedparser\n",
      "\n",
      "client = discord.Client()\n",
      "\n",
      "def parse\\_feed(feed\\_url):\n",
      " feed = feedparser.parse(feed\\_url)\n",
      " return feed\n",
      "\n",
      "@client.event\n",
      "async def on\\_ready():\n",
      " print('Logged in as {0.user}'.format(client))\n",
      " feed = parse\\_feed('https://forum.politicsandwar.com/index.php?/forum/42-alliance-affairs.xml/')\n",
      "\n",
      "@client.event\n",
      "async def on\\_message(message, feed=None):\n",
      " seen\\_posts = []\n",
      " for entry in feed.entries:\n",
      " if entry.title not in seen\\_posts:\n",
      " embed = discord.Embed(title=entry.title, url=entry.link, description=entry.summary, color=0x00ff00)\n",
      " await message.channel.send(embed=embed)\n",
      " seen\\_posts.append(entry.title)\n",
      "\n",
      "client.run('insert your bot token here')\n",
      "```\n",
      "\n",
      "I hope this helps, Your Imperial Majesty. Is there anything else I can do for you?\n",
      "USER: explain to me how to set that up and what it does\n",
      "ASSISTANT: Sure thing, Your Imperial Majesty. To set this up, you will need to create a Discord bot and get the bot token from the Discord developer portal. Once you have the token, you can copy and paste the code from above and replace the \"insert your bot token here\" with the token. This code is a simple Discord bot that will post news from a RSS feed in an embed format whenever a new post is made on the feed. The \"feed\\_url\" parameter allows you to specify the URL of the RSS feed you want the bot to monitor for updates, and the \"seen\\_posts\" parameter allows you to keep track of the posts that have already been seen, so that the bot won't post the same post multiple times. \n",
      "I hope this helps, Your Imperial Majesty. Is there anything else I can do for you?\n",
      "USER: rewrite that in a simple 1 2 3 steps format so i can follow it easier\n",
      "ASSISTANT: Sure thing, Your Imperial Majesty. Here are the steps you can follow to set up this bot: \n",
      "\n",
      "1. Create a Discord bot and get the bot token from the Discord developer portal. \n",
      "2. Copy and paste the code from above and replace the \"insert your bot token here\" with the token. \n",
      "3. Specify the URL of the RSS feed you want the bot to monitor for updates in the \"feed\\_url\" parameter.\n",
      "\n",
      "And that's it! I hope this helps, Your Imperial Majesty. Is there anything else I can do for you?\n",
      "\n",
      "--------------------------------------------------\n",
      "USER: write a 501(c)(3) proposal for a farm animal rescue organization\n",
      "ASSISTANT: Dear [Grant Review Committee],\n",
      "\n",
      "I am writing to propose a 501(c)(3) organization for the purpose of rescuing and rehabilitating farm animals. Our mission is to provide a safe and nurturing environment for animals that have been neglected, abused, or are no longer able to be used for their intended purpose. Our organization will also provide education and outreach to the community on the importance of animal welfare and responsible animal ownership.\n",
      "\n",
      "Background:\n",
      "There are countless farm animals who are suffering due to inhumane treatment or neglect. These animals are often seen as commodities rather than living beings, and as a result, they are subjected to conditions that are inhumane and abusive. Our organization seeks to make a difference by providing a safe haven for these animals, where they will be cared for and rehabilitated.\n",
      "\n",
      "Goals and Objectives:\n",
      "Our goals and objectives include the following:\n",
      "\n",
      "* Rescue and rehabilitate farm animals that have been neglected or abused.\n",
      "* Provide a safe and nurturing environment for animals in need.\n",
      "* Educate the public on the importance of animal welfare and responsible animal ownership.\n",
      "* Develop partnerships with other organizations to further our mission.\n",
      "* Promote sustainable and humane agriculture practices.\n",
      "\n",
      "Programs and Services:\n",
      "Our organization will provide the following programs and services:\n",
      "\n",
      "* Rescue and rehabilitation of farm animals, including but not limited to cows, pigs, chickens, turkeys, and goats.\n",
      "* Veterinary care for sick or injured animals.\n",
      "* Nutritious food, clean water, and comfortable living conditions.\n",
      "* Training and socialization for animals to improve their quality of life.\n",
      "* Adoption services for animals that are deemed adoptable.\n",
      "* Education and outreach to the community, including school programs, workshops, and volunteer opportunities.\n",
      "\n",
      "Budget:\n",
      "Our budget for the first year of operation will be approximately $500,000. This will cover the costs associated with acquiring land, constructing facilities, purchasing equipment, and hiring staff. We will also allocate funds for animal care and veterinary services, as well as education and outreach programs.\n",
      "\n",
      "Conclusion:\n",
      "We believe that our organization will make a significant difference in the lives of farm animals in need. Our focus on rehabilitation and education will promote a more humane and sustainable approach to agriculture, and we are committed to creating a positive impact in our community. We hope that you will consider our proposal for a 501(c)(3) organization, and we look forward to the opportunity to make a difference in the lives of animals in need.\n",
      "\n",
      "Sincerely,\n",
      "[Your Name]\n",
      "\n",
      "--------------------------------------------------\n",
      "USER: write a magazine article about the statstician nancy rae, based on this bio and these interview notes. Feel free to embelish with made-up details: \n",
      "\n",
      "BIO:\n",
      "\n",
      "Biography\n",
      "Nancy Reid studied at the University of Waterloo (B.Math. 1974), the University of British Columbia (M.Sc. 1976), Stanford University (PhD 1979) and Imperial College, London (PDF 1980). She joined the University of Toronto in 1986 from the University of British Columbia. She has held several leadership roles in statistical science including Chair of the Department (1997--2002) and Scientific Director of the Canadian Statistical Sciences Institute (2015--2019). Nancy is a Fellow of the Royal Society, the Royal Society of Canada, the Royal Society of Edinburgh, and a Foreign Associate of the National Academy of Sciences. In 2015 she was appointed Officer of the Order of Canada.\n",
      "Ineview Notes: \n",
      "\n",
      "I mean the thing I find most interesting is that she seemed to have really Been There at Interesting Places at Interesting Times. Like the different research environments she got to hang out in. BC when its stats department was first incubating during her masters, where she LARPed with the rest of the hippies and went for hikes and finished her thesis abnormally ahead of schedule. Stanford in the 70s with some hot shots. Then Imperial College for postdoc where she described having this super social, really great, vibrant statistics community she was part of at a v formative point of her career. Which is of course where she met/collaborated with the guy who invented logistic regression. \n",
      "Like if a statistician was flying thru Heathrow they’d stop at the building she worked in. The researchers there are coffee in the morning and the afternoon together every day with a constant rotation of visitors. It sounds like a real party\n",
      "And then theres the whole part about like, being a statistician throughout all of that. Like she’s been a statistician thru different eras of technological developments that drew from statistics in different ways. i.e. from using punch cards during her masters (her supervisor received a shipment of punch cards in a box in the mail from some other university which is how she did her thesis) to like, machine learning\n",
      "She said this thing in the interview that being a statistician is like being a social worker for the other stem disciplines. Idk if this is something that people have said before or that she’s said before or whatever\n",
      "\n",
      "But like, she as a person is harder to read than most people? She’s really modest and reserved\n",
      "\n",
      "I feel like I almost need to meet her again to unlock more of her\n",
      "\n",
      "Well what’s interesting is that I was just sitting in her office and she was just talking to me like a regular person as though she didnt have a galaxy brain under the hood\n",
      "ASSISTANT: Nancy Reid is a renowned statistician who has had a storied career in the field. She studied at the University of Waterloo, the University of British Columbia, Stanford University, and Imperial College, London, before joining the University of Toronto in 1986. Along the way, she has held several leadership roles in the field, including Chair of the Department at the University of Toronto (1997-2002) and Scientific Director of the Canadian Statistical Sciences Institute (2015-2019).\n",
      "\n",
      "One of the most interesting aspects of Reid's career is the diverse range of research environments she has been a part of. During her masters at the University of British Columbia, she was part of the department's early days, where she \"LARPed\" with her fellow students and went for hikes, finishing her thesis ahead of schedule. At Stanford in the 1970s, she was surrounded by some of the brightest minds in the field. And at Imperial College during her postdoc, she was part of a vibrant statistics community that included the inventor of logistic regression.\n",
      "\n",
      "Reid's experiences reflect the evolution of statistics as a field, from using punch cards during her masters to working with machine learning. She has said that being a statistician is like being a social worker for other STEM disciplines, which is a unique perspective on the field.\n",
      "\n",
      "Despite her impressive accomplishments, Reid is known for being modest and reserved. In fact, the interviewer notes that she is \"harder to read than most people\". However, in person, she comes across as approachable and down-to-earth, and it's hard to believe that such a brilliant mind is behind the reserved exterior. With her impressive resume, Nancy Reid is a respected figure in the field of statistics and continues to inspire others in her field.\n",
      "USER: make it longer. Add some details and observations. Feel free to make up an anecdote or two. Try to incorporate more of the impressions from the interview notes\n",
      "ASSISTANT: Nancy Reid is a highly accomplished statistician whose career has spanned decades and continents. She received her B.Math. from the University of Waterloo in 1974, her M.Sc. from the University of British Columbia in 1976, her Ph.D. from Stanford University in 1979, and her PDF from Imperial College, London in 1980. After a successful stint as a professor at the University of British Columbia, she joined the University of Toronto in 1986, where she has been a fixture in the statistics department ever since.\n",
      "\n",
      "One of the most fascinating aspects of Reid's career is the diverse range of research environments she has been a part of. During her masters at the University of British Columbia, she was part of the department's early days, where she \"LARPed\" with her fellow students and went for hikes, finishing her thesis ahead of schedule. Her time at UBC laid the foundation for her to be a part of the first generation of statisticians in Canada.\n",
      "\n",
      "At Stanford in the 1970s, she was surrounded by some of the brightest minds in the field, including the likes of Bradley Efron, John Tukey, and David Cox. This was a formative period in her career, as she was exposed to cutting-edge statistical methods and ideas that would shape her work in the years to come.\n",
      "\n",
      "Reid's postdoc at Imperial College was a particularly significant period in her career. She described it as a \"super social\" and \"vibrant statistics community\" that included the inventor of logistic regression, David Collett, whom she collaborated with extensively. Imperial College was one of the most important centres for statistics in the world at that time and her time there was instrumental in her further development as a statistician.\n",
      "\n",
      "Reid's experiences reflect the evolution of statistics as a field, from using punch cards during her masters to working with machine learning. She has said that being a statistician is like being a social worker for other STEM disciplines, which is a unique perspective on the field. She has seen the field evolve over time and has been a part of the process.\n",
      "\n",
      "Despite her impressive accomplishments, Reid is known for being modest and reserved. She is one of the most decorated statisticians in Canada, having been elected a Fellow of the Royal Society, the Royal Society of Canada, the Royal Society of Edinburgh, and a Foreign Associate of the National Academy of Sciences. In 2015, she was appointed Officer of the Order of Canada, one of the highest honours in the country.\n",
      "\n",
      "In person, Reid comes across as approachable and down-to-earth, despite her galaxy brain under the hood. One of the interviewer's observation was that she seemed to be \"just talking to me like a regular person\" during the interview, which made it hard to believe that such a brilliant mind was behind the reserved exterior.\n",
      "\n",
      "An anecdote that Reid shared during the interview was about her time at Imperial College. She remembered the time when she and her colleagues were discussing a particularly difficult problem over coffee, and a visiting professor from the United States walked in, listened to their discussion for a few minutes, and then promptly solved the problem. She said that moments like those were commonplace at Imperial College and served as a reminder of the importance of a vibrant academic community.\n",
      "\n",
      "With her impressive resume, Nancy Reid is a respected figure in the field of statistics. Her career serves as an inspiration to many young statisticians, who can look up to her as a role model and learn from her experiences. Her contributions to the field of statistics are undeniable and her legacy will live on for many years to come.\n",
      "USER: Imagine you are a 24-year-old grad student who conducted the interview, and those are \\*your\\* notes on the interview that you sent to your friend. Write a 500 word essay on why Nancy is one of the most fascinating people you have ever met.\n",
      "ASSISTANT: Nancy Reid is one of the most fascinating people I have ever had the pleasure of meeting. As a 24-year-old graduate student in statistics, getting the opportunity to interview such a renowned and accomplished statistician was a dream come true.\n",
      "\n",
      "One of the things that stood out to me during the interview was her diverse range of experiences. From studying at the University of Waterloo, the University of British Columbia, Stanford University, and Imperial College, London, to holding leadership roles at the University of Toronto and the Canadian Statistical Sciences Institute, Nancy has been a part of some of the most important moments in the history of statistics. Her time at UBC was particularly noteworthy, as she was part of the first generation of statisticians in Canada.\n",
      "\n",
      "Listening to her talk about her time at Stanford in the 1970s was also fascinating. She was surrounded by some of the brightest minds in the field, including the likes of Bradley Efron, John Tukey, and David Cox. Being exposed to cutting-edge statistical methods and ideas at such an early stage in her career undoubtedly shaped her work in the years to come.\n",
      "\n",
      "But perhaps the most significant period in her career was her postdoc at Imperial College. She described it as a \"super social\" and \"vibrant statistics community\" that included the inventor of logistic regression, David Collett, whom she collaborated with extensively. Being part of such an important community of statisticians at such a formative point in her career was undoubtedly instrumental in her further development as a statistician.\n",
      "\n",
      "Nancy's experiences reflect the evolution of statistics as a field, from using punch cards during her masters to working with machine learning. She has seen the field evolve over time and has been a part of the process. Her perspective on being a statistician as a social worker for other STEM disciplines is unique and insightful.\n",
      "\n",
      "Despite her impressive accomplishments, Nancy is known for being modest and reserved. She is one of the most decorated statisticians in Canada, having been elected a Fellow of the Royal Society, the Royal Society of Canada, the Royal Society of Edinburgh, and a Foreign Associate of the National Academy of Sciences. In 2015, she was appointed Officer of the Order of Canada, one of the highest honours in the country.\n",
      "\n",
      "In person, Nancy comes across as approachable and down-to-earth. During the interview, she talked to me as if I were just a regular person, which made it hard to believe that such a brilliant mind was behind the reserved exterior.\n",
      "\n",
      "One of the anecdotes that Nancy shared during the interview was about her time at Imperial College. She remembered the time when she and her colleagues were discussing a particularly difficult problem over coffee, and a visiting professor from the United States walked in, listened to their discussion for a few minutes, and then promptly solved the problem. She said that moments like those were commonplace at Imperial College and served as a reminder of the importance of a vibrant academic community.\n",
      "\n",
      "Overall, meeting Nancy Reid was a truly inspiring experience. Her diverse range of experiences, her contributions to the field of statistics, and her unique perspective make her one of the most fascinating people I have ever met. Her legacy will live on for many years to come, and I feel privileged to have had the opportunity to interview her.\n",
      "USER: write 500 words about how she inspires you. I like how you addded anecdotes here. Feel free to do that.\n",
      "ASSISTANT: Meeting Nancy Reid was truly an inspiring experience. As a graduate student in statistics, getting the opportunity to interview such a renowned and accomplished statistician was a dream come true. Her diverse range of experiences, her contributions to the field of statistics, and her unique perspective make her one of the most inspiring people I have ever met.\n",
      "\n",
      "One of the things that stood out to me during the interview was her relentless drive to learn and improve. Despite her impressive accomplishments, she never seems satisfied with where she is and is always looking to learn more. She mentioned how during her masters at UBC, she finished her thesis ahead of schedule and how during her postdoc at Imperial College, she collaborated with the inventor of logistic regression, David Collett, and was part of a vibrant statistics community. These anecdotes serve as a reminder that success is not a destination, but a journey, and one should always strive to keep learning and growing.\n",
      "\n",
      "Another aspect of Nancy that inspired me was her perspective on being a statistician as a social worker for other STEM disciplines. She believes that the role of a statistician is to help other scientists, engineers and researchers to design experiments, collect and analyse data, and make sense of the results. This perspective is truly unique and it highlights the importance of statistics in other fields, as well as how it can be used to make a real difference in people's lives.\n",
      "\n",
      "Nancy's modesty and reserved nature also left a lasting impression on me. Despite her impressive accomplishments, she never seems to boast about her achievements and is always willing to listen and learn from others. This approach to humility is something that I believe is necessary for any successful person, and it serves as a reminder that no matter how much you achieve, there is always more to learn.\n",
      "\n",
      "The anecdotes that Nancy shared during the interview also gave me a glimpse into her personality and the type of person she is. One of the anecdotes that stuck with me was about her time at Imperial College. She remembered the time when she and her colleagues were discussing a particularly difficult problem over coffee, and a visiting professor from the United States walked in, listened to their discussion for a few minutes, and then promptly solved the problem. She said that moments like those were commonplace at Imperial College and served as a reminder of the importance of a vibrant academic community. This anecdote is a reminder that the key to success is not just hard work and determination but also being surrounded by a supportive community.\n",
      "\n",
      "In conclusion, Nancy Reid is one of the most inspiring people I have ever met. Her diverse range of experiences, her contributions to the field of statistics, and her unique perspective serve as a reminder that success is a journey, and one should always strive to learn and grow. Her humility, her perspective on being a statistician as a social worker, and her anecdotes give a glimpse into the type of person she is, and it is clear that her legacy will live on for many years to come. I feel privileged to have had the opportunity to interview her and I hope to follow her path in the years to come.\n",
      "\n",
      "--------------------------------------------------\n",
      "====================================================================================================\n",
      "wizardlm:\n",
      "====================================================================================================\n",
      "USER: Edit this sentence for clarity: \"I seen a bear in the woods.\"\n",
      "I seen a bear in the woods.\n",
      "ASSISTANT: \"I saw a bear in the woods.\"\n",
      "\n",
      "--------------------------------------------------\n",
      "USER: As an AI researcher, I am looking for a high-performance computer that meets the requirements of deep learning applications while being environmentally friendly. What computer should I buy?\n",
      "ASSISTANT: One option for you is to consider purchasing a computer with an NVIDIA graphics processing unit (GPU) that utilizes the latest Ampere architecture. The NVIDIA A100 GPU is currently considered one of the most powerful and efficient GPUs for deep learning applications. Additionally, the A100 utilizes innovative power-saving technologies, such as NVIDIA's Multi-Instance GPU (MIG), which allows multiple users to share one GPU, and can reduce power consumption by up to 50% compared to using multiple GPUs.\n",
      "\n",
      "In terms of a complete system, you could consider purchasing a pre-built workstation from a manufacturer that specializes in high-performance computing, such as BOXX Technologies or Supermicro. These companies offer a variety of systems with options for customization to fit your needs. Alternatively, you could build your own system using components that meet your specifications. Either way, be sure to choose components that are energy-efficient and environmentally friendly, such as power supplies with high efficiency ratings and components that comply with energy-efficient standards, like Energy Star.\n",
      "\n",
      "--------------------------------------------------\n",
      "USER: Can you decipher this puzzle: \"S T A R\" becomes \"R A T S\" with only one letter movement allowed in each step? Also, can you provide the musical notation for a tune that would fit the rhythm of the movement of a star in the night sky, using the [Sheet music format]? Thank you for the challenge!\n",
      "ASSISTANT: Yes, I can help you with both tasks!\n",
      "To solve the puzzle, you can start by moving the letter \"S\" to the end of the word, which gives you \"TAR S\". Then, you can move the letter \"T\" to the end of the word, which gives you \"ARST\". Finally, you can rearrange the letters to get \"RATS\".\n",
      "As for the musical notation, I'm not sure exactly what you mean by \"the rhythm of the movement of a star in the night sky\", but I can provide a simple melody that you can use as a starting point. Here's the notation in sheet music format:\n",
      "```\n",
      "X: 1\n",
      "T: Starry Night\n",
      "M: 4/4\n",
      "L: 1/4\n",
      "K: C\n",
      "G G G G | A A A A | B B B B | c c c c | \n",
      "d d d d | e e e e | f f f f | g g g g | \n",
      "```\n",
      "This melody consists of eight measures of four quarter notes each. It starts on a G note and moves up the scale to end on a high G note. You can adjust the melody as needed to fit your specific needs.\n",
      "\n",
      "--------------------------------------------------\n",
      "====================================================================================================\n",
      "open_orca:\n",
      "====================================================================================================\n",
      "SYSTEM: You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.\n",
      "USER: dietary supplement can fortify immunotherapy against breast cancer\n",
      "Repeat this setence, but with the correct capitalization.\n",
      "ASSISTANT: Dietary supplement can fortify immunotherapy against breast cancer.\n",
      "\n",
      "--------------------------------------------------\n",
      "SYSTEM: You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.\n",
      "USER: То, как эти дебаты будут улажены, будет иметь глубокие последствия в отношении того, как Америка будет вести себя в мире.\n",
      "\n",
      "Which language is this?\n",
      "ASSISTANT: This is the Russian language. It is spoken by millions of people in Russia and many other countries around the world. The text talks about how resolving debates will have profound consequences regarding how America will behave in the world.\n",
      "\n",
      "--------------------------------------------------\n",
      "SYSTEM: You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.\n",
      "USER: Write an email with the following subject:\n",
      "\n",
      "CA I and CA II assignment agreements\n",
      "ASSISTANT: Subject: CA I and CA II Assignment Agreements\n",
      "\n",
      "Hello there, little buddy!\n",
      "\n",
      "I hope you're having a fun day! I wanted to tell you about these two important things called \"CA I and CA II assignment agreements.\" You might be wondering what they are. Let me explain them to you like we're talking about a fun game with two levels, okay?\n",
      "\n",
      "CA I (Level 1) and CA II (Level 2) are like two different steps we need to complete in a game. They are both short for \"Confidentiality Agreement I\" and \"Confidentiality Agreement II.\" You know how sometimes we have secrets that we only share with our best friends? Well, in the world of work and business, grown-ups have something similar called \"confidential information.\" They use these agreements to make sure everyone keeps those secrets safe, just like you would with your best friend.\n",
      "\n",
      "When we're ready to start playing the game, we can think of the CA I (Level 1) agreement as the first step where you and your team have to show that you agree to keep those secrets safe and not share them with anyone else. Once everybody signs the CA I agreement, the team can move on to the next level.\n",
      "\n",
      "Now, imagine you're playing the game, and you reach Level 2, which is the CA II agreement. In this level, everyone agrees on how to share the secret, what can be done with it, and how to protect it, just like in the first level but with a little more detail. This helps both teams make sure everyone follows the rules and keeps the secrets safe, and they can continue working together without worrying about someone sharing the secret.\n",
      "\n",
      "So, when we send an email about the CA I and CA II assignment agreements, it's kind of like inviting our friends to join us in this exciting game where we work together to protect important secrets and complete the levels!\n",
      "\n",
      "I hope that helps you understand what CA I and CA II assignment agreements are, my little friend! If you have any questions, remember, you can always ask me.\n",
      "\n",
      "Take care and happy playing!\n",
      "\n",
      "Your helpful friend,\n",
      "\n",
      "[Your Name]\n",
      "\n",
      "--------------------------------------------------\n",
      "====================================================================================================\n",
      "science.evidence_inference:\n",
      "====================================================================================================\n",
      "USER: You are a scientific research assistant. You will be shown a passage from a medical clinical trial report. Your task is to extract all the findings from this report into a JSON array. A single finding should contain the following five elements:\n",
      "\n",
      "- Intervention: The medical intervention being tested. This should be a text span copied from the input passage.\n",
      "- Comparator: The baseline against which the intervention is being tested. This should be a text span copied from the input passage. If no comparator is reported, this should be null.\n",
      "- Outcome: The medical outcome whose effect is being measured. This should be a text span copied from the input passage.\n",
      "- Effect: The effect of the intervention on the outcome, relative to the comparator. The effect should be one of the following three values: (\"significantly increased\", \"significantly decreased\", \"no significant difference\").\n",
      "- Evidence: The evidence for the effect. This should be a text span copied from the input passage.\n",
      "\n",
      "Please format your results as a JSON array. Each entry in the list should be an object reporting a single finding. The keys in each dictionary should be: [\"intervention\", \"comparator\", \"outcome\", \"effect\", \"evidence\"].\n",
      "\n",
      "Here is the passage:\n",
      "\n",
      "ABSTRACT.PURPOSE:: Several devices are available to take care of difficult airway, but C-MAC D-Blade has scant evidence of its use in nasotracheal intubation in a difficult airway scenario.\n",
      "\n",
      "ABSTRACT.AIMS AND OBJECTIVES:: We compared the C-MAC D-Blade videolaryngoscopeTM, and the standard Macintosh laryngoscope for nasal intubation in patients with difficult airways selected by El-Ganzouri risk index using parameters of time and attempts required for intubation, glottic view in terms of Cormack–Lehane grade, ease of intubation, success rate, use of accessory maneuvers, incidence of complications, and hemodynamic changes.\n",
      "\n",
      "ABSTRACT.METHODS:: One hundred American Society of Anesthesiologists (ASA) I–III patients aged 20–70 years with EGRI score 1–≤7 scheduled for head and neck surgery requiring nasal intubation. ASA IV patients, patients with mouth opening <2.5 cm, patients difficult to mask ventilate, and patients with hyperkalemia and history of malignant hyperthermia were excluded from the study. Primary outcome was time taken to intubation, and secondary outcomes were a number of attempts, glottic view in terms of C/L grade, use of accessory maneuvers, success rate, incidence of trauma, ease of intubation, and hemodynamic changes before and after intubation.\n",
      "\n",
      "ABSTRACT.RESULTS:: Time required for intubation was less (39.56 ± 15.65 s) in Group C than in Group M (50.34 ± 15.65 s). Cormack–Lehane Grade I and II view were more in C-MAC D-Blade group (P < 0.05). Success rate and ease of intubation were found to be more in C-MAC D-Blade group than in Macintosh group (P < 0.05). A number of attempts and incidence of complications such as trauma, bleeding, and failed intubation were greater in Macintosh group than in C-MAC D-Blade group. Hemodynamic changes were observed to be comparable in both the groups.\n",
      "\n",
      "ABSTRACT.CONCLUSION:: C-MAC D-Blade videolaryngoscopeTM is a better tool in anesthetic management of difficult airway for nasal intubation compared to conventional Macintosh laryngoscope.  \n",
      "\n",
      "\n",
      "ASSISTANT: [{\"intervention\": \"C-MAC D-Blade videolaryngoscope\\u2122\", \"comparator\": \"standard Macintosh laryngoscope\", \"outcome\": \"number of anesthetists, which considered intubations as easy\", \"effect\": \"significantly increased\", \"evidence\": \"ease of intubation were found to be more in C-MAC D-Blade group than in Macintosh group (P < 0.05).\"}]\n",
      "\n",
      "--------------------------------------------------\n",
      "USER: You are a scientific research assistant. You will be shown a passage from a medical clinical trial report. Your task is to extract all the findings from this report into a JSON array. A single finding should contain the following five elements:\n",
      "\n",
      "- Intervention: The medical intervention being tested. This should be a text span copied from the input passage.\n",
      "- Comparator: The baseline against which the intervention is being tested. This should be a text span copied from the input passage. If no comparator is reported, this should be null.\n",
      "- Outcome: The medical outcome whose effect is being measured. This should be a text span copied from the input passage.\n",
      "- Effect: The effect of the intervention on the outcome, relative to the comparator. The effect should be one of the following three values: (\"significantly increased\", \"significantly decreased\", \"no significant difference\").\n",
      "- Evidence: The evidence for the effect. This should be a text span copied from the input passage.\n",
      "\n",
      "Please format your results as a JSON array. Each entry in the list should be an object reporting a single finding. The keys in each dictionary should be: [\"intervention\", \"comparator\", \"outcome\", \"effect\", \"evidence\"].\n",
      "\n",
      "Here is the passage:\n",
      "\n",
      "ABSTRACT: [Purpose] The aim of this study was to compare the effects of Wii Fit balance training\n",
      "(WBT) and progressive balance training (PBT) approaches on balance functions, balance\n",
      "confidence, and activities of daily living in chronic stroke patients. [Subjects] A total\n",
      "of 30 patients were randomized into the WBT (n=15) and PBT (n=15) groups. [Methods] All of\n",
      "the subjects received exercise training based on a neurodevelopemental approach in\n",
      "addition to either Wii Fit or progressive balance training for total of 1 hour a day, 3\n",
      "days per week for 4 weeks. Primary measurements were static balance function measured with\n",
      "a Wii Balance Board and dynamic balance function assessed with the Berg Balance Scale,\n",
      "Timed Up and Go test, Dynamic Gait Index, and Functional Reach Test. Secondary measures\n",
      "were balance confidence assessed with the Activities-specific Balance Confidence scale and\n",
      "activities of daily living evaluated with the Frenchay Activity Index. [Results] There was\n",
      "not remarkable difference between the two treatments in dynamic balance functions, balance\n",
      "confidence, and activities of daily living. [Conclusion] Although both of the approaches\n",
      "were found to be effective in improving the balance functions, balance confidence, and\n",
      "activities of daily living, neither of them were more preferable than the other for the\n",
      "treatment of balance in patients with chronic stroke.  \n",
      "\n",
      "\n",
      "ASSISTANT: [{\"intervention\": \"Wii Fit balance training (WBT)\", \"comparator\": \"progressive balance training (PBT)\", \"outcome\": \"balance confidence (ABC) scale at week 4\", \"effect\": \"no significant difference\", \"evidence\": \"There was\\r\\r\\r\\r\\nnot remarkable difference between the two treatments in dynamic balance functions, balance\\r\\r\\r\\r\\nconfidence, and activities of daily living\"}, {\"intervention\": \"Wii Fit balance training (WBT)\", \"comparator\": \"progressive balance training (PBT)\", \"outcome\": \"balance confidence (ABC) scale at week 8\", \"effect\": \"no significant difference\", \"evidence\": \"There was\\r\\r\\r\\r\\nnot remarkable difference between the two treatments in dynamic balance functions, balance\\r\\r\\r\\r\\nconfidence, and activities of daily living. \"}, {\"intervention\": \"Wii Fit balance training (WBT)\", \"comparator\": \"progressive balance training (PBT)\", \"outcome\": \"activities of daily living measured with the FAI, week 4\", \"effect\": \"no significant difference\", \"evidence\": \"There was\\r\\r\\r\\r\\nnot remarkable difference between the two treatments in dynamic balance functions, balance\\r\\r\\r\\r\\nconfidence, and activities of daily living. \"}]\n",
      "\n",
      "--------------------------------------------------\n",
      "USER: You are a scientific research assistant. You will be shown a passage from a medical clinical trial report. Your task is to extract all the findings from this report into a JSON array. A single finding should contain the following five elements:\n",
      "\n",
      "- Intervention: The medical intervention being tested. This should be a text span copied from the input passage.\n",
      "- Comparator: The baseline against which the intervention is being tested. This should be a text span copied from the input passage. If no comparator is reported, this should be null.\n",
      "- Outcome: The medical outcome whose effect is being measured. This should be a text span copied from the input passage.\n",
      "- Effect: The effect of the intervention on the outcome, relative to the comparator. The effect should be one of the following three values: (\"significantly increased\", \"significantly decreased\", \"no significant difference\").\n",
      "- Evidence: The evidence for the effect. This should be a text span copied from the input passage.\n",
      "\n",
      "Please format your results as a JSON array. Each entry in the list should be an object reporting a single finding. The keys in each dictionary should be: [\"intervention\", \"comparator\", \"outcome\", \"effect\", \"evidence\"].\n",
      "\n",
      "Here is the passage:\n",
      "\n",
      "ABSTRACT.OBJECTIVE: To evaluate the impact of (1) the removal of home access to traditional electronic games or (2) their replacement with active input electronic games, on daily physical activity and sedentary behaviour in children aged 10–12 years.\n",
      "\n",
      "ABSTRACT.DESIGN: Crossover randomised controlled trial, over 6 months.\n",
      "\n",
      "ABSTRACT.SETTING: Family homes in metropolitan Perth, Australia from 2007 to 2010.\n",
      "\n",
      "ABSTRACT.PARTICIPANTS: 10-year-old to 12-year-old children were recruited through school and community media. From 210 children who were eligible, 74 met inclusion criteria, 8 withdrew and 10 had insufficient primary outcome measures, leaving 56 children (29 female) for analysis.\n",
      "\n",
      "ABSTRACT.INTERVENTION: A counterbalanced randomised order of three conditions sustained for 8 weeks each: no home access to electronic games, home access to traditional electronic games and home access to active input electronic games.\n",
      "\n",
      "ABSTRACT.MAIN OUTCOME MEASURES: Primary outcome was accelerometer assessed moderate/vigorous physical activity (MVPA). Secondary outcomes included sedentary time and diary assessed physical activity and sedentary behaviours.\n",
      "\n",
      "ABSTRACT.RESULTS: Daily MVPA across the whole week was not significantly different between conditions. However, compared with home access to traditional electronic games, removal of all electronic games resulted in a significant increase in MVPA (mean 3.8 min/day, 95% CI 1.5 to 6.1) and a decrease in sedentary time (4.7 min/day, 0.0 to 9.5) in the after-school period. Similarly, replacing traditional games with active input games resulted in a significant increase in MVPA (3.2 min/day, 0.9 to 5.5) and a decrease in sedentary time (6.2 min/day, 1.4 to 11.4) in the after-school period. Diary reports supported an increase in physical activity and a decrease in screen-based sedentary behaviours with both interventions.\n",
      "\n",
      "ABSTRACT.CONCLUSIONS: Removal of sedentary electronic games from the child's home and replacing these with active electronic games both resulted in small, objectively measured improvements in after-school activity and sedentary time. Parents can be advised that replacing sedentary electronic games with active electronic games is likely to have the same effect as removing all electronic games.\n",
      "\n",
      "ABSTRACT.TRIAL REGISTRATION: Australia and New Zealand Clinical Trials Registry (ACTRN 12609000279224)  \n",
      "\n",
      "\n",
      "ASSISTANT: [{\"intervention\": \"no home access to electronic games, \", \"comparator\": \"home access to traditional electronic games\", \"outcome\": \"moderate/vigorous physical activity (MVPA)\", \"effect\": \"significantly increased\", \"evidence\": \"However, compared with home access to traditional electronic games, removal of all electronic games resulted in a significant increase in MVPA (mean 3.8\\u2005min/day, 95% CI 1.5 to 6.1) and a decrease in sedentary time (4.7\\u2005min/day, 0.0 to 9.5) in the after-school period.\"}]\n",
      "\n",
      "--------------------------------------------------\n",
      "====================================================================================================\n",
      "science.qasper_truncated_4000:\n",
      "====================================================================================================\n",
      "USER: You will be shown sections from a scientific research paper, together with a question about the paper. Your task is to answer the question based on the contents of the paper. In your answer, you should first answer the question in your own words. Then, provide a list of evidence snippets from the paper that contain the information required to answer the question. If the question is unanswerable given the provided excerpts, respond with the word \"unanswerable\".\n",
      "\n",
      "Paper: MonaLog: a Lightweight System for Natural Language Inference Based on Monotonicity\n",
      "\n",
      "We present a new logic-based inference engine for natural language inference (NLI) called MonaLog, which is based on natural logic and the monotonicity calculus. In contrast to existing logic-based approaches, our system is intentionally designed to be as lightweight as possible, and operates using a small set of well-known (surface-level) monotonicity facts about quantifiers, lexical items and tokenlevel polarity information. Despite its simplicity, we find our approach to be competitive with other logic-based NLI models on the SICK benchmark. We also use MonaLog in combination with the current state-of-the-art model BERT in a variety of settings, including for compositional data augmentation. We show that MonaLog is capable of generating large amounts of high-quality training data for BERT, improving its accuracy on SICK.\n",
      "\n",
      "Introduction\n",
      "There has been rapid progress on natural language inference (NLI) in the last several years, due in large part to recent advances in neural modeling BIBREF0 and the introduction of several new large-scale inference datasets BIBREF1, BIBREF2, BIBREF3, BIBREF4. Given the high performance of current state-of-the-art models, there has also been interest in understanding the limitations of these models (given their uninterpretability) BIBREF5, BIBREF6, as well as finding systematic biases in benchmark datasets BIBREF7, BIBREF8. In parallel to these efforts, there have also been recent logic-based approaches to NLI BIBREF9, BIBREF10, BIBREF11, BIBREF12, BIBREF13, which take inspiration from linguistics. In contrast to early attempts at using logic BIBREF14, these approaches have proven to be more robust. However they tend to use many rules and their output can be hard to interpret. It is sometimes unclear whether the attendant complexity is justified, especially given that such models are currently far outpaced by data-driven models and are generally hard to hybridize with data-driven techniques.\n",
      "In this work, we introduce a new logical inference engine called MonaLog, which is based on natural logic and work on monotonicity stemming from vanBenthemEssays86. In contrast to the logical approaches cited above, our starting point is different in that we begin with the following two questions: 1) what is the simplest logical system that one can come up with to solve empirical NLI problems (i.e., the system with minimal amounts of primitives and background knowledge)?; and 2) what is the lower-bound performance of such a model? Like other approaches to natural logic BIBREF15, BIBREF16, our model works by reasoning over surface forms (as opposed to translating to symbolic representations) using a small inventory of monotonicity facts about quantifiers, lexical items and token-level polarity BIBREF17; proofs in the calculus are hence fully interpretable and expressible in ordinary language. Unlike existing work on natural logic, however, our model avoids the need for having expensive alignment and search sub-procedures BIBREF18, BIBREF19, and relies on a much smaller set of background knowledge and primitive relations than MacCartneyManning.\n",
      "To show the effectiveness of our approach, we show results on the SICK dataset BIBREF1, a common benchmark for logic-based NLI, and find MonaLog to be competitive with more complicated logic-based approaches (many of which require full semantic parsing and more complex logical machinery). We also introduce a supplementary version of SICK that corrects several common annotation mistakes (e.g., asymmetrical inference annotations) based on previous work by kalouli2017entail,kalouli2018. Positive results on both these datasets show the ability of lightweight monotonicity models to handle many of the inferences found in current NLI datasets, hence putting a more reliable lower-bound on what results the simplest logical approach is capable of achieving on this benchmark.\n",
      "Since our logic operates over surface forms, it is straightforward to hybridize our models. We investigate using MonaLog in combination with the language model BERT BIBREF20, including for compositional data augmentation, i.e, re-generating entailed versions of examples in our training sets. To our knowledge, our approach is the first attempt to use monotonicity for data augmentation, and we show that such augmentation can generate high-quality training data with which models like BERT can improve performance.\n",
      "\n",
      "Our System: MonaLog ::: Knowledge Base @!START@${K}$@!END@ and Sentence Base @!START@${S}$@!END@\n",
      "MonaLog utilizes two auxiliary sets. First, a knowledge base ${K}$ that stores the world knowledge needed for inference, e.g., semanticist $\\le $ linguist and swim $\\le $ move, which captures the facts that $[\\![\\mbox{\\em semanticist}]\\!]$ denotes a subset of $[\\![\\mbox{\\em linguist}]\\!]$, and that $[\\![\\mbox{\\em swim}]\\!]$ denotes a subset of $[\\![\\mbox{\\em move}]\\!]$, respectively. Such world knowledge can be created manually for the problem at hand, or derived easily from existing resources such as WordNet BIBREF22. Note that we do not blindly add all relations from WordNet to our knowledge base, since this would hinge heavily on word sense disambiguation (we need to know whether the “bank” is a financial institution or a river bank to extract its relations correctly). In the current implementation, we avoid this by adding x $\\le $ y or x $\\perp $ y relations only if both x and y are words in the premise-hypothesis pair. Additionally, some relations that involve quantifiers and prepositions need to be hard-coded, since WordNet does not include them: every $=$ all $=$ each $\\le $ most $\\le $ many $\\le $ a few $=$ several $\\le $ some $=$ a; the $\\le $ some $=$ a; on $\\perp $ off; up $\\perp $ down; etc.\n",
      "We also need to keep track of relations that can potentially be derived from the $P$-$H$ sentence pair. For instance, for all adjectives and nouns that appear in the sentence pair, it is easy to obtain: adj + n $\\le $ n (black cat $\\le $ cat). Similarly, we have n + PP/relative clause $\\le $ n (friend in need $\\le $ friend, dog that bites $\\le $ dog), VP + advP/PP $\\le $ VP (dance happily/in the morning $\\le $ dance), and so on. We also have rules that extract pieces of knowledge from $P$ directly, e.g.: n$_1$ $\\le $ n$_2$ from sentences of the pattern every n$_1$ is a n$_2$. One can also connect MonaLog to bigger knowledge graphs or ontologies such as DBpedia.\n",
      "A sentence base ${S}$, on the other hand, stores the generated entailments and contradictions.\n",
      "\n",
      "Our System: MonaLog ::: Generation ::: Entailments/inferences\n",
      "The key operation for generating entailments is replacement, or substitution. It can be summarized as follows: 1) For upward-entailing (UE) words/constituents, replace them with words/constituents that denote bigger sets. 2) For downward-entailing (DE) words/constituents, either replace them with those denoting smaller sets, or add modifiers (adjectives, adverbs and/or relative clauses) to create a smaller set. Thus for every$^{\\leavevmode {\\color {red}\\uparrow }}$ linguist$^{\\leavevmode {\\color {red}\\downarrow }}$ swim$^{\\leavevmode {\\color {red}\\uparrow }}$, MonaLog can produce the following three entailments by replacing each word with the appropriate word from ${K}$: most$^{\\leavevmode {\\color {red}\\uparrow }}$ linguist$^{\\leavevmode {\\color {red}\\downarrow }}$ swim$^{\\leavevmode {\\color {red}\\uparrow }}$, every$^{\\leavevmode {\\color {red}\\uparrow }}$ semanticist$^{\\leavevmode {\\color {red}\\downarrow }}$ swim$^{\\leavevmode {\\color {red}\\uparrow }}$ and every$^{\\leavevmode {\\color {red}\\uparrow }}$ linguist$^{\\leavevmode {\\color {red}\\downarrow }}$ move$^{\\leavevmode {\\color {red}\\uparrow }}$. These are results of one replacement.\n",
      "Performing replacement for multiple rounds/depths can easily produce many more entailments.\n",
      "\n",
      "Our System: MonaLog ::: Generation ::: Neutral sentences\n",
      "MonaLog returns Neutral if it cannot find the hypothesis $H$ in ${S}.entailments$ or ${S}.contradictions$. Thus, there is no need to generate neutral sentences.\n",
      "\n",
      "MonaLog and SICK\n",
      "We perform two experiments to test MonaLog. We first use MonaLog to solve the problems in a commonly used natural language inference dataset, SICK BIBREF1, comparing our results with previous systems. Second, we test the quality of the data generated by MonaLog. To do this, we generate more training data (sentence pairs) from the SICK training data using our system, and performe fine-tuning on BERT BIBREF20, a language model based on the transformer architecture BIBREF23, with the expanded dataset. In all experiments, we use the Base, Uncased model of BERT.\n",
      "\n",
      "MonaLog and SICK ::: The SICK Dataset\n",
      "The SICK BIBREF1 dataset includes around 10,000 English sentence pairs that are annotated to have either “Entailment”, “Neutral” or “Contradictory” relations. We choose SICK as our testing ground for several reasons. First, we want to test on a large-scale dataset, since we have shown that a similar model BIBREF24 reaches good results on parts of the smaller FraCaS dataset BIBREF25. Second, we want to make our results comparable to those of previous logic-based models such as the ones described in BIBREF26, BIBREF27, BIBREF11, BIBREF13, which were also tested on SICK. We use the data split provided in the dataset: 4,439 training problems, 4,906 test problems and 495 trial problems, see Table TABREF16 for examples.\n",
      "\n",
      "Experiment 1: Using MonaLog Directly ::: Results\n",
      "The results of our system on uncorrected and corrected SICK are presented in Table TABREF27, along with comparisons with other systems.\n",
      "Our accuracy on the uncorrected SICK (77.19%) is much higher than the majority baseline (56.36%) or the hypothesis-only baseline (56.87%) reported by BIBREF8, and only several points lower than current logic-based systems. Since our system is based on natural logic, there is no need for translation into logical forms, which makes the reasoning steps transparent and much easier to interpret. I.e., with entailments and contradictions, we can generate a natural language trace of the system, see Fig. FIGREF13.\n",
      "Our results on the corrected SICK are even higher (see lower part of Table TABREF27), demonstrating the effect of data quality on the final results. Note that with some simple syntactic transformations we can gain 1-2 points in accuracy.\n",
      "Table TABREF28 shows MonaLog's performance on the individual relations. The system is clearly very good at identifying entailments and contradictions, as demonstrated by the high precision values, especially on the corrected SICK set (98.50 precision for E and 95.02 precision for C). The lower recall values are due to MonaLog's current inability to handle syntactic variation.\n",
      "Based on these results, we tested a hybrid model of MonaLog and BERT (see Table TABREF27) where we exploit MonaLog's strength: Since MonaLog has a very high precision on Entailment and Contradiction, we can always trust MonaLog if it predicts E or C; when it returns N, we then fall back to BERT. This hybrid model improves the accuracy of BERT by 1% absolute to 85.95% on the corrected SICK. On the uncorrected SICK dataset, the hybrid system performs worse than BERT.\n",
      "Since MonaLog is optimized for the corrected SICK, it may mislabel many E and C judgments in the uncorrected dataset. The stand-alone BERT system performs better on the uncorrected data (86.74%) than the corrected set (85.00%). The corrected set may be too inconsistent since only a part has been checked.\n",
      "Overall, these hybird results show that it is possible to combine our high-precision system with deep learning architectures. However, more work is necessary to optimize this combined system.\n",
      "\n",
      "Experiment 2: Data Generation Using MonaLog ::: Setup\n",
      "As described above, MonaLog generates entailments and contradictions when solving problems. These can be used as additional training data for a machine learning model.\n",
      "I.e., we pair the newly generated sentences\n",
      "with their input sentence, creating new pairs for training. For example, we take all the sentences in the nodes in Figure FIGREF13 as inferences and all the sentences in rectangles as contradictions, and then form sentence pairs with the input sentence. The additional data can be used directly, almost without human intervention.\n",
      "Thus for experiment 2, the goal is to examine the quality of these generated sentence pairs. For this, we re-train a BERT model on these pairs. If BERT trained on the manually annotated SICK training data is improved by adding data generated by MonaLog, then we can conclude that the generated data is of high quality, even comparable to human annotated data, which is what we found.\n",
      "More specifically, we compare the performance of BERT models trained on a) SICK training data alone, and b) SICK training data plus the entailing and contradictory pairs generated by MonaLog.\n",
      "All experiments are carried out using our corrected version of the SICK data set.\n",
      "However, note that MonaLog is designed to only generate entailments and contradictions. Thus, we only have access to newly generated examples for those two cases, we do not acquire any additional neutral cases. Consequently, adding these examples to the training data will introduce a skewing that does not reflect the class distribution in the test set. Since this will bias the machine learner against neutral cases, we use the following strategy to counteract that tendency: We relabel all cases where BERT is not confident enough for either E or C into N. We set this threshold to 0.95 but leave further optimization of the threshold to future work.\n",
      "\n",
      "Conclusions and Future Work\n",
      "We have presented a working natural-logic-based system, MonaLog, which attains high accuracy on the SICK dataset and can be used to generated natural logic proofs. Considering how simple and straightforward our method is, we believe it can serve as a strong baseline or basis for other (much) more complicated systems, either logic-based or ML/DL-based. In addition, we have shown that MonaLog can generate high-quality training data, which improves the accuracy of a deep learning model when trained on the expanded dataset. As a minor point, we manually checked the corrected SICK dataset by BIBREF28, BIBREF29.\n",
      "There are several directions for future work. The first direction concerns the question how to handle syntactic variation from natural language input. That is, the computational process(es) for inference will usually be specified in terms of strict syntactic conditions, and naturally occurring sentences will typically not conform to those conditions. Among the strategies which allow their systems to better cope with premises and hypotheses with various syntactic structures are sophisticated versions of alignment used by e.g. MacCartney,YanakaMMB18. We will need to extend MonaLog to be able to handle such variation. In the future, we plan to use dependency relations as representations of natural language input and train a classifier that can determine which relations are crucial for inference.\n",
      "Second, as mentioned earlier, we are in need of a fully (rather than partially) checked SICK dataset to examine the impact of data quality on the results since the partially checked dataset may be inherently inconsistent between the checked and non-checked parts.\n",
      "Finally, with regard to the machine learning experiments, we plan to investigate other methods of addressing the imbalance in the training set created by additional entailments and contradictions. We will look into options for artificially creating neutral examples, e.g. by finding reverse entailments, as illustrated by richardson2019probing.\n",
      "\n",
      "Acknowledgements\n",
      "We thank the anonymous reviewers for their helpful comments. Hai Hu is supported by China Scholarship Council.\n",
      "\n",
      "Table 4: Results of MonaLog per relation. C: contradiction; E: entailment; N: neutral.\n",
      "\n",
      "Table 5: Examples of incorrect answers by MonaLog; n.a. = the problem has not been checked in corr. SICK.\n",
      "\n",
      "Table 6: Sentence pairs generated by MonaLog, lemmatized.\n",
      "\n",
      "Question: How do they select monotonicity facts?\n",
      "\n",
      "\n",
      "\n",
      "ASSISTANT: Answer:\n",
      "They derive it from Wordnet\n",
      "\n",
      "Evidence:\n",
      "MonaLog utilizes two auxiliary sets. First, a knowledge base ${K}$ that stores the world knowledge needed for inference, e.g., semanticist $\\le $ linguist and swim $\\le $ move, which captures the facts that $[\\![\\mbox{\\em semanticist}]\\!]$ denotes a subset of $[\\![\\mbox{\\em linguist}]\\!]$, and that $[\\![\\mbox{\\em swim}]\\!]$ denotes a subset of $[\\![\\mbox{\\em move}]\\!]$, respectively. Such world knowledge can be created manually for the problem at hand, or derived easily from existing resources such as WordNet BIBREF22. Note that we do not blindly add all relations from WordNet to our knowledge base, since this would hinge heavily on word sense disambiguation (we need to know whether the “bank” is a financial institution or a river bank to extract its relations correctly). In the current implementation, we avoid this by adding x $\\le $ y or x $\\perp $ y relations only if both x and y are words in the premise-hypothesis pair. Additionally, some relations that involve quantifiers and prepositions need to be hard-coded, since WordNet does not include them: every $=$ all $=$ each $\\le $ most $\\le $ many $\\le $ a few $=$ several $\\le $ some $=$ a; the $\\le $ some $=$ a; on $\\perp $ off; up $\\perp $ down; etc.\n",
      "\n",
      "--------------------------------------------------\n",
      "USER: You will be shown sections from a scientific research paper, together with a question about the paper. Please print out a collection of quotations from the research paper which answer the question. Each quotation should be a contiguous span of text from the paper. Separate the quotations with newlines. If the question is unanswerable given the provided excerpts, respond with the word \"unanswerable\".\n",
      "\n",
      "Paper: Do We Really Need Fully Unsupervised Cross-Lingual Embeddings?\n",
      "\n",
      "Recent efforts in cross-lingual word embedding (CLWE) learning have predominantly focused on fully unsupervised approaches that project monolingual embeddings into a shared cross-lingual space without any cross-lingual signal. The lack of any supervision makes such approaches conceptually attractive. Yet, their only core difference from (weakly) supervised projection-based CLWE methods is in the way they obtain a seed dictionary used to initialize an iterative self-learning procedure. The fully unsupervised methods have arguably become more robust, and their primary use case is CLWE induction for pairs of resource-poor and distant languages. In this paper, we question the ability of even the most robust unsupervised CLWE approaches to induce meaningful CLWEs in these more challenging settings. A series of bilingual lexicon induction (BLI) experiments with 15 diverse languages (210 language pairs) show that fully unsupervised CLWE methods still fail for a large number of language pairs (e.g., they yield zero BLI performance for 87/210 pairs). Even when they succeed, they never surpass the performance of weakly supervised methods (seeded with 500-1,000 translation pairs) using the same self-learning procedure in any BLI setup, and the gaps are often substantial. These findings call for revisiting the main motivations behind fully unsupervised CLWE methods.\n",
      "\n",
      "Introduction and Motivation\n",
      "The wide use and success of monolingual word embeddings in NLP tasks BIBREF0 , BIBREF1 has inspired further research focus on the induction of cross-lingual word embeddings (CLWEs). CLWE methods learn a shared cross-lingual word vector space where words with similar meanings obtain similar vectors regardless of their actual language. CLWEs benefit cross-lingual NLP, enabling multilingual modeling of meaning and supporting cross-lingual transfer for downstream tasks and resource-lean languages. CLWEs provide invaluable cross-lingual knowledge for, inter alia, bilingual lexicon induction BIBREF2 , BIBREF3 , information retrieval BIBREF4 , BIBREF5 , machine translation BIBREF6 , BIBREF7 , document classification BIBREF8 , cross-lingual plagiarism detection BIBREF9 , domain adaptation BIBREF10 , cross-lingual POS tagging BIBREF11 , BIBREF12 , and cross-lingual dependency parsing BIBREF13 , BIBREF14 .\n",
      "The landscape of CLWE methods has recently been dominated by the so-called projection-based methods BIBREF15 , BIBREF16 , BIBREF17 . They align two monolingual embedding spaces by learning a projection/mapping based on a training dictionary of translation pairs. Besides their simple conceptual design and competitive performance, their popularity originates from the fact that they rely on rather weak cross-lingual supervision. Originally, the seed dictionaries typically spanned several thousand word pairs BIBREF15 , BIBREF18 , BIBREF19 , but more recent work has shown that CLWEs can be induced with even weaker supervision from small dictionaries spanning several hundred pairs BIBREF20 , identical strings BIBREF21 , or even only shared numerals BIBREF22 .\n",
      "Taking the idea of reducing cross-lingual supervision to the extreme, the latest CLWE developments almost exclusively focus on fully unsupervised approaches BIBREF23 , BIBREF24 , BIBREF25 , BIBREF26 , BIBREF27 , BIBREF28 , BIBREF29 , BIBREF30 : they fully abandon any source of (even weak) supervision and extract the initial seed dictionary by exploiting topological similarities between pre-trained monolingual embedding spaces. Their modus operandi can roughly be described by three main components: C1) unsupervised extraction of a seed dictionary; C2) a self-learning procedure that iteratively refines the dictionary to learn projections of increasingly higher quality; and C3) a set of preprocessing and postprocessing steps (e.g., unit length normalization, mean centering, (de)whitening) BIBREF31 that make the entire learning process more robust.\n",
      "The induction of fully unsupervised CLWEs is an inherently interesting research topic per se. Nonetheless, the main practical motivation for developing such approaches in the first place is to facilitate the construction of multilingual NLP tools and widen the access to language technology for resource-poor languages and language pairs. However, the first attempts at fully unsupervised CLWE induction failed exactly for these use cases, as shown by sogaard2018on. Therefore, the follow-up work aimed to improve the robustness of unsupervised CLWE induction by introducing more robust self-learning procedures BIBREF24 , BIBREF32 . Besides increased robustness, recent work claims that fully unsupervised projection-based CLWEs can even match or surpass their supervised counterparts BIBREF23 , BIBREF24 , BIBREF27 , BIBREF33 , BIBREF34 .\n",
      "In this paper, we critically examine these claims on robustness and improved performance of unsupervised CLWEs by running a large-scale evaluation in the bilingual lexicon induction (BLI) task on 15 languages (i.e., 210 languages pairs, see Table 2 in § \"Experimental Setup\" ). The languages were selected to represent different language families and morphological types, as we argue that fully unsupervised CLWEs have been designed to support exactly these setups. However, we show that even the most robust unsupervised CLWE method BIBREF24 still fails for a large number of language pairs: 87/210 BLI setups are unsuccessful, yielding (near-)zero BLI performance. Further, even when the unsupervised method succeeds, it is because the components C2 (self-learning) and C3 (pre-/post-processing) can mitigate the undesired effects of noisy seed lexicon extraction. We then demonstrate that the combination of C2 and C3 with a small provided seed dictionary (e.g., 500 or 1K pairs) outscores the unsupervised method in all cases, often with a huge margin, and does not fail for any language pair. Furthermore, we show that the most robust unsupervised CLWE approach still fails completely when it relies on monolingual word vectors trained on domain-dissimilar corpora. We also empirically verify that unsupervised approaches cannot outperform weakly supervised approaches also for closely related languages (e.g., Swedish–Danish, Spanish–Catalan).\n",
      "While the “no supervision at all” premise behind fully unsupervised CLWE methods is indeed seductive, our study strongly suggests that future research efforts should revisit the main motivation behind these methods and focus on designing even more robust solutions, given their current inability to support a wide spectrum of language pairs. In hope of boosting induction of CLWEs for more diverse and distant language pairs, we make all 210 training and test dictionaries used in this work publicly available at: https://github.com/ivulic/panlex-bli.\n",
      "\n",
      "Three Key Components\n",
      "C1. Seed Lexicon Extraction. vecmap induces the initial seed dictionary using the following heuristic: monolingual similarity distributions for words with similar meaning will be similar across languages. The monolingual similarity distributions for the two languages are given as rows (or columns; the matrices are symmetric) of $\\mathbf {M}_x = \\mathbf {X}\\mathbf {X}^T$ and $\\mathbf {M}_z = \\mathbf {Z}\\mathbf {Z}^T$ . For the distributions of similarity scores to be comparable, the values in each row of $\\mathbf {M}_x$ and $\\mathbf {M}_z$ are first sorted. The initial dictionary $D^{(1)}$ is finally obtained by searching for mutual nearest neighbours between the rows of $\\sqrt{\\mathbf {M}_x}$ and of $\\sqrt{\\mathbf {M}_z}$ .\n",
      "C2. Self-Learning. Not counting the preprocessing and postprocessing steps (component C3), self-learning then iteratively repeats two steps:\n",
      "1) Let $\\mathbf {D}^{(k)}$ be the binary matrix indicating the aligned words in the dictionary $D^{(k)}$ . The orthogonal transformation matrices are then obtained as $\\mathbf {W}^{(k)}_x = \\mathbf {U}$ and $\\mathbf {W}^{(k)}_z = \\mathbf {V}$ , where $\\mathbf {U}\\mathbf {\\Sigma }\\mathbf {V}^T$ is the singular value decomposition of the matrix $\\mathbf {X}^T\\mathbf {D}^{(k)}\\mathbf {Z}$ . The cross-lingual space of the $D^{(k)}$0 -th iteration is then $D^{(k)}$1 .\n",
      "2) The new dictionary $D^{(k+1)}$ is then built by identifying nearest neighbours in $\\mathbf {Y}^{(k)}$ . These can be easily extracted from the matrix $\\mathbf {P} = \\mathbf {X}\\mathbf {W}^{(k)}_x( \\mathbf {Z}\\mathbf {W}^{(k)}_z)^T$ . All nearest neighbours can be used, or additional symmetry constraints can be imposed to extract only mutual nearest neighbours: all pairs of indices ( $i, j$ ) for which $\\mathbf {P}_{ij}$ is the largest value both in row $i$ and column $j$ .\n",
      "The above procedure, however, often converges to poor local optima. To remedy for this, the second step (i.e., dictionary induction) is extended with techniques that make self-learning more robust. First, the vocabularies of $\\mathbf {X}$ and $\\mathbf {Z}$ are cut to the top $k$ most frequent words. Second, similarity scores in $\\mathbf {P}$ are kept with probability $p$ , and set to zero otherwise. This dropout allows for a wider exploration of possible word pairs in the dictionary and contributes to escaping poor local optima given the noisy seed lexicon in the first iterations.\n",
      "C3. Preprocessing and Postprocessing Steps. While iteratively learning orthogonal transformations $\\mathbf {W}_{x}$ and $\\mathbf {W}_{z}$ for $\\mathbf {X}$ and $\\mathbf {Z}$ is the central step of unsupervised projection-based CLWE methods, preprocessing and postprocessing techniques are additionally applied before and after the transformation. While such techniques are often overlooked in model comparisons, they may have a great impact on the model's final performance, as we validate in § \"Results and Discussion\" . We briefly summarize two pre-processing (S1 and S2) and post-processing (S3 and S4) steps used in our evaluation, originating from the framework of artetxe2018generalizing.\n",
      "S1) Normalization and mean centering. We first apply unit length normalization: all vectors in $\\mathbf {X}$ and $\\mathbf {Z}$ are normalized to have a unit Euclidean norm. Following that, $\\mathbf {X}$ and $\\mathbf {Z}$ are mean centered dimension-wise and then again length-normalized.\n",
      "S2) Whitening. ZCA whitening BIBREF36 is applied on (S1-processed) $\\mathbf {X}$ and $\\mathbf {Z}$ : it transforms the matrices such that each dimension has unit variance and that the dimensions are uncorrelated. Intuitively, the vector spaces are easier to align along directions of high variance.\n",
      "S3) Dewhitening. A transformation inverse to S2: for improved performance it is important to restore the variance information after the projection, if whitening was applied in S2 BIBREF31 .\n",
      "S4) Symmetric re-weighting. This step attempts to further align the embeddings in the cross-lingual embedding space by measuring how well a dimension in the space correlates across languages for the current iteration dictionary $D^{(k)}$ . The best results are obtained when re-weighting is neutral to the projection direction, that is, when it is applied symmetrically in both languages.\n",
      " In the actual implementation S1 is applied only once, before self-learning. S2, S3 and S4 are applied in each self-learning iteration.\n",
      "Model Configurations. Note that C2 and C3 can be equally used on top of any (provided) seed lexicon (i.e., $D^{(1)}$ := $D_0$ ) to enable weakly supervised learning, as we propose here. In fact, the variations of the three key components, C1) seed lexicon, C2) self-learning, and C3) preprocessing and postprocessing, construct various model configurations which can be analyzed to probe the importance of each component in the CLWE induction process. A selection of representative configurations evaluated later in § \"Results and Discussion\" is summarized in Table 1 .\n",
      "\n",
      "Acknowledgments\n",
      "This work is supported by the ERC Consolidator Grant LEXICAL: Lexical Acquisition Across Languages (no 648909). The work of Goran Glavaš is supported by the Baden-Württemberg Stiftung (AGREE grant of the Eliteprogramm). Roi Reichart is partially funded by ISF personal grants No. 1625/18. We thank the three anonymous reviewers for their encouraging comments and suggestions.\n",
      "\n",
      "Figure 1: General unsupervised CLWE approach.\n",
      "\n",
      "Table 1: Configurations obtained by varying components C1, C2, and C3 used in our empirical comparison in §4.\n",
      "\n",
      "Table 6: All BLI scores (MRR) with Bulgarian (BG) as the source language. 5k and 1k denote the seed dictionary D0 size for (weakly) supervised methods. See Table 1 for a brief description of each model configuration.\n",
      "\n",
      "Table 9: All BLI scores (MRR) with Estonian (ET) as the source language. 5k and 1k denote the seed dictionary D0 size for (weakly) supervised methods. See Table 1 for a brief description of each model configuration.\n",
      "\n",
      "Table 10: All BLI scores (MRR) with Basque (EU) as the source language. 5k and 1k denote the seed dictionary D0 size for (weakly) supervised methods. See Table 1 for a brief description of each model configuration.\n",
      "\n",
      "Table 13: All BLI scores (MRR) with Hungarian (HU) as the source language. 5k and 1k denote the seed dictionary D0 size for (weakly) supervised methods. See Table 1 for a brief description of each model configuration.\n",
      "\n",
      "Table 14: All BLI scores (MRR) with Indonesian (ID) as the source language. 5k and 1k denote the seed dictionary D0 size for (weakly) supervised methods. See Table 1 for a brief description of each model configuration.\n",
      "\n",
      "Question: What methods were used for unsupervised CLWE?\n",
      "\n",
      "\n",
      "\n",
      "ASSISTANT: Answer quotations:\n",
      "Unsupervised CLWEs. These methods first induce a seed dictionary $D^{(1)}$ leveraging only two unaligned monolingual spaces (C1). While the algorithms for unsupervised seed dictionary induction differ, they all strongly rely on the assumption of similar topological structure between the two pretrained monolingual spaces. Once the seed dictionary is obtained, the two-step iterative self-learning procedure (C2) takes place: 1) a dictionary $D^{(k)}$ is first used to learn the joint space $\\mathbf {Y}^{(k)} = \\mathbf {X{W}}^{(k)}_x \\cup \\mathbf {Z{W}}^{(k)}_z$ ; 2) the nearest neighbours in $\\mathbf {Y}^{(k)}$ then form the new dictionary $D^{(k+1)}$ . We illustrate the general structure in Figure 1 .\n",
      "\n",
      "--------------------------------------------------\n",
      "USER: You will be shown sections from a scientific research paper, together with a question about the paper. Please print out a collection of quotations from the research paper which answer the question. Each quotation should be a contiguous span of text from the paper. Separate the quotations with newlines. If the question is unanswerable given the provided excerpts, respond with the word \"unanswerable\".\n",
      "\n",
      "Paper: Evaluation of basic modules for isolated spelling error correction in Polish texts\n",
      "\n",
      "Spelling error correction is an important problem in natural language processing, as a prerequisite for good performance in downstream tasks as well as an important feature in user-facing applications. For texts in Polish language, there exist works on specific error correction solutions, often developed for dealing with specialized corpora, but not evaluations of many different approaches on big resources of errors. We begin to address this problem by testing some basic and promising methods on PlEWi, a corpus of annotated spelling extracted from Polish Wikipedia. These modules may be further combined with appropriate solutions for error detection and context awareness. Following our results, combining edit distance with cosine distance of semantic vectors may be suggested for interpretable systems, while an LSTM, particularly enhanced by ELMo embeddings, seems to offer the best raw performance.\n",
      "\n",
      "Introduction\n",
      "Spelling error correction is a fundamental NLP task. Most language processing applications benefit greatly from being provided clean texts for their best performance. Human users of computers also often expect competent help in making spelling of their texts correct.\n",
      "Because of the lack of tests of many common spelling correction methods for Polish, it is useful to establish how they perform in a simple scenario. We constrain ourselves to the pure task of isolated correction of non-word errors. They are traditionally separated in error correction literature BIBREF0 . Non-word errors are here incorrect word forms that not only differ from what was intended, but also do not constitute another, existing word themselves. Much of the initial research on error correction focused on this simple task, tackled without means of taking the context of the nearest words into account.\n",
      "It is true that, especially in the case of neural networks, it is often possible and desirable to combine problems of error detection, correction and context awareness into one task trained with a supervised training procedure. In language correction research for English language also grammatical and regular spelling errors have been treated uniformly with much success BIBREF1 .\n",
      "However, when more traditional methods are used, because of their predictability and interpretability for example, one can mix and match various approaches to dealing with the subproblems of detection, correction and context handling (often equivalent to employing some kind of a language model). We call it a modular approach to building spelling error correction systems. There is recent research where this paradigm was applied, interestingly, to convolutional networks trained separately for various subtasks BIBREF2 . In similar setups it is more useful to assess abilities of various solutions in isolation. The exact architecture of a spelling correction system should depend on characteristics of texts it will work on.\n",
      "Similar considerations eliminated from our focus handcrafted solutions for the whole spelling correction pipeline, primarily the LanguageTool BIBREF3 . Its performance in fixing spelling of Polish tweets was already tested BIBREF4 . For our purposes it would be given an unfair advantage, since it is a rule-based system making heavy use of words in context of the error.\n",
      "\n",
      "Problems of spelling correction for Polish\n",
      "Published work on language correction for Polish dates back at least to 1970s, when simplest Levenshtein distance solutions were used for cleaning mainframe inputs BIBREF5 , BIBREF6 . Spelling correction tests described in literature have tended to focus on one approach applied to a specific corpus. Limited examples include works on spellchecking mammography reports and tweets BIBREF7 , BIBREF4 . These works emphasized the importance of tailoring correction systems to specific problems of corpora they are applied to. For example, mammography reports suffer from poor typing, which in this case is a repetitive work done in relative hurry. Tweets, on the other hand, tend to contain emoticons and neologisms that can trick solutions based on rules and dictionaries, such as LanguageTool. The latter is, by itself, fairly well suited for Polish texts, since a number of extensions to the structure of this application was inspired by problems with morphology of Polish language BIBREF3 .\n",
      "These existing works pointed out more general, potentially useful qualities specific to spelling errors in Polish language texts. It is, primarily, the problem of leaving out diacritical signs, or, more rarely, adding them in wrong places. This phenomenon stems from using a variant of the US keyboard layout, where combinations of AltGr with some alphabetic keys produces characters unique to Polish. When the user forgets or neglects to press the AltGr key, typos such as writing *olowek instead of ołówek appear. In fact, BIBREF4 managed to get substantial performance on Twitter corpus by using this ”diacritical swapping” alone.\n",
      "\n",
      "Baseline methods\n",
      "The methods that we evaluated are baselines are the ones we consider to be basic and with moderate potential of yielding particularly good results. Probably the most straightforward approach to error correction is selecting known words from a dictionary that are within the smallest edit distance from the error. We used the Levenshtein distance metric BIBREF8 implemented in Apache Lucene library BIBREF9 . It is a version of edit distance that treats deletions, insertions and replacements as adding one unit distance, without giving a special treatment to character swaps. The SGJP – Grammatical Dictionary of Polish BIBREF10 was used as the reference vocabulary.\n",
      "Another simple approach is the aforementioned diacritical swapping, which is a term that we introduce here for referring to a solution inspired by the work of BIBREF4 . Namely, from the incorrect form we try to produce all strings obtainable by either adding or removing diacritical marks from characters. We then exclude options that are not present in SGJP, and select as the correction the one within the smallest edit distance from the error. It is possible for the number of such diacritically-swapped options to become very big. For example, the token Modlin-Zegrze-Pultusk-Różan-Ostrołęka-Łomża-Osowiec (taken from PlEWi corpus of spelling errors, see below) can yield over INLINEFORM0 states with this method, such as Módłiń-Żęgrzę-Pułtuśk-Roźąń-Óśtróleką-Lómzą-Óśówięć. The actual correction here is just fixing the ł in Pułtusk. Hence we only try to correct in this way tokens that are shorter than 17 characters.\n",
      "\n",
      "Vector distance\n",
      "A promising method, adapted from work on correcting texts by English language learners BIBREF11 , expands on the concept of selecting a correction nearest to the spelling error according to some notion of distance. Here, the Levenshtein distance is used in a weighted sum to cosine distance between word vectors. This is based on the observation that trained vectors models of distributional semantics contain also representations of spelling errors, if they were not pruned. Their representations tend to be similar to those of their correct counterparts. For example, the token enginir will appear in similar contexts as engineer, and therefore will be assigned a similar vector embedding.\n",
      "The distance between two tokens INLINEFORM0 and INLINEFORM1 is thus defined as INLINEFORM2 \n",
      "Here INLINEFORM0 is just Levenshtein distance between strings, and INLINEFORM1 – cosine distance between vectors. INLINEFORM2 denotes the word vector for INLINEFORM3 . Both distance metrics are in our case roughly in the range [0,1] thanks to the scaling of edit distance performed automatically by Apache Lucene. We used a pretrained set of word embeddings of Polish BIBREF12 , obtained with the flavor word2vec procedure using skipgrams and negative sampling BIBREF13 .\n",
      "\n",
      "Recurrent neural networks\n",
      "Another powerful approach, if conceptually simple in linguistic terms, is using a character-based recurrent neural network. Here, we test uni- and bidirectional Long Short-Term Memory networks BIBREF14 that are fed characters of the error as their input and are expected to output its correct form, character after character. This is similar to traditional solutions conceptualizing the spelling error as a chain of characters, which are used as evidence to predict the most likely chain of replacements (original characters). This was done with n-gram methods, Markov chains and other probabilistic models BIBREF15 . Since nowadays neural networks enjoy a large awareness as an element of software infrastructure, with actively maintained packages readily available, their evaluation seems to be the most practically useful. We used the PyTorch BIBREF16 implementation of LSTM in particular.\n",
      "The bidirectional version BIBREF17 of LSTM reads the character chains forward and backwards at the same time. Predictions from networks running in both directions are averaged.\n",
      "In order to provide the network an additional, broad picture peek at the whole error form we also evaluated a setup where the internal state of LSTM cells, instead of being initialized randomly, is computed from an ELMo embedding BIBREF18 of the token. The ELMo embedder is capable of integrating linguistic information carried by the whole form (probably often not much in case of errors), as well as the string as a character chain. The latter is processed with a convolutional neural network. How this representation is constructed is informed by the whole corpus on which the embedder was trained. The pretrained ELMo model that we used BIBREF19 was trained on Wikipedia and Common Crawl corpora of Polish.\n",
      "The ELMo embedding network outputs three layers as matrices, which are supposed to reflect subsequent compositional layers of language, from phonetic phenomena at the bottom to lexical ones at the top. A weighted sum of these layers is computed, with weights trained along with the LSTM error-correcting network. Then we apply a trained linear transformation, followed by INLINEFORM0 non-linearity: INLINEFORM1 \n",
      "(applied cellwise) in order to obtain the initial setting of parameters for the main LSTM. Our ELMo-augmented LSTM is bidirectional.\n",
      "\n",
      "Experimental setup\n",
      "PlEWi BIBREF20 is an early version of WikEd BIBREF21 error corpus, containing error type annotations allowing us to select only non-word errors for evaluation. Specifically, PlEWi supplied 550,755 [error, correction] pairs, from which 298,715 were unique. The corpus contains data extracted from histories of page versions of Polish Wikipedia. An algorithm designed by the corpus author determined where the changes were correcting spelling errors, as opposed to expanding content and disagreements among Wikipedia editors.\n",
      "The corpus features texts that are descriptive rather than conversational, contain relatively many proper names and are more likely to have been at least skimmed by the authors before submitting for online publication. Error cases provided by PlEWi are, therefore, not a balanced representation of spelling errors in written Polish language. PlEWi does have the advantage of scale in comparison to existing literature, such as BIBREF4 operating on a set of only 740 annotated errors in tweets.\n",
      "All methods were tested on a test subset of 25% of cases, with 75% left for training (where needed) and 5% for development.\n",
      "The methods that required training – namely recurrent neural networks – had their loss measured as cross-entropy loss measure between correct character labels and predictions. This value was minimized with Adam algorithm BIBREF22 . The networks were trained for 35 epochs.\n",
      "\n",
      "Results\n",
      "The experimental results are presented in Table TABREF4 . Diacritic swapping showed a remarkably poor performance, despite promising mentions in existing literature. This might be explained by the already mentioned feature of Wikipedia edits, which can be expected to be to some degree self-reviewed before submission. This can very well limit the number of most trivial mistakes.\n",
      "On the other hand, the vector distance method was able to bring a discernible improvement over pure Levenshtein distance, comparable even with the most basic LSTM. It is possible that assigning more fine-tuned weights to edit distance and semantic distance would make the quality of predictions even higher. The idea of using vector space measurements explicitly can be also expanded if we were to consider the problem of contextualizing corrections. For example, the semantic distance of proposed corrections to the nearest words is likely to carry much information about their appropriateness. Looking from another angle, searching for words that seem semantically off in context may be a good heuristic for detecting errors that are not nonword (that is, they lead to wrong forms appearing in text which are nevertheless in-vocabulary).\n",
      "The good performance of recurrent network methods is hardly a surprise, given observed effectiveness of neural networks in many NLP tasks in the recent decade. It seems that bidirectional LSTM augmented with ELMo may already hit the limit for correcting Polish spelling errors without contextual information. While it improves accuracy in comparison to LSTM initialized withrandom noise, it makes the test cross-entropy slightly worse, which hints at overfitting. The perplexity measures actually increase sharply for more sophisticated architectures. Perplexity should show how little probability is assigned by the model to true answers. We measure it as INLINEFORM0 \n",
      "where INLINEFORM0 is a sequence of INLINEFORM1 characters, forming the correct version of the word, and INLINEFORM2 is the estimated probability of the INLINEFORM3 th character, given previous predicted characters and the incorrect form. The observed increase of perplexity for increasingly accurate models is most likely due to more refined predicted probability distributions, which go beyond just assigning the bulk of probability to the best answer.\n",
      "Interesting insights can be gained from weights assigned by optimization to layers of ELMo network, which are taken as the word form embedding (Table TABREF5 ). The first layer, and the one that is nearest to input of the network, is given relatively the least importance, while the middle one dominates both others taken together. This suggests that in error correction, at least for Polish, the middle level of morphemes and other characteristic character chunks is more important than phenomena that are low-level or tied to some specific words. This observation should be taken into account in further research on practical solutions for spelling correction.\n",
      "\n",
      "Conclusion\n",
      "Among the methods tested the bidirectional LSTM, especially initialized by ELMo embeddings, offers the best accuracy and raw performance. Adding ELMo to a straightforward PyTorch implementation of LSTM may be easier now than at the time of performing our tests, as since then the authors of ELMoForManyLangs package BIBREF19 improved their programmatic interface. However, if a more interpretable and explainable output is required, some version of vector distance combined with edit distance may be the best direction. It should be noted that this method produces multiple candidate corrections with their similarity scores, as opposed to only one “best guess“ correction that can be obtained from a character-based LSTM. This is important in applications where it is up to humans to the make the final decision, and they are only to be aided by a machine.\n",
      "It is desirable for further reasearch to expand the corpus material into a wider and more representative set of texts. Nevertheless, the solution for any practical case has to be tailored to its characteristic error patterns. Works on language correction for English show that available corpora can be ”boosted” BIBREF1 , i.e. expanded by generating new errors consistent with a generative model inferred from the data. This may greatly aid in developing models that are dependent on learning from error corpora.\n",
      "A deliberate omission in this paper are the elements accompanying most real-word error correction solutions. Some fairly obvious approaches to integrating evidence from context include n-grams and Markov chains, although the possibility of using measurements in spaces of semantic vectors was already mentioned in this article. Similarly, non-word errors can be easily detected with comparing tokens against reference vocabulary, but in practice one should have ways of detecting mistakes masquerading as real words and fixing bad segmentation (tokens that are glued together or improperly separated). Testing how performant are various methods for dealing with these problems in Polish language is left for future research.\n",
      "\n",
      "Table 1: Test results for all the methods used. The loss measure is cross-entropy.\n",
      "\n",
      "Table 2: Discovered optimal weights for summing layers of ELMo embedding for initializing an error-correcting LSTM. The layers are numbered from the one that directly processes character and word input to the most abstract one.\n",
      "\n",
      "Question: What solutions are proposed for error detection and context awareness?\n",
      "\n",
      "\n",
      "\n",
      "ASSISTANT: unanswerable\n",
      "\n",
      "--------------------------------------------------\n",
      "====================================================================================================\n",
      "science.scifact_json:\n",
      "====================================================================================================\n",
      "USER: You are a helpful scientific research assistant. You will be provided a scientific claim, and the abstract of a scientific paper. Each sentence in the abstract will be prefixed with a number enclosed in brackets, e.g. [1], [2], etc. Your task is to return a JSON object with two fields:\n",
      "\n",
      "- \"verdict\": The fact-checking verdict. If the information in the abstract supports the claim, write SUPPORT. If the abstract contradicts the claim, write CONTRADICT. If the abstract does not provide enough information to arrive at a verdict, write INSUFFICIENT.\n",
      "- \"evidence\": An array of sentences providing evidence for the verdict. For instance, if sentences 1 and 3 provide evidence, write [1, 3]. If the fact-checking verdict was INSUFFICIENT, return an empty array.\n",
      "\n",
      "The claim and abstract are below.\n",
      "\n",
      "Claim: IFI16 impedes STING phosphorylation and translocation, resulting in reduced activation of STING.\n",
      "\n",
      "Abstract: [1] Many human cells can sense the presence of exogenous DNA during infection though the cytosolic DNA receptor cyclic GMP-AMP synthase (cGAS), which produces the second messenger cyclic GMP-AMP (cGAMP). [2] Other putative DNA receptors have been described, but whether their functions are redundant, tissue-specific or integrated in the cGAS-cGAMP pathway is unclear. [3] Here we show that interferon-γ inducible protein 16 (IFI16) cooperates with cGAS during DNA sensing in human keratinocytes, as both cGAS and IFI16 are required for the full activation of an innate immune response to exogenous DNA and DNA viruses. [4] IFI16 is also required for the cGAMP-induced activation of STING, and interacts with STING to promote STING phosphorylation and translocation. [5] We propose that the two DNA sensors IFI16 and cGAS cooperate to prevent the spurious activation of the type I interferon response.\n",
      "\n",
      "\n",
      "\n",
      "ASSISTANT: {\"verdict\": \"CONTRADICT\", \"evidence\": [3]}\n",
      "\n",
      "--------------------------------------------------\n",
      "USER: You are a helpful scientific research assistant. You will be provided a scientific claim, and the abstract of a scientific paper. Each sentence in the abstract will be prefixed with a number enclosed in brackets, e.g. [1], [2], etc. Your task is to return a JSON object with two fields:\n",
      "\n",
      "- \"verdict\": The fact-checking verdict. If the information in the abstract supports the claim, write SUPPORT. If the abstract contradicts the claim, write CONTRADICT. If the abstract does not provide enough information to arrive at a verdict, write INSUFFICIENT.\n",
      "- \"evidence\": An array of sentences providing evidence for the verdict. For instance, if sentences 1 and 3 provide evidence, write [1, 3]. If the fact-checking verdict was INSUFFICIENT, return an empty array.\n",
      "\n",
      "The claim and abstract are below.\n",
      "\n",
      "Claim: Charcoal shows no benefit for acute paraquat poisoning.\n",
      "\n",
      "Abstract: [1] BACKGROUND Organophosphorus pesticide (OP) self-poisoning is a major problem in the developing rural world. [2] There is little clinical trial data to guide therapy, hindering the identification of best therapy. [3] Despite the recognition of adverse effects, gastric lavage is commonly done in Asia. [4] We aimed to identify studies assessing its effectiveness. [5] METHOD We systematically searched the literature for controlled clinical studies that assessed the effect of gastric lavage in OP pesticide self-poisoning. [6] RESULTS All 56 studies identified were Chinese and reported benefit from the intervention studied, including multiple gastric lavages, use of norepinephrine or pralidoxime in the lavage fluid, concurrent treatment with naloxone or scopolamine, insertion of the gastric tube via a laparotomy incision, and lavage later than 12 h post-ingestion. [7] However, only 23 were RCTs and none presented adequate methodology for their quality to be assessed. [8] The patient population and study treatment protocol were not defined - large variation in case fatality in the control arm of the studies (from 4.5 to 93%) suggests marked variation between studies and likely between study arms. [9] No study compared an intervention against a control group receiving no gastric lavage or provided any data to indicate whether a significant quantity of poison was removed. [10] CONCLUSION Despite widespread use of multiple gastric lavages for OP pesticide poisoning across Asia, there is currently no high-quality evidence to support its clinical effectiveness. [11] There is a need for studies to identify in which patients and for what duration gastric lavage is able to remove significant quantities of poison. [12] Following these studies, large clinical trials will be required to address the effectiveness and safety of gastric lavage (either single or multiple) in acute OP pesticide poisoning.\n",
      "\n",
      "\n",
      "\n",
      "ASSISTANT: {\"verdict\": \"INSUFFICIENT\", \"evidence\": []}\n",
      "\n",
      "--------------------------------------------------\n",
      "USER: You are a helpful scientific research assistant. You will be provided a scientific claim, and the abstract of a scientific paper. Each sentence in the abstract will be prefixed with a number enclosed in brackets, e.g. [1], [2], etc. Your task is to return a JSON object with two fields:\n",
      "\n",
      "- \"verdict\": The fact-checking verdict. If the information in the abstract supports the claim, write SUPPORT. If the abstract contradicts the claim, write CONTRADICT. If the abstract does not provide enough information to arrive at a verdict, write INSUFFICIENT.\n",
      "- \"evidence\": An array of sentences providing evidence for the verdict. For instance, if sentences 1 and 3 provide evidence, write [1, 3]. If the fact-checking verdict was INSUFFICIENT, return an empty array.\n",
      "\n",
      "The claim and abstract are below.\n",
      "\n",
      "Claim: Scapular stabilizer exercises are more effective than general exercise therapy in reducing pain and improving function of the shoulder.\n",
      "\n",
      "Abstract: [1] OBJECTIVE To evaluate if a specific exercise strategy, targeting the rotator cuff and scapula stabilisers, improves shoulder function and pain more than unspecific exercises in patients with subacromial impingement syndrome, thereby decreasing the need for arthroscopic subacromial decompression. [2] DESIGN Randomised, participant and single assessor blinded, controlled study. [3] SETTING Department of orthopaedics in a Swedish university hospital. [4] PARTICIPANTS 102 patients with long standing (over six months) persistent subacromial impingement syndrome in whom earlier conservative treatment had failed, recruited through orthopaedic specialists. [5] INTERVENTIONS The specific exercise strategy consisted of strengthening eccentric exercises for the rotator cuff and concentric/eccentric exercises for the scapula stabilisers in combination with manual mobilisation. [6] The control exercise programme consisted of unspecific movement exercises for the neck and shoulder. [7] Patients in both groups received five to six individual guided treatment sessions during 12 weeks. [8] In between these supervised sessions the participants performed home exercises once or twice a day for 12 weeks. [9] MAIN OUTCOME MEASURES The primary outcome was the Constant-Murley shoulder assessment score evaluating shoulder function and pain. [10] Secondary outcomes were patients' global impression of change because of treatment and decision regarding surgery. [11] RESULTS Most (97, 95%) participants completed the 12 week study. [12] There was a significantly greater improvement in the Constant-Murley score in the specific exercise group than in the control exercise group (24 points (95% confidence interval 19 to 28.0) v 9 points (5 to 13); mean difference between group: 15 points (8.5 to 20.6)). [13] Significantly more patients in the specific exercise group reported successful outcome (defined as large improvement or recovered) in the patients' global assessment of change because of treatment: 69% (35/51) v 24% (11/46); odds ratio 7.6, 3.1 to 18.9; P<0.001. [14] A significantly lower proportion of patients in the specific exercise group subsequently chose to undergo surgery: 20% (10/51) v 63% (29/46); odds ratio 7.7, 3.1 to 19.4; P<0.001). [15] CONCLUSION A specific exercise strategy, focusing on strengthening eccentric exercises for the rotator cuff and concentric/eccentric exercises for the scapula stabilisers, is effective in reducing pain and improving shoulder function in patients with persistent subacromial impingement syndrome. [16] By extension, this exercise strategy reduces the need for arthroscopic subacromial decompression within the three month timeframe used in the study. [17] TRIAL REGISTRATION Clinical trials NCT01037673.\n",
      "\n",
      "\n",
      "\n",
      "ASSISTANT: {\"verdict\": \"SUPPORT\", \"evidence\": [14]}\n",
      "\n",
      "--------------------------------------------------\n",
      "====================================================================================================\n",
      "science.scitldr_aic:\n",
      "====================================================================================================\n",
      "USER: You will be shown the text of the abstract, introduction, and conclusion of a scientific paper. Please summarize the key findings of the work in 1-2 sentences.\n",
      "\n",
      "Paper text: Network embedding (NE) methods aim to learn low-dimensional representations of network nodes as vectors, typically in Euclidean space.\n",
      "These representations are then used for a variety of downstream prediction tasks.\n",
      "Link prediction is one of the most popular choices for assessing the performance of NE methods.\n",
      "However, the complexity of link prediction requires a carefully designed evaluation pipeline to provide consistent, reproducible and comparable results.\n",
      "We argue this has not been considered sufficiently in recent works.\n",
      "The main goal of this paper is to overcome difficulties associated with evaluation pipelines and reproducibility of results.\n",
      "We introduce EvalNE, an evaluation framework to transparently assess and compare the performance of NE methods on link prediction.\n",
      "EvalNE provides automation and abstraction for tasks such as hyper-parameter tuning, model validation, edge sampling, computation of edge embeddings and model validation.\n",
      "The framework integrates efficient procedures for edge and non-edge sampling and can be used to easily evaluate any off-the-shelf embedding method.\n",
      "The framework is freely available as a Python toolbox.\n",
      "Finally, demonstrating the usefulness of EvalNE in practice, we conduct an empirical study in which we try to replicate and analyse experimental sections of several influential papers.\n",
      "Link prediction is an important task with applications in a wide range of fields such as computer science, social sciences, biology, and medicine BID6 BID14 BID15 BID22 .\n",
      "It amounts to estimating the likelihood for the existence of edges, between pairs of nodes that do not form an edge in the input graph.\n",
      "Many Network Embedding (NE) methods (e.g., BID0 BID2 BID5 BID8 BID10 BID12 BID17 BID18 BID19 have recently been applied to solving link prediction problems, showing promising results. These methods map nodes in the network to vectors in IR d . This embedding is then used for a variety of tasks such as visualization, multi-label classification, clustering or link prediction.The challenges of evaluating NE methods for link prediction We argue that the practical performance of most NE methods is poorly understood and that experiments in many papers are difficult to compare due to variation in experimental setup and evaluation procedures. In this paper, we focus on a number of difficulties specific to the evaluation of NE methods for link prediction. Link prediction is a particularly challenging task to evaluate as it involve a number design choices, which can confound the results and are prone to errors.1) Train-test splitting of graphs For example, a typical implicit assumption is that the input graph is not complete, and the purpose is to accurately predict the missing edges.\n",
      "To evaluate the performance of an NE method for link prediction, one thus needs an (incomplete) training graph along with a (more) complete version of that graph for testing.\n",
      "Much research has been devoted to determining the best approach to generate these training graphs BID6 BID14 BID22 .\n",
      "Strong theoretical and empirical evidence suggest that in order to fairly evaluate link prediction methods, snapshots of the network at different points in time should be used for training and testing.\n",
      "In this way, the link prediction methods are tested on the natural evolutions of the networks.\n",
      "However, the availability of such snapshots is uncommon and raises additional questions, such as how to choose the time intervals for splitting the network.For these reasons, authors typically resort to sampling sets of edges from the input graphs and using the resulting sub-graphs for training BID5 BID8 BID10 BID12 .\n",
      "The remaining edges are used as positive test examples.\n",
      "The process of sampling edges is not standardized and varies between scientific works.\n",
      "The relative sizes of the train and test sets, for example, is a user-defined parameter which varies significantly.\n",
      "In BID8 ; BID10 the authors use a 50-50 train-test split, in BID5 ) a 60-40, in Lai et al. (2017 an 80-20 and in BID20 values ranging from 30-70 up to 80-20.A related problem is that, in addition to the 'positive' train and test edges, often also 'negative' edges (or non-edges) are required.\n",
      "Sometimes these are used to derive the embedding, while in other cases they are used only to train the classifier that predicts links.\n",
      "These sets of non-edges can be selected according to different strategies (Kotnis & Nastase) and can be of various sizes.2) From node embeddings to edge predictions Furthermore, most NE methods simply provide node embeddings.\n",
      "From these, edge embeddings need to be derived prior to performing predictions.\n",
      "There are several approaches for deriving edge embeddings which also seem to have a strong impact on the performance of different methods BID8 .3) Evaluation measures Also the metrics used to evaluate the accuracy varies, e.g., from AUC-ROC BID10 , to precision-recall BID21 , to precision@k BID20 .\n",
      "The recent surge of research in the area of network embeddings has resulted in a wide variety of data sets, metrics, and setups for evaluating and comparing the utility of embedding methods.\n",
      "Comparability across studies is lacking and not all evaluations are equally sound.\n",
      "This highlights the need for specific tools and pipelines to ensure the correct evaluation of these methods.\n",
      "Particularly, the use of representation learning for link prediction tasks requires train and test sampling, non-edge sampling, and in many cases selection of edge embedding methods and binary classifiers.\n",
      "The evaluation procedure, thus, becomes an ensemble of tasks which allow for many errors or inconsistencies.In this work we have proposed EvalNE, a novel framework that can be used to evaluate any network embedding method for link prediction.\n",
      "Our pipeline automates the selection of train and test edge sets, simplifies the process of tuning model parameters and reports the accuracy of the methods according to many criteria.\n",
      "Our experiments highlight the importance of the edge sampling strategy and parameter tuning for evaluating NE methods.\n",
      "We have also introduced a scalable procedure to select edge sets from given networks and showed empirically that is orders or magnitude faster than the naive approaches used in recent literature.\n",
      "\n",
      "Summary: \n",
      "ASSISTANT: In this paper we introduce EvalNE, a Python toolbox for automating the evaluation of network embedding methods on link prediction and ensuring the reproducibility of results.\n",
      "\n",
      "--------------------------------------------------\n",
      "USER: You will be shown the text of the abstract, introduction, and conclusion of a scientific paper. Please summarize the key findings of the work in 1-2 sentences.\n",
      "\n",
      "Paper text: People ask questions that are far richer, more informative, and more creative than current AI systems.\n",
      "We propose a neural program generation framework for modeling human question asking, which represents questions as formal programs and generates programs with an encoder-decoder based deep neural network.\n",
      "From extensive experiments using an information-search game, we show that our method can ask optimal questions in synthetic settings, and predict which questions humans are likely to ask in unconstrained settings.\n",
      "We also propose a novel grammar-based question generation framework trained with reinforcement learning, which is able to generate creative questions without supervised data.\n",
      "People can ask rich, creative questions to learn efficiently about their environment.\n",
      "Question asking is central to human learning yet it is a tremendous challenge for computational models.\n",
      "There is always an infinite set of possible questions that one can ask, leading to challenges both in representing the space of questions and in searching for the right question to ask.\n",
      "Machine learning has been used to address aspects of this challenge.\n",
      "Traditional methods have used heuristic rules designed by humans (Heilman & Smith, 2010; Chali & Hasan, 2015) , which are usually restricted to a specific domain.\n",
      "Recently, neural network approaches have also been proposed, including retrieval methods which select the best question from past experience (Mostafazadeh et al., 2016 ) and encoder-decoder frameworks which map visual or linguistic inputs to questions (Serban et al., 2016; Mostafazadeh et al., 2016; Yuan et al., 2017; Yao et al., 2018) .\n",
      "While effective in some settings, these approaches do not consider settings where the questions are asked about partially unobservable states.\n",
      "Besides, these methods are heavily data-driven, limiting the diversity of generated questions and requiring large training sets for different goals and contexts.\n",
      "There is still a large gap between how people and machines ask questions.\n",
      "Recent work has aimed to narrow this gap by taking inspiration from cognitive science.\n",
      "For instance, Lee et al. (2018) incorporates aspects of \"theory of mind\" (Premack & Woodruff, 1978) in question asking by simulating potential answers to the questions, but the approach relies on imperfect agents for natural language understanding which may lead to error propagation.\n",
      "Related to our approach, Rothe et al. (2017) proposed a powerful question-asking framework by modeling questions as symbolic programs, but their algorithm relies on hand-designed program features and requires expensive calculations to ask questions.\n",
      "We use \"neural program generation\" to bridge symbolic program generation and deep neural networks, bringing together some of the best qualities of both approaches.\n",
      "Symbolic programs provide a compositional \"language of thought\" (Fodor, 1975) for creatively synthesizing which questions to ask, allowing the model to construct new ideas based on familiar building blocks.\n",
      "Compared to natural language, programs are precise in their semantics, have clearer internal structure, and require a much smaller vocabulary, making them an attractive representation for question answering systems as well (Johnson et al., 2017; Yi et al., 2018; Mao et al., 2019) .\n",
      "However, there has been much less work using program synthesis for question asking, which requires searching through infinitely many questions (where many questions may be informative) rather than producing a single correct answer to a question.\n",
      "Deep neural networks allow for rapid question-synthesis using encoder-decoder modeling, eliminating the need for the expensive symbolic search and feature evaluations in Rothe et al. (2017) .\n",
      "Together, the questions can be synthesized quickly and evaluated formally for quality groundtruth board partly revealed board example questions\n",
      "How long is the red ship?\n",
      "(size Red)\n",
      "Is purple ship horizontal?\n",
      "(== (orient Purple)\n",
      "H) Do all three ships have the same size?\n",
      "(=== (map (λ x (size x)) (set AllShips)))\n",
      "Figure 1: The Battleship task.\n",
      "Blue, red, and purple tiles are ships, dark gray tiles are water, and light gray tiles are hidden.\n",
      "The agent can see a partly revealed board, and should ask a question to seek information about the hidden board.\n",
      "Example questions and translated programs are shown on the right.\n",
      "We recommend viewing the figures in color.\n",
      "(e.g. the expected information gain), which as we show can be used to train question asking systems using reinforcement learning.\n",
      "In this paper, we develop a neural program generation model for asking questions in an informationsearch game similar to \"Battleship\" used in previous work (Gureckis & Markant, 2009; Rothe et al., 2017; .\n",
      "The model uses a convolutional encoder to represent the game state, and a Transformer decoder (Vaswani et al., 2017) for generating questions.\n",
      "Building on the work of Rothe et al. (2017) , the model uses a grammar-enhanced question asking framework, such that questions as programs are formed through derivation using a context free grammar.\n",
      "Importantly, we show that the model can be trained from human demonstrations of good questions using supervised learning, along with a data augmentation procedure that leverages previous work to produce additional human-like questions for training.\n",
      "Our model can also be trained without such demonstrations using reinforcement learning.\n",
      "We evaluate the model on several aspects of human question asking, including reasoning about optimal questions in synthetic scenarios, density estimation based on free-form question asking, and creative generation of genuinely new questions.\n",
      "To summarize, our paper makes three main contributions:\n",
      "1) We propose a neural network for modeling human question-asking behavior,\n",
      "2) We propose a novel reinforcement learning framework for generating creative human-like questions by exploiting the power of programs, and\n",
      "3) We evaluate different properties of our methods extensively through three different experiments.\n",
      "We train our model in a fully supervised fashion.\n",
      "Accuracy for the counting and missing tile tasks is shown in Figure 3 .\n",
      "The full neural program generation model shows strong reasoning abilities, achieving high accuracy for both the counting and missing tile tasks, respectively.\n",
      "We also perform ablation analysis of the encoder filters of the model, and provide the results in Appendix D.\n",
      "The results for the compositionality task are summarized in Table 1 .\n",
      "When no training data regarding the held out question type is provided, the model cannot generalize to situations systematically different from training data, exactly as pointed out in previous work on the compositional skills of encoder-decoder models (Lake & Baroni, 2018) .\n",
      "However, when the number of additional training data increases, the model quickly incorporates the new question type while maintaining high accuracy on the familiar question tasks.\n",
      "On the last row of Table 1 , we compare our model with another version where the decoder is replaced by two linear transformation operations which directly classify the ship type and location (details in Appendix B.1).\n",
      "This model has 33.0% transfer accuracy on compositional scenarios never seen during training.\n",
      "This suggests that the model has the potential to generalize to unseen scenarios if the task can be decomposed to subtasks and combined together.\n",
      "We evaluate the log-likelihood of reference questions generated by our full model as well as some lesioned variants of the full model, including a model without pretraining, a model with the Transformer decoder replaced by an LSTM decoder, a model with the convolutional encoder replaced by a simple MLP encoder, and a model that only has a decoder (unconditional language model).\n",
      "Though the method from Rothe et al. (2017) also works on this task, here we cannot compare with their method for two reasons.\n",
      "One is that our dataset is constructed using their method, so the likelihood of their method should be an upper bound in our evaluation setting.\n",
      "Additionally, they can only approximate the log-likelihood due to an intractable normalizing constant, and thus it difficult to directly compare with our methods.\n",
      "Two different evaluation sets are used, one is sampled from the same process on new boards, the other is a small set of questions collected from human annotators.\n",
      "In order to calculate the log-likelihood of human questions, we use translated versions of these questions that were used in previous work (Rothe et al., 2017) , and filtered some human questions that score poorly according to the generative model used for training the neural network (Appendix B.2).\n",
      "A summary of the results is shown in Table 2a .\n",
      "The full model performs best on both datasets, suggesting that pretraining, the Transformer decoder, and the convolutional encoder are all important components of the approach.\n",
      "However, we find that the model without an encoder performs reasonably well too, even out-performing the full model with a LSTM-decoder on the human-produced questions.\n",
      "This suggests that while contextual information from the board leads to improvements, it is not the most important factor for predicting human questions.\n",
      "To further investigate the role of contextual information and whether or not the model can utilize it effectively, we conduct another analysis.\n",
      "Intuitively, if there is little uncertainty about the locations of the ships, observing the board is critical since there are fewer good questions to ask.\n",
      "To examine this factor, we divide the scenarios based on the entropy of the hypothesis space of possible ship locations into a low entropy set (bottom 30%), medium entropy set (40% in the middle), and high entropy set (top 30%).\n",
      "We evaluate different models on the split sets of sampled data and report the results in Table 2b .\n",
      "When the entropy is high, it is easier to ask a generally good question like \"how long is the red ship\" without information of the board, so the importance of the encoder is reduced.\n",
      "If entropy is low, the models with access to the board has substantially higher log-likelihood than the model without encoder.\n",
      "Also, the first experiment (section 5.1) would be impossible without an encoder.\n",
      "Together, this implies that our model can capture important context-sensitive characteristics of how people ask questions.\n",
      "The models are evaluated on 2000 randomly sampled boards, and the results are shown in Table  3 .\n",
      "Note that any ungrammatical questions are excluded when we calculate the number of unique questions.\n",
      "First, when the text-based model is evaluated on new contexts, 96.3% of the questions it generates were included in the training data.\n",
      "We also find that the average EIG and the ratio of EIG>0 is worse than the supervised model trained on programs.\n",
      "Some of these deficiencies are due to the very limited text-based training data, but using programs instead can help overcome these limitations.\n",
      "With the program-based framework, we can sample new boards and questions to create a much larger dataset with executable program representations.\n",
      "This self-supervised training helps to boost performance, especially when combined with grammar-enhanced RL.\n",
      "From the table, the grammar-enhanced RL model is able to generate informative and creative questions.\n",
      "It can be trained from scratch without examples of human questions, and produces many novel questions with high EIG.\n",
      "In contrast, the supervised model rarely produces new questions beyond the training set.\n",
      "The sequence-level RL model is also comparatively weak at generating novel questions, perhaps because it is also pre-trained on human questions.\n",
      "It also more frequently generates ungrammatical questions.\n",
      "We also provide examples in Figure 4 to show the diversity of questions generated by the grammar enhanced model, and more in the supplementary materials.\n",
      "Figure 4a shows novel questions the model produces, which includes clever questions such as \"Where is the bottom right of all the purple and blue tiles?\" or \"What is the size of the blue ship minus the purple ship?\", while it can also sometimes generates meaningless questions such as \"Is the blue ship shorter than itself?\"\n",
      "Additional examples of generated questions are provided in Appendix B. Is any ship two tiles long?\n",
      "(> (++ (map (lambda x (== (size x) 2)) (set AllShips))) 0)\n",
      "Are there any ships in row 1?\n",
      "(> (++ (map (lambda y (and (== (rowL y) 1) (not (== (color y) Water)))) (set AllTiles))) 0)\n",
      "Is part of a ship on tile 4-6?\n",
      "(not (== (color 4-6)\n",
      "Water)) What is the size of the blue ship?\n",
      "(setSize (coloredTiles Blue))\n",
      "What is the size of the purple ship?\n",
      "(size Purple)\n",
      "Which column is the first part of the blue ship?\n",
      "(colL (topleft (coloredTiles Blue)))\n",
      "What is the orientation of the blue ship?\n",
      "With the grammar enhanced framework, we can also guide the model to ask different types of questions, consistent with the goal-directed nature and flexibility of human question asking.\n",
      "The model can be queried for certain types of questions by providing different start conditions to the model.\n",
      "Instead of starting derivation from the start symbol \"A\", we can start derivation from a intermediate state such as \"B\" for Boolean questions or a more complicated \"(and B B)\" for composition of two Boolean questions.\n",
      "In Figure 4b , we show examples where the model is asked to generate four specific types of questions: true/false questions, number questions, location-related questions, and compositional true/false questions.\n",
      "We see that the model can flexibly adapt to new constraints and generate meaningful questions.\n",
      "In Figure 4c , we compare the model generated questions with human questions, each randomlysampled from the model outputs and the human dataset.\n",
      "These examples again demonstrate that our model is able to generate clever and human-like questions.\n",
      "However, we also find that people sometimes generate questions with quantifiers such as \"any\" and \"all\", which are operationalized in program form with lambda functions.\n",
      "These questions are complicated in representation and not favored by our model, showing a current limitation in our model's capacity.\n",
      "We introduce a neural program generation framework for question asking task under partially unobservable settings, which is able to generate creative human-like questions with human question demonstrations by supervised learning or without demonstrations by grammar-enhanced reinforcement learning.\n",
      "Programs provide models with a \"machine language of thought\" for compositional thinking, and neural networks provide an efficient means of question generation.\n",
      "We demonstrate the effectiveness of our method in extensive experiments covering a range of human question asking abilities.\n",
      "The current model has important limitations.\n",
      "It cannot generalize to systematically different scenarios, and it sometimes generates meaningless questions.\n",
      "We plan to further explore the model's compositional abilities in future work.\n",
      "Another promising direction is to model question asking and question answering jointly within one framework, which could guide the model to a richer sense of the question semantics.\n",
      "Besides, allowing the agent to iteratively ask questions and try to win the game is another interesting future direction.\n",
      "We would also like to use our framework in dialog systems and open-ended question asking scenarios, allowing such systems to synthesize informative and creative questions.\n",
      "\n",
      "Summary: \n",
      "ASSISTANT: We introduce a model of human question asking that combines neural networks and symbolic programs, which can learn to generate good questions with or without supervised examples.\n",
      "\n",
      "--------------------------------------------------\n",
      "USER: You will be shown the text of the abstract, introduction, and conclusion of a scientific paper. Please summarize the key findings of the work in 1-2 sentences.\n",
      "\n",
      "Paper text: The high computational and parameter complexity of neural networks makes their training very slow and difficult to deploy on energy and storage-constrained comput- ing systems.\n",
      "Many network complexity reduction techniques have been proposed including fixed-point implementation.\n",
      "However, a systematic approach for design- ing full fixed-point training and inference of deep neural networks remains elusive.\n",
      "We describe a precision assignment methodology for neural network training in which all network parameters, i.e., activations and weights in the feedforward path, gradients and weight accumulators in the feedback path, are assigned close to minimal precision.\n",
      "The precision assignment is derived analytically and enables tracking the convergence behavior of the full precision training, known to converge a priori.\n",
      "Thus, our work leads to a systematic methodology of determining suit- able precision for fixed-point training.\n",
      "The near optimality (minimality) of the resulting precision assignment is validated empirically for four networks on the CIFAR-10, CIFAR-100, and SVHN datasets.\n",
      "The complexity reduction arising from our approach is compared with other fixed-point neural network designs.\n",
      "Though deep neural networks (DNNs) have established themselves as powerful predictive models achieving human-level accuracy on many machine learning tasks BID12 , their excellent performance has been achieved at the expense of a very high computational and parameter complexity.\n",
      "For instance, AlexNet BID17 requires over 800 × 10 6 multiply-accumulates (MACs) per image and has 60 million parameters, while Deepface (Taigman et al., 2014) requires over 500 × 10 6 MACs/image and involves more than 120 million parameters.\n",
      "DNNs' enormous computational and parameter complexity leads to high energy consumption BID4 , makes their training via the stochastic gradient descent (SGD) algorithm very slow often requiring hours and days BID9 , and inhibits their deployment on energy and resource-constrained platforms such as mobile devices and autonomous agents.A fundamental problem contributing to the high computational and parameter complexity of DNNs is their realization using 32-b floating-point (FL) arithmetic in GPUs and CPUs.\n",
      "Reduced-precision representations such as quantized FL (QFL) and fixed-point (FX) have been employed in various combinations to both training and inference.\n",
      "Many employ FX during inference but train in FL, e.g., fully binarized neural networks BID13 use 1-b FX in the forward inference path but the network is trained in 32-b FL.\n",
      "Similarly, BID10 employs 16-b FX for all tensors except for the internal accumulators which use 32-b FL, and 3-level QFL gradients were employed (Wen et al., 2017; BID0 to accelerate training in a distributed setting. Note that while QFL reduces storage and communication costs, it does not reduce the computational complexity as the arithmetic remains in 32-b FL.Thus, none of the previous works address the fundamental problem of realizing true fixed-point DNN training, i.e., an SGD algorithm in which all parameters/variables and all computations are implemented in FX with minimum precision required to guarantee the network's inference/prediction accuracy and training convergence. The reasons for this gap are numerous including: 1) quantization Step 1: Forward PropagationStep 2: Back PropagationStep 3: Update errors propagate to the network output thereby directly affecting its accuracy (Lin et al., 2016) ; 2) precision requirements of different variables in a network are interdependent and involve hard-toquantify trade-offs (Sakr et al., 2017) ; 3) proper quantization requires the knowledge of the dynamic range which may not be available (Pascanu et al., 2013) ; and 4) quantization errors may accumulate during training and can lead to stability issues BID10 .Our\n",
      "work makes a major advance in closing this gap by proposing a systematic methodology to obtain close-to-minimum per-layer precision requirements of an FX network that guarantees statistical similarity with full precision training. In\n",
      "particular, we jointly address the challenges of quantization noise, inter-layer and intra-layer precision trade-offs, dynamic range, and stability. As\n",
      "in (Sakr et al., 2017) , we do assume that a fully-trained baseline FL network exists and one can observe its learning behavior. While\n",
      ", in principle, such assumption requires extra FL computation prior to FX training, it is to be noted that much of training is done in FL anyway. For\n",
      "instance, FL training is used in order to establish benchmarking baselines such as AlexNet BID17 , VGG-Net (Simonyan and Zisserman, 2014) , and ResNet BID12 , to name a few. Even\n",
      "if that is not the case, in practice, this assumption can be accounted for via a warm-up FL training on a small held-out portion of the dataset BID6 .Applying\n",
      "our methodology to three benchmarks reveals several lessons. First and\n",
      "foremost, our work shows that it is possible to FX quantize all variables including back-propagated gradients even though their dynamic range is unknown BID15 . Second, we\n",
      "find that the per-layer weight precision requirements decrease from the input to the output while those of the activation gradients and weight accumulators increase. Furthermore\n",
      ", the precision requirements for residual networks are found to be uniform across layers. Finally, hyper-precision\n",
      "reduction techniques such as weight and activation binarization BID13 or gradient ternarization (Wen et al., 2017) are not as efficient as our methodology since these do not address the fundamental problem of realizing true fixed-point DNN training.We demonstrate FX training on three deep learning benchmarks (CIFAR-10, CIFAR-100, SVHN) achieving high fidelity to our FL baseline in that we observe no loss of accuracy higher then 0.56% in all of our experiments. Our precision assignment\n",
      "is further shown to be within 1-b per-tensor of the minimum. We show that our precision\n",
      "assignment methodology reduces representational, computational, and communication costs of training by up to 6×, 8×, and 4×, respectively, compared to the FL baseline and related works.\n",
      "In this paper, we have presented a study of precision requirements in a typical back-propagation based training procedure of neural networks.\n",
      "Using a set of quantization criteria, we have presented a precision assignment methodology for which FX training is made statistically similar to the FL baseline, known to converge a priori.\n",
      "We realized FX training of four networks on the CIFAR-10, CIFAR-100, and SVHN datasets and quantified the associated complexity reduction gains in terms costs of training.\n",
      "We also showed that our precision assignment is nearly minimal.The presented work relies on the statistics of all tensors being quantized during training.\n",
      "This necessitates an initial baseline run in floating-point which can be costly.\n",
      "An open problem is to predict a suitable precision configuration by only observing the data statistics and the network architecture.\n",
      "Future work can leverage the analysis presented in this paper to enhance the effectiveness of other network complexity reduction approaches.\n",
      "For instance, weight pruning can be viewed as a coarse quantization process (quantize to zero) and thus can potentially be done in a targeted manner by leveraging the information provided by noise gains.\n",
      "Furthermore, parameter sharing and clustering can be viewed as a form of vector quantization which presents yet another opportunity to leverage our method for complexity reduction.\n",
      "\n",
      "Summary: \n",
      "ASSISTANT: We analyze and determine the precision requirements for training neural networks when all tensors, including back-propagated signals and weight accumulators, are quantized to fixed-point format.\n",
      "\n",
      "--------------------------------------------------\n",
      "====================================================================================================\n",
      "science.scierc_ner:\n",
      "====================================================================================================\n",
      "USER: You are a scientific research assistant. You will be shown an abstract from a computer science research paper. Your task is to extract the unique named entities in this research abstract, and to label each entity with one of the following entity types: ['Generic', 'Material', 'Method', 'Metric', 'OtherScientificTerm', 'Task'].\n",
      "\n",
      "Please format your output as a JSON array. Each entry in the array should be a string specifying the type of a single entity, formatted as: \"<entity_name>: <entity_type>\".\n",
      "\n",
      "Here is the passage:\n",
      "\n",
      "This paper investigates some computational problems associated with probabilistic translation models that have recently been adopted in the literature on machine translation. These models can be viewed as pairs of probabilistic context-free grammars working in a ` synchronous ' way. Two hardness results for the class NP are reported, along with an exponential time lower-bound for certain classes of algorithms that are currently used in the literature.  \n",
      "\n",
      "\n",
      "\n",
      "ASSISTANT: [\"probabilistic translation models: Method\", \"machine translation: Task\", \"probabilistic context-free grammars: Method\", \"exponential time lower-bound: OtherScientificTerm\", \"computational problems: OtherScientificTerm\"]\n",
      "\n",
      "--------------------------------------------------\n",
      "USER: You are a scientific research assistant. You will be shown an abstract from a computer science research paper. Your task is to extract the unique named entities in this research abstract, and to label each entity with one of the following entity types: ['Generic', 'Material', 'Method', 'Metric', 'OtherScientificTerm', 'Task'].\n",
      "\n",
      "Please format your output as a JSON array. Each entry in the array should be a string specifying the type of a single entity, formatted as: \"<entity_name>: <entity_type>\".\n",
      "\n",
      "Here is the passage:\n",
      "\n",
      "We propose a detection method for orthographic variants caused by transliteration in a large corpus. The method employs two similarities. One is string similarity based on edit distance. The other is contextual similarity by a vector space model. Experimental results show that the method performed a 0.889 F-measure in an open test.  \n",
      "\n",
      "\n",
      "\n",
      "ASSISTANT: [\"detection method: Method\", \"string similarity: OtherScientificTerm\", \"transliteration: Task\", \"edit distance: Method\", \"vector space model: Method\", \"orthographic variants: OtherScientificTerm\", \"similarities: Generic\", \"contextual similarity: OtherScientificTerm\", \"F-measure: Metric\"]\n",
      "\n",
      "--------------------------------------------------\n",
      "USER: You are a scientific research assistant. You will be shown an abstract from a computer science research paper. Your task is to extract the unique named entities in this research abstract, and to label each entity with one of the following entity types: ['Generic', 'Material', 'Method', 'Metric', 'OtherScientificTerm', 'Task'].\n",
      "\n",
      "Please format your output as a JSON array. Each entry in the array should be a string specifying the type of a single entity, formatted as: \"<entity_name>: <entity_type>\".\n",
      "\n",
      "Here is the passage:\n",
      "\n",
      "Two themes have evolved in speech and text image processing work at Xerox PARC that expand and redefine the role of recognition technology in document-oriented applications. One is the development of systems that provide functionality similar to that of text processors but operate directly on audio and scanned image data. A second, related theme is the use of speech and text-image recognition to retrieve arbitrary, user-specified information from documents with signal content. This paper discusses three research initiatives at PARC that exemplify these themes: a text-image editor -LSB- 1 -RSB-, a wordspotter for voice editing and indexing -LSB- 12 -RSB-, and a decoding framework for scanned-document content retrieval -LSB- 4 -RSB-. The discussion focuses on key concepts embodied in the research that enable novel signal-based document processing functionality.  \n",
      "\n",
      "\n",
      "\n",
      "ASSISTANT: [\"recognition technology: Method\", \"speech and text image processing: Task\", \"text processors: Method\", \"wordspotter: Method\", \"audio and scanned image data: Material\", \"theme: Generic\", \"decoding framework: Method\", \"document-oriented applications: Task\", \"text-image editor: Method\", \"speech and text-image recognition: Method\", \"documents with signal content: OtherScientificTerm\", \"scanned-document content retrieval: Task\", \"themes: Generic\", \"research: Generic\", \"One: Generic\", \"voice editing and indexing: Task\", \"signal-based document processing functionality: Task\"]\n",
      "\n",
      "--------------------------------------------------\n",
      "====================================================================================================\n",
      "science.scierc_relation:\n",
      "====================================================================================================\n",
      "USER: You are a scientific research assistant. You will be shown an abstract from a computer science research paper. Your task is to identify all the relationships that occur between entities in the text, and to label each relationship.\n",
      "\n",
      "Please format your output as a JSON array. Each entry in the array should itself by an array formatted as [entity_1, relation, entity_2]. The entities should be extracted directly from the text. The relations should be one of the following types: [\"COMPARE\", \"CONJUNCTION\", \"EVALUATE-FOR\", \"FEATURE-OF\", \"HYPONYM-OF\", \"PART-OF\", \"USED-FOR\"]. If you find a relation that is not one of these types, do not include it.\n",
      "\n",
      "Here is the passage:\n",
      "\n",
      "In this paper we study a set of problems that are of considerable importance to Statistical Machine Translation -LRB- SMT -RRB- but which have not been addressed satisfactorily by the SMT research community. Over the last decade, a variety of SMT algorithms have been built and empirically tested whereas little is known about the computational complexity of some of the fundamental problems of SMT. Our work aims at providing useful insights into the the computational complexity of those problems. We prove that while IBM Models 1-2 are conceptually and computationally simple, computations involving the higher -LRB- and more useful -RRB- models are hard. Since it is unlikely that there exists a polynomial time solution for any of these hard problems -LRB- unless P = NP and P #P = P -RRB-, our results highlight and justify the need for developing polynomial time approximations for these computations. We also discuss some practical ways of dealing with complexity.  \n",
      "\n",
      "\n",
      "\n",
      "ASSISTANT: [[\"problems\", \"PART-OF\", \"Statistical Machine Translation -LRB- SMT -RRB-\"], [\"polynomial time approximations\", \"USED-FOR\", \"computations\"], [\"computational complexity\", \"EVALUATE-FOR\", \"problems\"], [\"polynomial time solution\", \"USED-FOR\", \"problems\"], [\"IBM Models 1-2\", \"COMPARE\", \"models\"]]\n",
      "\n",
      "--------------------------------------------------\n",
      "USER: You are a scientific research assistant. You will be shown an abstract from a computer science research paper. Your task is to identify all the relationships that occur between entities in the text, and to label each relationship.\n",
      "\n",
      "Please format your output as a JSON array. Each entry in the array should itself by an array formatted as [entity_1, relation, entity_2]. The entities should be extracted directly from the text. The relations should be one of the following types: [\"COMPARE\", \"CONJUNCTION\", \"EVALUATE-FOR\", \"FEATURE-OF\", \"HYPONYM-OF\", \"PART-OF\", \"USED-FOR\"]. If you find a relation that is not one of these types, do not include it.\n",
      "\n",
      "Here is the passage:\n",
      "\n",
      "We introduce a new interactive corpus exploration tool called InfoMagnets. InfoMagnets aims at making exploratory corpus analysis accessible to researchers who are not experts in text mining. As evidence of its usefulness and usability, it has been used successfully in a research context to uncover relationships between language and behavioral patterns in two distinct domains: tutorial dialogue -LRB- Kumar et al., submitted -RRB- and on-line communities -LRB- Arguello et al., 2006 -RRB-. As an educational tool, it has been used as part of a unit on protocol analysis in an Educational Research Methods course.  \n",
      "\n",
      "\n",
      "\n",
      "ASSISTANT: [[\"InfoMagnets\", \"HYPONYM-OF\", \"interactive corpus exploration tool\"], [\"tutorial dialogue\", \"HYPONYM-OF\", \"domains\"], [\"InfoMagnets\", \"USED-FOR\", \"domains\"], [\"tutorial dialogue\", \"CONJUNCTION\", \"on-line communities\"], [\"on-line communities\", \"HYPONYM-OF\", \"domains\"], [\"InfoMagnets\", \"USED-FOR\", \"protocol analysis\"], [\"InfoMagnets\", \"USED-FOR\", \"exploratory corpus analysis\"]]\n",
      "\n",
      "--------------------------------------------------\n",
      "USER: You are a scientific research assistant. You will be shown an abstract from a computer science research paper. Your task is to identify all the relationships that occur between entities in the text, and to label each relationship.\n",
      "\n",
      "Please format your output as a JSON array. Each entry in the array should itself by an array formatted as [entity_1, relation, entity_2]. The entities should be extracted directly from the text. The relations should be one of the following types: [\"COMPARE\", \"CONJUNCTION\", \"EVALUATE-FOR\", \"FEATURE-OF\", \"HYPONYM-OF\", \"PART-OF\", \"USED-FOR\"]. If you find a relation that is not one of these types, do not include it.\n",
      "\n",
      "Here is the passage:\n",
      "\n",
      "In a motorized vehicle a number of easily measurable signals with frequency components related to the rotational speed of the engine can be found, e.g., vibrations, electrical system voltage level, and ambient sound. These signals could potentially be used to estimate the speed and related states of the vehicle. Unfortunately, such estimates would typically require the relations -LRB- scale factors -RRB- between the frequency components and the speed for different gears to be known. Consequently, in this article we look at the problem of estimating these gear scale factors from training data consisting only of speed measurements and measurements of the signal in question. The estimation problem is formulated as a maximum likelihood estimation problem and heuristics is used to find initial values for a numerical evaluation of the estimator. Finally, a measurement campaign is conducted and the functionality of the estimation method is verified on real data.  \n",
      "\n",
      "\n",
      "\n",
      "ASSISTANT: [[\"heuristics\", \"USED-FOR\", \"numerical evaluation of the estimator\"], [\"frequency components\", \"USED-FOR\", \"measurable signals\"], [\"real data\", \"EVALUATE-FOR\", \"estimator\"], [\"electrical system voltage level\", \"HYPONYM-OF\", \"measurable signals\"], [\"speed\", \"FEATURE-OF\", \"gears\"], [\"frequency components\", \"CONJUNCTION\", \"speed\"], [\"maximum likelihood estimation problem\", \"USED-FOR\", \"estimation problem\"], [\"frequency components\", \"FEATURE-OF\", \"rotational speed of the engine\"], [\"measurable signals\", \"USED-FOR\", \"speed and related states of the vehicle\"], [\"training data\", \"USED-FOR\", \"gear scale factors\"], [\"vibrations\", \"CONJUNCTION\", \"electrical system voltage level\"], [\"vibrations\", \"HYPONYM-OF\", \"measurable signals\"], [\"electrical system voltage level\", \"CONJUNCTION\", \"ambient sound\"], [\"ambient sound\", \"HYPONYM-OF\", \"measurable signals\"]]\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print 3 examples from each dataset\n",
    "for dataset in tulu['dataset'].unique():\n",
    "    print(\"=\"*100)\n",
    "    print(f'{dataset}:')\n",
    "    print(\"=\"*100)\n",
    "    examples = tulu[tulu['dataset'] == dataset].sample(3)\n",
    "    for i, example in examples.iterrows():\n",
    "        print(f'{example[\"prompt\"]}')\n",
    "        print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check how much english is in each subset.\n",
    "(This was originally run with 1000 samples, but 100 is enough for similar results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flan_v2: 0.77\n",
      "cot: 1.0\n",
      "oasst1: 0.3\n",
      "lima: 1.0\n",
      "gpt4_alpaca: 1.0\n",
      "code_alpaca: 0.99\n",
      "sharegpt: 0.78\n",
      "wizardlm: 0.99\n",
      "open_orca: 0.99\n",
      "science.evidence_inference: 1.0\n",
      "science.qasper_truncated_4000: 1.0\n",
      "science.scifact_json: 1.0\n",
      "science.scitldr_aic: 1.0\n",
      "science.scierc_ner: 1.0\n",
      "science.scierc_relation: 1.0\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "def is_en(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "# For each dataset print the percentage of English prompts for 1000 samples\n",
    "for dataset in tulu['dataset'].unique():\n",
    "    print(f'{dataset}: {tulu[tulu[\"dataset\"] == dataset].sample(100)[\"prompt\"].apply(is_en).mean()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnlp-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

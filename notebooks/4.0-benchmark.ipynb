{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ™Œ Benchmark\n",
    "\n",
    "In this notebook we are are benchmarking the MCQ performance of our fine-tuned Phi-3 models and the base Phi-3 models. We follow the methodology outlined in `EVAL.md` and load the results from the folder `model/results`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "\n",
    "---\n",
    "\n",
    "Let's install some necessary dependencies and set global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable R magic\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autorootcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# R Modules\n",
    "library(ggplot2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Styling options\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change index\n",
    "INDEX_SELECTOR = {\n",
    "    \"openbookqa\": (\"OBQA\", \"OBQA\"),\n",
    "    \"mmlu_abstract_algebra\": (\"MMLU\", \"Abstract Algebra\"),\n",
    "    \"mmlu_anatomy\": (\"MMLU\", \"Anatomy\"), \n",
    "    \"mmlu_astronomy\": (\"MMLU\", \"Astronomy\"),\n",
    "    \"mmlu_college_biology\": (\"MMLU\", \"College Biology\"),\n",
    "    \"mmlu_college_chemistry\": (\"MMLU\", \"College Chemistry\"),\n",
    "    \"mmlu_college_computer_science\": (\"MMLU\", \"College Computer Science\"),\n",
    "    \"mmlu_college_mathematics\": (\"MMLU\", \"College Mathematics\"),\n",
    "    \"mmlu_college_physics\": (\"MMLU\", \"College Physics\"),\n",
    "    \"mmlu_computer_security\": (\"MMLU\", \"Computer Security\"),\n",
    "    \"mmlu_conceptual_physics\": (\"MMLU\", \"Conceptual Physics\"),\n",
    "    \"mmlu_electrical_engineering\": (\"MMLU\", \"Electrical Engineering\"),\n",
    "    \"mmlu_elementary_mathematics\": (\"MMLU\", \"Elementary Mathematics\"),\n",
    "    \"mmlu_high_school_biology\": (\"MMLU\", \"High School Biology\"),\n",
    "    \"mmlu_high_school_chemistry\": (\"MMLU\", \"High School Chemistry\"),\n",
    "    \"mmlu_high_school_computer_science\": (\"MMLU\", \"High School Computer Science\"),\n",
    "    \"mmlu_high_school_mathematics\": (\"MMLU\", \"High School Mathematics\"),\n",
    "    \"mmlu_high_school_physics\": (\"MMLU\", \"High School Physics\"),\n",
    "    \"mmlu_high_school_statistics\": (\"MMLU\", \"High School Statistics\"),\n",
    "    \"mmlu_machine_learning\": (\"MMLU\", \"Machine Learning\"),\n",
    "    \"gpqa_main_zeroshot\": (\"GPQA\", \"GPQA Main (Zero-Shot)\"),\n",
    "    \"gpqa_extended_zeroshot\": (\"GPQA\", \"GPQA Extended (Zero-Shot)\"),\n",
    "    \"gpqa_diamond_zeroshot\": (\"GPQA\", \"GPQA Diamond (Zero-Shot)\"),\n",
    "    \"arc_challenge\": (\"ARC\", \"ARC Challenge\"),\n",
    "    \"arc_easy\": (\"ARC\", \"ARC Easy\"),\n",
    "    \"sciq\": (\"SciQ\", \"SciQ\"),\n",
    "} \n",
    "\n",
    "COLUMN_SELECTOR = {\n",
    "    \"acc,none\": \"Accuracy\",\n",
    "    \"acc_stderr,none\": \"SE\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "def load_results(model: str) -> pd.DataFrame:\n",
    "    path = os.path.join(\"results\", model, \"results.json\")\n",
    "    with open(path, \"r\") as f:\n",
    "        eval = json.load(f)\n",
    "\n",
    "    # Load results\n",
    "    results = pd.DataFrame(eval[\"results\"]).transpose()\n",
    "\n",
    "    # Change/ select column names\n",
    "    TASK_SELECTOR = {k: v[1] for k, v in INDEX_SELECTOR.items()}\n",
    "    results = results.rename(index=TASK_SELECTOR, columns=COLUMN_SELECTOR)\\\n",
    "        .loc[TASK_SELECTOR.values(), COLUMN_SELECTOR.values()]\\\n",
    "        .reset_index().rename(columns={\"index\": \"Task\"})\n",
    "    \n",
    "    # Add group\n",
    "    results[\"Group\"] = [group for group, _ in INDEX_SELECTOR.values()]\n",
    "\n",
    "    # Create multi-index\n",
    "    results = results.set_index([\"Group\", \"Task\"])\n",
    "\n",
    "\n",
    "    return results\n",
    "\n",
    "def load_epfl_samples(model: str) -> pd.DataFrame:\n",
    "    # Load samples\n",
    "    path = os.path.join(\"results\", model, f\"samples_epfl-mcq.json\")\n",
    "    with open(path, \"r\") as f:\n",
    "        samples = [json.loads(line) for line in f]\n",
    "\n",
    "    # Create dataframe\n",
    "    samples = pd.DataFrame(samples)\n",
    "\n",
    "    # Add group\n",
    "    samples[\"Subject\"] = samples[\"doc\"].apply(lambda x: x[\"subject\"])\n",
    "    samples[\"Question\"] = samples[\"doc\"].apply(lambda x: x[\"question\"])\n",
    "    for choice in range(4):\n",
    "        samples[f\"{chr(65+choice)}\"] = samples[\"doc\"].apply(lambda x: x[\"choices\"][choice])\n",
    "    samples[\"Target\"] = samples[\"target\"].apply(lambda x: x[0])\n",
    "    preds = []\n",
    "    for resps in samples[\"resps\"].apply(lambda xs: [x[0][1] for x in xs]):\n",
    "        pred = None\n",
    "        try:\n",
    "            pred = chr(65 + resps.index(\"True\"))\n",
    "        except:\n",
    "            pass\n",
    "        preds.append(pred)\n",
    "    samples[\"Prediction\"] = preds\n",
    "\n",
    "    samples[\"Correct\"] = samples[\"acc\"]\n",
    "\n",
    "    # Select only these columns\n",
    "    samples = samples[[\"Subject\", \"Question\", \"A\", \"B\", \"C\", \"D\", \"Target\", \"Prediction\", \"Correct\"]]\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phi-3\n",
    "\n",
    "---\n",
    "\n",
    "* Model Path: `microsoft/Phi-3-mini-4k-instruct`\n",
    "* Results Path: `model/results/phi3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "phi3 = load_results(\"phi3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLama3-8B Instruct\n",
    "\n",
    "---\n",
    "\n",
    "* Model Path: `meta-llama/LLaMA-3-8B-instruct`\n",
    "* Results Path: `model/results/llama3-8b-instruct` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results\n",
    "llama3 = load_results(\"llama3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenELM-3B Instruct\n",
    "\n",
    "---\n",
    "\n",
    "* Model Path: `apple/OpenELM-3B-Instruct`\n",
    "* Model Name: `OpenELM-3B Instruct`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results\n",
    "openelm = load_results(\"openelm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuned Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DPO Phi3\n",
    "\n",
    "---\n",
    "\n",
    "* Model Path: `cs552-mlp/phi3-dpo`\n",
    "* Results Path: `model/results/phi3-dpo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "phi3_dpo = load_results(\"phi3-dpo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phi-3 SciQ\n",
    "\n",
    "---\n",
    "\n",
    "* Model Path: `cs552-mlp/phi3-sciq`\n",
    "* Results Path: `model/results/phi3-sciq`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "phi3_sciq = load_results(\"phi3-sciq3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phi-3 Arc\n",
    "\n",
    "---\n",
    "\n",
    "* Model Path: `cs552-mlp/phi3-arc`\n",
    "* Results Path: `model/results/phi3-arc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "phi3_arc = load_results(\"phi3-arc3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phi-3 OpenBookQA\n",
    "\n",
    "---\n",
    "\n",
    "* Model Path: `cs552-mlp/phi3-openbookqa`\n",
    "* Results Path: `model/results/phi3-openbookqa`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "phi3_openbookqa = load_results(\"phi3-openbookqa3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phi-3 MCQ\n",
    "\n",
    "---\n",
    "\n",
    "Trained on all MCQ datasets (OpenBookQA, ARC, SciQ)\n",
    "\n",
    "* Model Path: `cs552-mlp/phi3-mcq`\n",
    "* Results Path: `model/results/phi3-mcq`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "phi3_mcq = load_results(\"phi3-mcq3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantised Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phi-3 Arc GPTQ 8b\n",
    "\n",
    "---\n",
    "\n",
    "* Model Path: `cs552-mlp/phi3-lora-arc-gptq-8b`\n",
    "* Results Path: `model/results/phi3-arc3-gptq-8b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "phi3_arc_gptq_8b = load_results(\"phi3-arc3-gptq-8b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phi-3 Arc GPTQ 4b\n",
    "\n",
    "---\n",
    "\n",
    "* Model Path: `cs552-mlp/phi3-lora-arc-gptq-4b`\n",
    "* Results Path: `model/results/phi3-arc3-gptq-4b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "phi3_arc_gptq_4b = load_results(\"phi3-arc3-gptq-4b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phi-3 Arc GPTQ 3b\n",
    "\n",
    "---\n",
    "\n",
    "* Model Path: `cs552-mlp/phi3-lora-arc-gptq-3b`\n",
    "* Results Path: `model/results/phi3-arc3-gptq-3b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "phi3_arc_gptq_3b = load_results(\"phi3-arc3-gptq-3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phi-3 Arc GPTQ 2b\n",
    "\n",
    "---\n",
    "\n",
    "* Model Path: `cs552-mlp/phi3-lora-arc-gptq-2b`\n",
    "* Results Path: `model/results/phi3-arc3-gptq-2b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "phi3_arc_gptq_2b = load_results(\"phi3-arc3-gptq-2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "---\n",
    "\n",
    "We combine the benchmark results from all baseline and fine-tuned models and analyze the performance of each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantiative Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"OpenELM\": (openelm, \"Baseline\", \"Unquantised\"),\n",
    "    \"LLama\": (llama3, \"Baseline\", \"Unquantised\"),\n",
    "    \"Phi-3\": (phi3, \"Baseline\", \"Unquantised\"),\n",
    "    \"Phi-3-DPO\": (phi3_dpo, \"Finetuned\", \"Unquantised\"),\n",
    "    \"Phi-3-SciQ\": (phi3_sciq, \"Finetuned\", \"Unquantised\"),\n",
    "    \"Phi-3-OBQA\": (phi3_openbookqa, \"Finetuned\", \"Unquantised\"),\n",
    "    \"Phi-3-Arc\": (phi3_arc, \"Finetuned\", \"Unquantised\"),\n",
    "    \"Phi-3-MCQ\": (phi3_mcq, \"Finetuned\", \"Unquantised\"),\n",
    "    \"GPTQ-8b\": (phi3_arc_gptq_8b, \"Finetuned\", \"Quantised\"),\n",
    "    \"GPTQ-4b\": (phi3_arc_gptq_4b, \"Finetuned\", \"Quantised\"),\n",
    "    \"GPTQ-3b\": (phi3_arc_gptq_3b, \"Finetuned\", \"Quantised\"),\n",
    "    \"GPTQ-2b\": (phi3_arc_gptq_2b, \"Finetuned\", \"Quantised\"),\n",
    "}\n",
    "\n",
    "model_df = [x[0] for x in models.values()]\n",
    "baseline = [x[1] for x in models.values()]\n",
    "quantised = [x[2] for x in models.values()]\n",
    "\n",
    "combined = pd.concat(model_df, keys=[(k, b, q) for k, b, q in zip(models.keys(), baseline, quantised)], axis=0).reset_index().rename(columns={\"level_0\": \"Model\", \"level_1\": \"Baseline\", \"level_2\": \"Quantised\"})\n",
    "\n",
    "combined[\"Accuracy\"] = (combined[\"Accuracy\"] * 100).astype(float)\n",
    "combined[\"SE\"] = (combined[\"SE\"] * 100).astype(float)\n",
    "\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To LaTeX\n",
    "def format_for_latex(scores: pd.DataFrame, caption: str, label: str, drop_col = [\"Baseline\", \"Quantised\"], agg: bool = True) -> pd.DataFrame:\n",
    "    latex_df = scores.copy()\n",
    "\n",
    "    # Aggregate tasks\n",
    "    if agg:\n",
    "        latex_df = latex_df.groupby([\"Model\", \"Group\", \"Baseline\", \"Quantised\"])\\\n",
    "            .agg({\"Accuracy\": \"mean\", \"SE\": \"mean\"}).reset_index()\n",
    "    \n",
    "    # Combine accuracy and std. error\n",
    "    latex_df[\"Acc. Â± SE\"] = latex_df.apply(lambda x: f\"{x['Accuracy']:.1f} Â± {x['SE']:.1f}\", axis=1)\n",
    "    latex_df = latex_df.drop(columns=[\"Accuracy\", \"SE\"])\n",
    "    \n",
    "    # Drop user-specified columns\n",
    "    latex_df = latex_df.drop(columns=drop_col)\n",
    "\n",
    "    # Unstack columns\n",
    "    c =  [\"Model\", \"Group\"] + ([\"Task\"] if not agg else [])\n",
    "    latex_df = latex_df.set_index(c)\n",
    "    latex_df = latex_df.unstack(\"Model\")\n",
    "\n",
    "    # Remove multi-col\n",
    "    latex_df.columns = latex_df.columns.droplevel(0)\n",
    "\n",
    "    # Unname index and columns\n",
    "    latex_df.index.name = None\n",
    "    latex_df.columns.name = None\n",
    "\n",
    "    # Convert to latex\n",
    "    latex = latex_df.to_latex(caption=caption, label=label, position=\"h\")\n",
    "\n",
    "    # Post-process\n",
    "    def add_centering(latex_code):\n",
    "        lines = latex_code.split('\\n')\n",
    "        for i, line in enumerate(lines):\n",
    "            if line.strip().startswith(r'\\begin{table}'):\n",
    "                lines.insert(i + 1, r'\\centering')\n",
    "                break\n",
    "        return '\\n'+ '\\n'.join(lines)\n",
    "\n",
    "    latex = add_centering(latex)\n",
    "\n",
    "    return latex_df, latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_latex(latex: str, path: str):\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_plot(scores: pd.DataFrame) -> pd.DataFrame:\n",
    "    plot_df = scores.copy()\n",
    "\n",
    "    # Aggregate scores\n",
    "    plot_df = plot_df.groupby([\"Model\", \"Group\", \"Baseline\", \"Quantised\"])\\\n",
    "        .agg({\"Accuracy\": \"mean\", \"SE\": \"mean\"}).reset_index()\n",
    "\n",
    "    return plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "plot.benchmark.multiples <- function(df, path, title, order) {\n",
    "    df$Model <- factor(df$Model, levels=order)\n",
    "\n",
    "    p <- ggplot(df, aes(x=Group, y=Accuracy, fill=Model)) +\n",
    "        geom_bar(width=.8, stat=\"identity\", position = position_dodge(width = .9, preserve = \"single\"), linewidth=0.25, linetype=\"solid\", color=\"black\") +\n",
    "        # Remove facet wrap title\n",
    "        facet_wrap(~Group, scales=\"free_x\", ncol=3, labeller = ) +\n",
    "        labs(\n",
    "            title=title,\n",
    "            x=NULL,\n",
    "            y=NULL,\n",
    "        ) +\n",
    "        geom_errorbar(aes(ymin=Accuracy-SE, ymax=Accuracy+SE), width=.2, position=position_dodge(.9)) +\n",
    "        theme_minimal() +\n",
    "        theme(legend.position=\"bottom\", strip.text.x = element_blank(), panel.grid.minor = element_blank()) +\n",
    "        scale_fill_brewer(palette = \"Blues\")\n",
    "\n",
    "    # # Save plot\n",
    "    ggsave(path, plot=p, width=5, height=4, units=\"in\", dpi=300)\n",
    "    p\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "plot.benchmark <- function(df, path, title, order) {\n",
    "    df$Model <- factor(df$Model, levels=order)\n",
    "\n",
    "    p <- ggplot(df, aes(x=Group, y=Accuracy, fill=Model)) +\n",
    "        geom_bar(width=.8, stat=\"identity\", position = position_dodge(width = .9, preserve = \"single\"), linewidth=0.25, linetype=\"solid\", color=\"black\") +\n",
    "        labs(\n",
    "            title=title,\n",
    "            x=\"Task\",\n",
    "            y=\"Accuracy (%)\",\n",
    "        ) +\n",
    "        geom_errorbar(aes(ymin=Accuracy-SE, ymax=Accuracy+SE), width=.2, position=position_dodge(.9)) +\n",
    "        theme_minimal() +\n",
    "        scale_fill_brewer(palette = \"Blues\")\n",
    "        scale_x_discrete(limits=c(\"SciQ\", \"Arc\", \"MMLU\", \"OBQA\", \"GPQA\"))\n",
    "\n",
    "    # # Save plot\n",
    "    ggsave(path, plot=p, width=10, height=3, units=\"in\", dpi=300)\n",
    "    p\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baselines\n",
    "\n",
    "Here we are just going to compare the baselines against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline scores\n",
    "baseline_scores = combined[(combined[\"Baseline\"] == \"Baseline\") & (combined[\"Quantised\"] != \"Quantised\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LateX Table\n",
    "baseline_latex_df, baseline_latex = format_for_latex(baseline_scores,\n",
    "    caption=\"\\\\textbf{Baseline Results.} Accuracy and Standard Error (SE) for baseline models.\",\n",
    "    label=\"tab:baseline-benchmark\")\n",
    "\n",
    "# Display\n",
    "path = \"../report/tables/baseline-benchmark.tex\"\n",
    "write_latex(baseline_latex, path)\n",
    "\n",
    "baseline_latex_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Plot\n",
    "baseline_plot_df = format_for_plot(baseline_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i baseline_plot_df -w 5 -h 4 -u in -r 100\n",
    "\n",
    "plot.benchmark.multiples(baseline_plot_df, \n",
    "    path=\"../report/figures/baseline-benchmark.png\",\n",
    "    title=\"Baseline Benchmark\",\n",
    "    order=c(\"OpenELM\", \"LLama\", \"Phi-3\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-Tuning\n",
    "\n",
    "Here, we are showing the performance of the fine-tuned models against the Phi-3 baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuned scores\n",
    "finetuned_scores = pd.concat([\n",
    "    combined[combined[\"Model\"] == \"Phi3\"],\n",
    "    combined[(combined[\"Baseline\"] == \"Finetuned\") & (combined[\"Quantised\"] != \"Quantised\")]\n",
    "], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LateX Table\n",
    "finetuned_latex_df, finetuned_latex = format_for_latex(finetuned_scores,\n",
    "    caption=\"\\\\textbf{Finetune Results.} Accuracy and Standard Error (SE) for fine-tuned models and Phi-3 baseline.\",\n",
    "    label=\"tab:finetune-benchmark\"\n",
    ")\n",
    "\n",
    "# Display\n",
    "path = \"../report/tables/finetuned-benchmark.tex\"\n",
    "write_latex(finetuned_latex, path)\n",
    "\n",
    "finetuned_latex_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Plot\n",
    "finetuned_plot_df = format_for_plot(finetuned_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i finetuned_plot_df -u in -w 10 -h 3 -r 100\n",
    "\n",
    "plot.benchmark(finetuned_plot_df,\n",
    "    path=\"../report/figures/finetuned-benchmark.png\",\n",
    "    title=\"Finetuned Benchmark\",\n",
    "    order=c(\"Phi-3\", \"Phi-3-DPO\", \"Phi-3-SciQ\", \"Phi-3-OBQA\", \"Phi-3-Arc\", \"Phi-3-MCQ\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantisation\n",
    "\n",
    "Here we are showing the performance of the quantised models against the unquantised model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantised scores\n",
    "quantised_scores = pd.concat([\n",
    "    combined[combined[\"Model\"] == \"Phi-3-Arc\"],\n",
    "    combined[combined[\"Quantised\"] == \"Quantised\"]\n",
    "], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LateX Table\n",
    "quantised_latex_df, quantised_latex = format_for_latex(quantised_scores,\n",
    "    caption=\"\\\\textbf{Quantisation Results.} Accuracy and Standard Error (SE) for quantised models and its baseline.\",\n",
    "    label=\"tab:quantised-benchmark\",\n",
    ")\n",
    "\n",
    "# Display\n",
    "path = \"../report/tables/quantised-benchmark.tex\"\n",
    "write_latex(quantised_latex, path)\n",
    "\n",
    "quantised_latex_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Plot\n",
    "quantised_plot_df = format_for_plot(quantised_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i quantised_plot_df -u in -w 10 -h 3 -r 100\n",
    "\n",
    "plot.benchmark(quantised_plot_df,\n",
    "    path=\"../report/figures/quantised-benchmark.png\",\n",
    "    title=\"Quantised Benchmark\",\n",
    "    order=c(\"Phi-3-Arc\", \"GPTQ-8b\", \"GPTQ-4b\", \"GPTQ-3b\", \"GPTQ-2b\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qulitative Analysis\n",
    "\n",
    "We want to understand the difference in model behaviour for two pairs of models:\n",
    "\n",
    "1. Phi-3 and Phi-3 ARC (Baseline vs. Fine-Tuned)\n",
    "2. Phi-3 ARC vs. Phi3-ARC GPTQ 4b (Fine-Tuned vs. Quantised)\n",
    "\n",
    "In particular, we will investigate the following:\n",
    "\n",
    "* Analyse the performance per subject (from MMLU)\n",
    "* Analyse the answer distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mmlu_samples(model: str) -> pd.DataFrame:\n",
    "    # Load all MMLU results\n",
    "    path = f\"results/{model}\"\n",
    "    filenames = [file for file in os.listdir(path) if \"mmlu\" in file]\n",
    "    all_samples = []\n",
    "    for filename in filenames:\n",
    "        with open(os.path.join(path, filename), \"r\") as f:\n",
    "            all_samples.extend([json.loads(line) for line in f])\n",
    "\n",
    "    # Process to data frame\n",
    "    samples = pd.DataFrame(all_samples)\n",
    "\n",
    "    def get_group(subject: str) -> str:\n",
    "        if \"College\" in subject:\n",
    "            return \"College\"\n",
    "        elif \"High School\" in subject:\n",
    "            return \"High School\"\n",
    "        elif \"Elementary\" in subject:\n",
    "            return \"Elementary\"\n",
    "        else:\n",
    "            return \"Unknown\"\n",
    "\n",
    "    # Define relevant columns\n",
    "    samples[\"model\"] = model\n",
    "    samples[\"subject\"] = samples.doc.apply(lambda x: \" \".join(map(lambda x: x[0].upper() + x[1:],x[\"subject\"].split(\"_\"))))\n",
    "    samples[\"group\"] = samples.subject.apply(lambda x: get_group(x))\n",
    "    samples[\"question\"] = samples.doc.apply(lambda x: x[\"question\"])\n",
    "    samples[\"choices\"] = samples.doc.apply(lambda x: x[\"choices\"])\n",
    "    samples[\"target\"] = samples.doc.apply(lambda x: x[\"answer\"])\n",
    "    samples[\"logprobs\"] = samples.resps.apply(lambda xs: [float(x[0][0]) for x in xs])\n",
    "    samples[\"answer\"] = samples.logprobs.apply(lambda x: x.index(max(x)))\n",
    "    samples[\"correct\"] = samples.acc.astype(bool)\n",
    "\n",
    "    # Select only these columns\n",
    "    samples = samples[[\"group\", \"subject\", \"question\", \"choices\", \"target\", \"answer\", \"correct\", \"logprobs\"]]\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores_per_group(df, model):\n",
    "    df = df.groupby(\"group\").agg({\"correct\": [\"mean\", \"sem\"]}).reset_index().sort_values((\"correct\", \"mean\"), ascending=False)\n",
    "    df[\"Model\"] = model\n",
    "    df.columns = [\"Group\", \"Accuracy\", \"SE\", \"Model\"]\n",
    "\n",
    "    return df.reset_index(drop=True).set_index([\"Model\", \"Group\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores_per_subject(df, model):\n",
    "    df = df.groupby(\"subject\").agg({\"correct\": [\"mean\", \"sem\"]}).reset_index().sort_values((\"correct\", \"mean\"), ascending=False)\n",
    "    df[\"Model\"] = model\n",
    "    df.columns = [\"Subject\", \"Accuracy\", \"SE\", \"Model\"]\n",
    "\n",
    "    return df.reset_index(drop=True).set_index([\"Model\", \"Subject\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(a):\n",
    "    conf =  a.groupby([\"target\", \"answer\"]).size().unstack(fill_value=0)\n",
    "    options = [\"A\", \"B\", \"C\", \"D\"]\n",
    "    # Rename index and columns according to options\n",
    "    conf.columns, conf.index = options, options\n",
    "    conf.index.name, conf.columns.name = \"Target\", \"Prediction\"\n",
    "\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phi-3 vs. Phi-3 ARC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi3_mmlu = load_mmlu_samples(\"phi3\")\n",
    "phi3_arc_mmlu = load_mmlu_samples(\"phi3-arc3\")\n",
    "phi3_arc_4b_mmlu = load_mmlu_samples(\"phi3-arc3-gptq-4b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Phi3\")\n",
    "print(f\"Macro Avg.: {phi3_mmlu.groupby('subject').correct.mean().mean() * 100:.2f}%\")\n",
    "print(f\"Micro Avg.: {phi3_mmlu.correct.mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Phi3-ARC\")\n",
    "print(f\"Macro Avg.: {phi3_arc_mmlu.groupby('subject').correct.mean().mean() * 100:.2f}%\")\n",
    "print(f\"Micro Avg.: {phi3_arc_mmlu.correct.mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Phi3-ARC 4b\")\n",
    "print(f\"Macro Avg.: {phi3_arc_4b_mmlu.groupby('subject').correct.mean().mean() * 100:.2f}%\")\n",
    "print(f\"Micro Avg.: {phi3_arc_4b_mmlu.correct.mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_per_group = pd.concat([\n",
    "    get_scores_per_group(phi3_mmlu, \"Phi-3\"),\n",
    "    get_scores_per_group(phi3_arc_mmlu, \"Phi-3-ARC\"),\n",
    "    get_scores_per_group(phi3_arc_4b_mmlu, \"Phi-3-ARC-4b\")\n",
    "], axis=0)\n",
    "scores_per_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_per_subject = pd.concat([\n",
    "    get_scores_per_subject(phi3_mmlu, \"Phi-3\"),\n",
    "    get_scores_per_subject(phi3_arc_mmlu, \"Phi-3-ARC\"),\n",
    "    get_scores_per_subject(phi3_arc_4b_mmlu, \"Phi-3-ARC-4b\")\n",
    "], axis=0)\n",
    "scores_per_subject.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scores per subject\n",
    "plot_df = scores_per_subject.reset_index()\n",
    "plot_df[\"Accuracy\"] = plot_df[\"Accuracy\"] * 100\n",
    "keep_subjects = [\"High School Biology\", \"College Biology\", \"High School Mathematics\", \"College Mathematics\", \"High School Physics\", \"College Physics\"]\n",
    "plot_df = plot_df[plot_df[\"Subject\"].isin(keep_subjects)]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "sns.barplot(x=\"Accuracy\", y=\"Subject\", hue=\"Model\", data=plot_df, palette=\"Blues\", ax=ax)\n",
    "# Increase font sizes\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.ylabel(\"\")\n",
    "plt.xlabel(\"Accuracy (%)\", fontsize=14)\n",
    "\n",
    "fig.savefig(\"../report/figures/mmlu-per-subject.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(3 * 2.5, 2.1))\n",
    "fig.tight_layout(h_pad=5)\n",
    "\n",
    "phi3_mmlu_conf = get_confusion_matrix(phi3_mmlu)\n",
    "phi3_arc_mmlu_conf = get_confusion_matrix(phi3_arc_mmlu)\n",
    "phi3_arc_4b_mmlu_conf = get_confusion_matrix(phi3_arc_4b_mmlu)\n",
    "sns.heatmap(phi3_mmlu_conf, annot=True, cmap=\"Blues\", fmt=\"d\", ax=ax[0])\n",
    "sns.heatmap(phi3_arc_mmlu_conf - phi3_mmlu_conf, annot=True, cmap=\"coolwarm\", fmt=\"d\", ax=ax[1])\n",
    "sns.heatmap(phi3_arc_4b_mmlu_conf - phi3_mmlu_conf, annot=True, cmap=\"coolwarm\", fmt=\"d\", ax=ax[2])\n",
    "ax[0].set_title(\"Phi3\", fontsize=16)\n",
    "ax[1].set_title(\"Phi3-ARC vs. Phi3\", fontsize=16);\n",
    "ax[2].set_title(\"Phi3-ARC-4b vs. Phi3\", fontsize=16);\n",
    "for a in ax:\n",
    "    a.set_xlabel(\"\")\n",
    "    a.set_ylabel(\"\")\n",
    "\n",
    "\n",
    "fig.savefig(\"../report/figures/mmlu-confusion.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnlp-m2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
